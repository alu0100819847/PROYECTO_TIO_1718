Haciendo que la instrucción que tiene la dependencia espere para ejecutarse hasta que sus operandos hayan sido calculados
Ninguna respuesta es correcta
Emisión múltiple de instrucciones, etapa de graduación y common data bus
No, en ningún caso
En la etapa de ejecución
Problemas estructurales
3
Unidad de detección de riesgos y de anticipación
Igual que el caso de la máquina segmentada de 5 etapas.
Dividiendo la etapa IF en dos etapas, en las que si la instrucción es un salto, se obtengan los valores de los registros necesarios para calcular su dirección en la etapa ID (se necesita acceso a registros en etapas IF y una unidad ALU extra).
Reducir el número de etapas
Cuando las estrategias de anticipación y bloqueo no resuelvan el riesgo de control y/o datos
MIMD
Permite localización contigua asignando a un mismo módulo de memoria y solo el acceso a una palabra en un mismo ciclo.
Vectoriales, SIMD; Superescalares, SIMD
Porque indica la longitud que puede tener cualquier vector dentro de un programa, sin sobrepasar un determinado máximo.
No, porque conviene que el sistema sea más tolerante a fallos que el rendimiento que debe desarrollar, de forma que se pierda el menor número de datos posible en caso de fallo.
Se accede a memoria con un patrón conocido
Encadenamiento 
Viene dada por la frecuencia con la que la unidad funcional puede aceptar nuevos operandos
Búsqueda, seleccionar hilo, decodificación, ejecución, memoria y escritura.\r\n
En SMT no es necesario duplicar las unidades funcionales, no así en CMP.
Aumentar la emisión múltiple.
El Niagara tiene 16Kb y 8Kb de memoria caché a nivel 1, para instrucciones y datos respectivamente.\r\n
Una prioridad igual a la de los hilos no especulados.
SIMD
No están agrupadas
Ninguna es cierta
Unique Management Access
Productividad
Tiempo de respuesta
“ADD”, “SUB” y “MUL”
Ampliar el repertorio de instrucciones para permitir operaciones en paralelo
Temporal y segmentada
SIDS,SIDM,MIDS,MIDM
VLIW
Tiene un repertorio de instrucciones más amplio y más complejo que RISC
Todas son correctas.
MIMD porque un único núcleo ejecuta al mismo tiempo instrucciones distintas sobre\r\ndatos diferentes.
Las que acceden a los niveles físicos de los componentes hardware.
No.
Si, los sistemas multiprocesadores no admiten procesadores con HyperThreading.
Heterogéneos.
Si, cuando el rendimiento del procesador con HyperThreading es el ideal, es decir, cuando utiliza los recursos necesarios para que el procesador no tenga que intercambiar los procesos en ejecución.
No, porque MPI no paraleliza el código.
Desconocido.
Malla, Hipercubo y N-cubo K-aria
Únicamente bidireccional
c
No es compatible con el software existente
4.8 Ghz
Ha comenzado a ser construido con la tecnología de fabricación de 65nm
La memoria caché no forma parte de la jerarquía de memoria
Elimina el bloque que haya sido referenciado menos veces
4 accesos simultáneos 
42 en FIFO, 36 en LRU y 32 en óptimo
Sí, porque ha copiado en primer lugar la palabra necesaria
Necesitará un bit que indique en qué procesador está la palabra y otro que indique si la palabra es válida
Porque así reduce a la mitad el tiempo de reseteo de los bits
Reemplazando 2 vértices del hipercubo adimensional por un anillo de k nodos.
Que el nodo raíz y los nodos cerca de él se convierten en cuellos de botella.
La caché de instrucciones
Pentium II
Predictor local, predictor de torneo y predictor correlativo
Ninguna de las respuestas es correcta
Salto retardado.
Jsva no permite la programación mediante threads.
Arquitectura avanzada de VLIW de alto rendimiento para favorecer el paralelismo\r\na nivel de instrucción.
Si el número de bancos es estrictamente mayor que el tiempo de acceso expresado en ciclos de reloj
El orden en el que se ejecutan los load y los store siempre es indiferente, aunque el load escriba en el mismo registro que lee el store
En la etapa de graduación
Las dependencias de datos
1
Burbujas, predicción y anticipación.
Cuando la circuitería no puede soportar la combinación de instrucciones que se quiere ejecutar en el mismo ciclo.
Unidad de detección de riesgos, de anticipación y de control
No existirían tales dependencias, puesto que cuando se termina la ejecución (ALU) se escribe en registro, por lo que está accesible para la siguiente instrucción.
No se puede adelantar el cálculo de la dirección de un salto en ningún caso. 
Aumentar el número de etapas
Cuando hay más de dos instrucciones dependientes entre sí
Combinación de un unidad escalar común y una unidad vectorial encargada de manipular todas las operaciones vectoriales.\r\n
Permite localización contigua asignando a un mismo módulo de memoria y acceso a varias palabras en un mismo ciclo.
Vectoriales, MIMD; Superescalares, MIMD
No es necesario, ya que la longitud del vector es conocida en tiempo de compilación.
No, ya que la función de gestión de transferencias se puede realizar mediante una unidad funcional dedicada a dicha tarea en concreto.
No, porque conviene maximizar el rendimiento de la máquina que la tolerancia a los fallos.
Existen riesgos de control y de datos al igual que en los procesadores segmentados
Strip-mining
No suele mejorar el rendimiento en los vectores de tamaño medio
Su bajo consumo.
Búsqueda, decoficación, seleccionar hilo, ejecuación y escritura.
10 ciclos.
Tres.
Los registros.
Una prioridad menor que a los hilos no especulados.
MIMD
Almacena el byte más significativo en la dirección más alta de memoria
Ejecuta 6 instrucciones por ciclo en un pipeline de 10 estados
SIMD
Permite realizar operaciones en paralelo empaquetando las instrucciones en paquetes de 256 bits\r\n
Aceleración
Modo bidireccional
Tiempo de respuesta
MIPS
SISD
SSD,SID,MMD,MID
Tiempo antiguo*[(1 + fracción mejorada) + (fracción mejorada/aceleración mejorada)]
Tiene un repertorio de instrucciones menos amplio y menos complejo que RISC
La voluntad de los principales fabricantes para crear procesadores multinúcleo más\r\npotentes.
MIMD porque varios núcleos autónomos ejecutan simultáneamente instrucciones\r\ndiferentes sobre datos diferentes.
La memoria.
Dependiendo del tipo de procesador multinúcleo donde se ejecuten.
El número de núcleos del procesador, el número de núcleos que comparten un\r\ndeterminado nivel de caché, el tamaño del bloque de caché y el grado de\r\nasociatividad de los conjuntos de caché.
Uniformes.
No. Todos los núcleos de un procesador multinúcleo deben trabajar a la misma\r\nfrecuencia de reloj al estar alojados en el mismo dispositivo.
Si, sólo cuando ejecuta programas secuenciales.
Vectorial.
100.
Hipercubo, Estrella, N-cubo K-aria
Únicamente unidireccional
d
Hay demasiado software para su uso
Dos Core 2 Duo 
Ha comenzado a ser construido con la tecnología de fabricación de 45nm
Memoria caché 
Elimina el bloque cuyo contador de lecturas esté a cero. Es una mejora de NRU
2 accesos simultáneos
No se efectúan reemplazamientos
Sí, porque ha detenido la ejecución de la intrucción para tratar el fallo de caché
Necesitará dos bits que indiquen en qué procesador está la palabra y otro que indique si la palabra es válida
Reemplazando cada lado del hipercubo adimensional por un cubo de k nodos.
Que existen excesivos caminos alternativos entre cualquier par de nodos.
Con las matrices sistólicas.
Las estaciones de reserva
Unidad de punto flotante del IBM 360/91
Predictor correlativo, predictor local, predictor de torneo
Almacena para cada registro de la máquina si contiene un valor usable o solamente basura
Ninguna de las respuestas es correcta
En la etapa de rollback
Las bifurcaciones.
2
Burbujas, predicción y decisiones retardadas.
Cuando una instrucción depende del resultado de una instrucción previa que todavía está en el cauce.
Unidad de anticipación
Un juego adicional de instrucciones que facilita el cambio de contexto.
Sólo existirían dependencias cuando la instrucción de la que depende el registro es un lw, puesto que se tiene que esperar un ciclo para tener el valor correcto del registro.
Incapacidad de tolerar fallos.
Si, siempre que cumpla que no emita más de una instrucción por ciclo.
Reducir el tiempo de ejecución de la instrucción que más tarde (load)
Cuando hay un riesgo de control seguido de un riesgo de datos
Combinación de varias unidades vectoriales que paralelizan el trabajo simultáneamente.\r\n
Sí, ya que se consigue acceder a los datos de una manera más rápida al estar justamente al lado del procesador.
32
Sí, ya que al dedicarse a la gestión de transferencias entre memoria o entre registros, descarga a las unidades funcionales de esta tarea y modulariza el diseño.
Sí, pues no presenta ninguna incompatibilidad ni perdida de eficiencia de la máquina.
Ninguna es cierta
Ejecutar sentencias condicionalmente
Permite que una operación vectorial comience tan pronto como los elementos individuales de su operando vectorial estén listos
Dos.\r\n
14 ciclos.
Cinco.
Cuenta con 6 unidades funcionales que se encuentran en un solo chip.
 Todas son verdaderas.
Ninguna de las anteriores es cierta.
Es una arquitectura avanzada de VLIW de alto rendimiento
Almacena el byte menos significativo en la dirección más baja de memoria
Cada instrucción puede ser ejecutada en una o más unidades funcionales
Los procesadores Pentium IV y Xeon son los primeros que lo soportan
Uniform Management Access
Rendimiento
Modo retardado
Speed-up
Ninguna es cierta
La forma en que comparten memoria
Indica dónde se almacenará el resultado de la instrucción
CISC
Tiene un repertorio de instrucciones menos complejo, pero más amplio que RISC
Las limitaciones físicas que restringen la capacidad de integración de los transistores\r\nde un procesador.
Todas son correctas.
La unidad de procesamiento.
Dependiendo del sistema operativo que las ejecute.
El número de núcleos del procesador.
Ninguno.
Sí. Se puede aplicar frecuencias totalmente distintas a los distintos núcleos en función\r\nde sus características.
Sí, siempre.
VLIW.
3, 10 ó 100.
Es una topología regular cuyos nodos pueden tener de n a 2n vecinos
Tanto bidireccional como unidireccional
Ninguna de las anteriores
Cuatro Pentium IV
Ha comenzado a ser construido con la tecnología de fabricación de 10nm
Memoria virtual
Elimina el bloque que lleve más tiempo sin ser referenciado
Caché coherente
101 en cada caso
No. Es posible que el operando B sea la primera palabra del bloque y que se esté utilizando "early-restart"
Necesitará un bit que indique en qué procesador está la palabra y dos que indiquen si la palabra es válida
Es un caso especial de anillo cordal y constituye una buena solución de compromiso entre complejidad y eficiencia.
Reemplazando cada vértice del hipercubo adimensional por un cubo de k nodos.
Que el embudo que se forma no puede eliminarse.
La facilidad de encaminamiento.
El reorder buffer
PowerPC 601
Predictor de torneo, predictor local, predictor correlativo
Almacena para cada registro de la máquina el índice de la estación de reserva cuya instrucción escribirá en ese registro
El algoritmo de Tomasulo no garantiza que se gradúen en orden
En la etapa de emisión
Tanto las dependencias de datos como las bifurcaciones.
No
Burbujas, anticipación y decisiones retardadas.
IF-->ID-->EX-->MEM-->WB 
Unidad de anticipación y de control
Sólo habría que anticipar en caso de que el registro destino de una operación de memoria fuese el registro fuente de la siguiente instrucción, puesto que dependencias entre otro tipo de instrucciones no se dan por la particularidad de la etapa 2. 
El de la resta
Si, siempre que cumpla que no ejecuta dos instrucciones en la misma etapa en el mismo ciclo.
Tamaño fijo de la instrucción, pocos formatos de instrucciones, menor número de etapas
Cuando no quede más remedio
Combinación de un unidad escalar segmentada  y una unidad vectorial encargada de manipular todas las operaciones vectoriales.
Vectoriales, MIMD; Superescalares, SIMD
64
No, ya que la tarea que desempeña no es algo fundamental en el funcionamiento de la máquina.
No, porque no se puede combinar un lenguaje secuencial como los anteriores con una máquina vectorial.
Encadenamiento
Reducción vectorial
La salida de una instrucción se anticipa a la entrada de la siguiente
Montecito es un procesador de doble núcleo y cuatro subprocesos de hardware por núcleo.\r\n
Un único thread no suele ser capaz de mantener ocupados todos los recursos de la máquina.
Un compilador exclusivo con el que compilar el software para generar programas capaces de ejecutarse en el procesador Niagara.\r\n
Diez.
La TLB.
Una instrucción.
MISD
Sólo pueden ser ejecutadas en una única unidad funcional
SISD
Arquitectura avanzada de CISC de alto rendimiento para favorecer el acceso a memoria\r\n
Es una extensión del juego de instrucciones MIPS aunque conlleva un bajo\r\nrendimiento en el procesado de gráficos 3D\r\n
Salto Incondicional
Aceleración
Memoria compartida
Acceder a posiciones de memoria que han sido utilizadas recientemente
SIMD
Indica el número de operaciones incluidas en la instrucción
RISC
Tiene un repertorio de instrucciones más complejo, pero menos amplio que RISC
Segmentado.
Cada uno de los núcleos de ejecución mantiene una pequeña memoria donde\r\nalmacena los datos empleados únicamente por el núcleo en cuestión.
La FPU.
Un problema de sincronización entre las cachés de cada uno de los núcleos.
La tecnología empleada en la creación de la jerarquía de cachés del procesador.
Ejecutarse solo en aquellos núcleos que tengan las capacidades de cómputo\r\nrequeridas por la aplicación.
Sí. Es conveniente emplear frecuencias de reloj distintas para los distintos núcleos de\r\nforma que se consiga obtener una aceleración elevada en la ejecución de aplicaciones\r\nparalelas.
Reduce la complejidad porque el repertorio de instrucciones disponibles es mayor.
No, porque MPI realiza una paralelización de forma generalista y por ello aprovechará de formas diferentes los procesadores.
Emplear un sistema de coherencia de caché basado en fisgoneo.
Es una topología irregular cuyos nodos pueden tener de n/2 a n vecinos
Ninguna de las anteriores
Una microarquitectura de 4 núcleos monolíticos
Dos procesadores Conroe
Memoria de acceso remoto
Los forzosos y los de arranque en frío
Caché bloqueante
36 en FIFO, 42 en LRU y 32 en óptimo
No. Critical-word-first no es una política de recuperación de fallos de caché
Todas las anteriores son falsas
Es el que se obtiene a partir de la configuración en hipercubo, añadiendo a cada nodo enlaces con los nodos que tengan una distancia potencia de 2.
Reemplazando cada vértice del hipercubo adimensional por un cubo de k nodos.
Que utilizan canales con excesivo ancho de banda en los canales cercanos al nodo raíz.
El common data bus(CDB)
MIPS R10000
Que la instrucción de esa estación de reserva no tiene operandos fuente
Almacena para cada registro de la máquina el índice de la estación de reserva cuyos operandos fuente referencian a este registro
En que no haya otros loads anteriores en el código y pendientes de ejecutarse que lean de la misma dirección
En la etapa de ejecución
WAR
Si, utilizando una estrategia de anticipación
Burbujas, reordenación del código y anticipación
ID-->IF-->EX-->MEM-->WB 
La ejecución de instrucciones en orden secuencial
Igual que el caso de la máquina segmentada de 5 etapas.
Si, siempre que las unidades estén en distintas etapas.
Tamaño fijo de la instrucción, rápida decodificación de la instrucción y menor número de etapas
En una instrucción lw seguida de una instrucción sw dependiente de la anterior
Combinación de varias unidades vectoriales que paralelizan el trabajo simultáneamente, además de una serie de unidades funcionales.
9
Desperdicio horizontal (horizontal waste).
No, porque al acceder a memoria para obtener los operandos, deberá pasar primero por la caché, cuya velocidad de acceso es menor que la que ofrece la memoria de la máquina.
128
Es indiferente su presencia como su eliminación de la máquina vectorial.
Sí, pero como es un lenguaje secuencial no se puede aprovechar el rendimiento vectorial ya que ni el lenguaje ni el compilador están preparados para explotarlo.
Reducción vectorial
Siempre
El tiempo de ejecución viene dado por el número de elementos del vector más las iniciaciones de las instrucciones
Single multithreading.
Búsqueda, decodificación, ejecución, memoria y escritura.
16 ciclos.
Nueve.
Ninguna de las anteriores es verdadera.
La cache.
Ninguna es cierta
Almacena el bit menos significativo en la dirección más alta de memoria
SIMD
Arquitectura avanzada de RISC de alto rendimiento para favorecer el acceso a memoria\r\n
Ninguna es cierta
Se introdujo en la familia Intel a partir de los procesadores Pentium III
Modo relativo a la FPU
“ADD”, “SUB” y “MUL”
Ninguna de las anteriores
La forma en que se paralelizan los datos
ISDS,ISMD,IMSD,IMMD
Tiempo antiguo*[(1 - fracción mejorada) + (aceleración mejorada/fracción mejorada)]
Ninguna es cierta
El aumento del número de núcleos de los procesadores actuales.
Uno de los núcleos de ejecución mantiene una memoria de gran tamaño donde se\r\nalmacenan los datos empleados por todos los núcleos del procesador.
Son capaces de paralelizar una aplicación.
Técnica utilizada por los procesadores HyperThreading para simular el paralelismo.
El número de núcleos que comparten un determinado nivel de caché.
Ejecutarse en todos los núcleos disponibles.
No. La frecuencia de reloj a la que trabajan los distintos núcleos del procesador ha de\r\nser la misma porque las características de los mismos son similares.
Es similar al caso en que se empleen núcleos homogéneos porque es el compilador o\r\nun intérprete inteligente quien realmente se encarga del reparto de tareas.
Todas las anteriores.
Emplear una red crossbar para la comunicación entre los distintos núcleos de ejecución.
Es una topología regular cuyos nodos pueden tener de n/2 a n vecinos
Se aumenta la conectividad y el diámetro
Controlador de memoria integrado en el chip
Dos procesadores Memron
MESI
Los de arranque en frío, los de capacidad y los de colisión
Caché no bloqueante 
Con la primera configuración
En una
Porque no solventaría los posibles fallos de caché
Es un tipo de arquitectura segmentada multidimensional diseñada para la realización de algoritmos específicos fijos.
En que el grado de nodo es siempre 6, independientemente de la dimensión del hipercubo, por lo que resulta una arquitectura escalable.
La escalabilidad.
Primero la número 1, luego la número 0 y finalmente la número 2, debido a la ejecución fuera de orden y a las diferentes latencias que tienen las unidades funcionales involucradas
Emisión, ejecución y escritura
Predictor de torneo, predictor correlativo, predictor local
Ninguna de las anteriores
Introduciendo las instrucciones en el reorder buffer en el momento de la emisión, y graduando solamente a la que se encuentra en la cabeza de dicha estructura
En la etapa de rollback
RAW
Sí, utilizando una estrategia de bloqueo
10
IF-->IC-->EX-->MEM-->WB
Solapamiento de instrucciones
No existirían tales dependencias, puesto que se asegura que una instrucción no puede comenzar antes de que termine la anterior.
Nunca. 
Tamaño fijo de la instrucción, menor número de etapas, mejor gestión de acceso a memoria
En una instrucción sw seguida de una instrucción lw dependiente de la anterior
Porque el acceso a memoria tanto para la carga como para el almacenamiento requiere un tiempo mayor que el necesitado por las máquinas vectoriales con registros vectoriales.\r\n
No, ya que la introducción de una caché en este tipo de máquinas es incompatible con la arquitectura básica de las mismas.
No tiene un valor fijo
Aumenta a medida que aumenta el tamaño del vector
El elevado aumento de niveles de caché que degrada el rendimiento de ésta.
No, porque pueden producirse conflictos internos a la máquina vectorial debido al mal uso de registros vectoriales y unidades funcionales vectoriales.
Strip-mining
Únicamente cuando los vectores que intervengan en la operación tengan la misma longitud
Se comparten recursos de forma síncrona.
Simultaneous multithreading.
Tres.
15 ciclos.
Conflictos de caché.
Los conflictos de caché generados por la ejecución simultánea de varios threads, que puede degradar considerablemente el rendimiento.
Todas las respuestas son correctas.
Tres instrucciones.
Simple Instruction Stream, Multiple Data Simple
Almacena el byte más significativo en la dirección más baja de memoria
Ambas son ciertas
Incorpora nuevas instrucciones 
Ninguna es cierta
Salto Condicional
Ninguna es cierta
No trabajan con memoria
Acceder a posiciones de memoria que no han sido utilizadas recientemente
MISD
Indica el número de instrucciones que siguen a la actual
Ninguna es cierta
Tiene un repertorio de instrucciones más reducido y menos complejo que CISC
SuperEscalar.
Paralelizan las aplicaciones según las especificaciones del programador.
Técnica utilizada por los procesadores multinúcleo para compartir memoria caché.
Si.
Ejecutarse en aquellos núcleos en que no se ejecuten otras aplicaciones.
Aumentando la frecuencia de reloj porque se consigue ejecutar un mayor número de\r\ninstrucciones por unidad de tiempo.
Aumenta debido a la necesidad existente de conocer cada núcleo de ejecución de\r\nforma individual.
Programar un sistema que gestione de forma eficaz la multitarea.
No, porque al ser NUMA cada hilo tiene su propio tiempo de acceso a memoria.
Es una topología irregular cuyos nodos pueden tener de n a 2n vecinos
Se aumenta la conectividad y se reduce el diámetro
Soporte para memoria DDR3
2.6 Ghz
Uno: el temporal
Los de colisión, los de capacidad y los de conflicto
Caché entrelazada
Con la segunda 
En cualquiera
Eso es falso. La TLB va entre la caché y la memoria principal
Es un anillo cordal en el que la conectividad del desplazador de barril es menor que la de cualquier anillo acorde con un grado de nodo mayor.
En sustituir cada vértice del cubo por un anillo, normalmente con un número menor de nodos que de dimensiones del cubo.
Los que, contra toda lógica, tienen las ramas más finas conforme nos acercamos a la raíz.
La reducción del diámetro de la red.
Primero la número 1, luego la número 0 y finalmente la número 2, debido a la ejecución fuera de orden
Búsqueda, decodificación y ejecución
Los campos Qj y Qk no pueden tener un valor de menos uno
En la ejecución especulativa las instrucciones deben ser emitidas en orden y graduadas en orden
Gracias al campo “dirección de memoria” del reorder buffer, que almacena la dirección de la instrucción. De esta manera se puede comparar si una instrucción va antes que otra en el código
En la etapa de graduación
WAW
Sí, reordenando el código
9
IC-->ID-->EX-->MEM-->WB
Ejecutar dos o más instrucciones en paralelo
Sólo existirían dependencias cuando la instrucción de la que depende el registro es un sw, puesto que se tiene que esperar un ciclo para tener el valor correcto del registro.
Si, si se quieren añadir un tipo de operaciones más a la ALU.
Mejor decodificación de la instrucción, pocos formatos de instrucciones, mejor gestión de acceso a memoria
Para los dos casos anteriores es igual de compleja
En el acceso de varios procesos a la memoria al mismo tiempo.
Es indiferente, ofrece el mismo rendimiento.
No es necesario, ya que la longitud del vector es conocida en tiempo de ejecución.
Disminuye a medida que aumenta el tamaño del vector
Porque no siguen las especificaciones dictadas por los diseñadores de computadores vectoriales.
Ninguna de las anteriores
No siempre, es posible hacer uso de máscaras para evitar realizar una operación sobre todos los elementos
Se comparten recursos de forma simultánea
Slight multithreading.
Es una evolución natural de las máquinas multithreading.
Una técnica para saber el estado del procesador.
Cuatro instrucciones.
Stream Instruction Single, Stream Data Multiple
Ninguna es cierta
MIMD
Ninguna es cierta
Precedió a la extensión MMX
Es una extensión para la arquitectura Intel que permite realizar operaciones MIMD
Se tiene el mismo tiempo de acceso a todas las palabras de memoria
“ROUND”, “FLOOR” y “CEIL”
Una variante de la arquitectura multicore.
Referenciar las direcciones “cercanas” a una posición de memoria accedida recientemente, poco tiempo después
El número de FPUs disponibles
Indica la operación a realizar
Único flujo de instrucciones sobre múltiples flujos de datos
Tiene un repertorio de instrucciones más amplio y menos complejo que CISC
El desarrollo de nuevos algoritmos intrínsicamente paralelos que permitan el\r\naprovechamiento del rendimiento de los procesadores multinúcleo.
El procesador mantiene una memoria de gran tamaño donde se almacenan los datos\r\nempleados por todos los núcleos de ejecución.
La interfaz de E/S.
Un problema de sincronización entre cachés en un procesador con HyperThreading.
No, pero siempre mantienen igual los componentes internos del núcleo.
Ejecutarse en p núcleos de ejecución consumiendo el mismo tiempo de cómputo en\r\ncada uno de ellos.
Aumentando la frecuencia de reloj si la aplicación ejecutada es secuencial o paralela.
No influye en absoluto.
En el de HyperThreading.
Ubicar en un mismo núcleo de ejecución a aquellos threads que acceden a los mismos datos.
Un tipo de topología irregular
Se reduce la conectividad y se aumenta el diámetro
Todas las anteriores
2.4 Ghz
Uno: el espacial
Los de colisión, los de arranque en frío y los de arranque en caliente
Reemplazamiento
Con ambas igual
Depende del número de conjuntos
Todas las anteriores son falsas
A partir de la configuración en anillo, añadiendo a cada nodo enlaces con los nodos que tengan una distancia potencia de 4.
La facilidad para ampliar el sistema.
Primero la número 1, y a la vez la 0 y la 2, debido a la ejecución fuera de orden y al paralelismo a nivel de instrucción
Que los operandos fuente de la instrucción en esa estación de reserva son direcciones de memoria
En la ejecución especulativa las instrucciones deben ser emitidas en orden, pero no tienen porqué graduarse en orden
En que no haya otros stores anteriores en el código y pendientes de ejecutarse que accedan a la misma dirección
Nada, solo es necesario que exista hueco en una estación de reserva
Ninguna
Es cierto, puesto que con anticipación se resuelve la dependencia WAR.
8
Reducir el número de instrucciones de un programa
Sólo habría que anticipar en caso de que el registro destino de una operación de memoria fuese el registro fuente de la siguiente instrucción, puesto que dependencias entre otro tipo de instrucciones no se dan por la particularidad de la etapa 2 y 3. 
Si, si nuestra ALU implementa operaciones en punto flotante.
Para solventar riesgos estructurales
Ninguna de las anteriores
Porque son más modernas que las máquinas memoria a memoria.\r\n
En el solapamiento de los distintos módulos de una memoria de forma segmentada.
Si, ya que no es necesaria para ningún tipo de operación en estas máquinas.
No es necesario, ya que la longitud del vector es conocida en tiempo de linkado.
Permanece constante en todo momento
Porque un compilador vectorial no tiene ninguna similitud con un compilador secuencial.
Encadenamiento
Ninguna de las anteriores es cierta
Una variante de la arquitectura multithreading.
Ninguna de las anteriores.
El bajo grado de emisión de las máquinas multithreading.
No repercutiría en una mejora significativa del rendimiento. 
Desperdicio vertical (vertical waste).
El necesario aumento de memoria principal con sus costes asociados.
Es una pauta que permite realizar cambio de contexto, pero que no\r\nasegura el cambio.
Se comparte entre todos los núcleos.
Single Instrucion Simple,Multi Data Stream
Ninguna es cierta
MISD
Incorpora nuevos tipos de datos
Non-Uniform Memory Access
CISC
Operación sobre la pila
Se tiene el mismo tiempo de acceso a las palabras de memoria de zonas múltiplos de 64 KB
Memoria no compartida
Referenciar las direcciones “lejanas” a una posición de memoria accedida recientemente, poco tiempo después
El número de ALUs disponibles
Ninguna es cierta
Tiempo antiguo*[(1 + fracción mejorada) + (aceleración mejorada/fracción mejorada)]
Tiene un repertorio de instrucciones más amplio y más complejo que CISC
Multinúcleo.
Paralelizan las aplicaciones según las especificaciones del compilador.
Estado de la CPU.
Necesita menor cantidad de información cuando se emplea memoria compartida\r\npara mantener la coherencia de memoria comparado con las arquitecturas de caché\r\nprivadas.
Un procesador simple con 3GHz. de frecuencia.
Aumentando el número de núcleos si la aplicación es secuencial.
Aumento de la eficiencia en el procesamiento de los datos por parte de un núcleo.
Programar un sistema que mantenga siempre coherencia entre cachés.
No, porque al ser NUMA cada hilo tiene reservado su propio espacio de direcciones.
Un tipo de topología n-cubo k-aria
Se reduce la conectividad y el diámetro
La pregunta no es cierta
A partir de la configuración en hipercubo, añadiendo a cada nodo enlaces con los nodos que tengan una distancia potencia de 2.
A k-aria.
igualmente efiente que Core 2 Quad
2.8 Ghz
La línea
Dos: el temporal y el espacial 
En el mapeado completamente asociativo
Escritura
En esta tecnología no se contemplan los reemplazos
Las instrucciones no se almacenan en memoria
Ninguna respuesta es correcta
Decodificación, ejecución y escritura retardada
Que los operandos fuente de la instrucción en esa estación de reserva ya han sido calculados
Se lo comunica la instrucción que ha generado los operandos por los que esperaba
Ninguna de las respuestas es cierta
Se emite a la estación de reserva correspondiente, pero se indica al reorder buffer que reserve el próximo hueco que quede libre para dicha instrucción\r\n
WAR
Es falso.
7
Decodificar la instrucción y leer del banco de registros
La productividad y la velocidad de ejecución de cada instrucción
Igual que el caso de la máquina segmentada de 5 etapas. 
Si, si la operación es de tipo de Memoria a Registro (no contemplada en la máquina segmentada normal)
Para solventar riesgos de control
Que el ensamblador se base en la máquina
Porque la arquitectura internas de las máquinas vectoriales con registros vectoriales ha sido más depurada que las de memoria a memoria.
En el solapamiento de los distintos módulos de una memoria.
Es vectorizable
No existe
No hay problema por usar un compilador secuencial.
Todas lo son
Se puede obtener rendimiento vectorial sin proporcionar suficiente ancho de banda de memoria
El Niagara utiliza 8 threads y grano fino.
Montecito es un procesador de doble núcleo y dos subprocesos de\r\nhardware por núcleo.
Uno.
Todas son falsas.
En eventos temporales y provocados por eventos.
Ninguna de las anteriores.
Permite pausar un proceso durante unos ciclos de reloj.
Es independiente para cada núcleo.
VLIW
Tiene un repertorio de instrucciones muy reducido
COMA
MIMD
Usa registros y formatos básicos distintos de los que utiliza MMX
Su objetivo es realizar operaciones en punto fijo
Modo registro
"AND","OR" y "XOR"
RISC
MIMD
Único flujo de instrucciones sobre un único  flujo de datos
Tiene un repertorio de instrucciones más reducido y más complejo que CISC
Todos lo son.
El procesador mantiene una memoria de gran tamaño donde cada núcleo posee una\r\nparte proporcional de la misma para almacenar los datos que está empleando.
No son capaces de paralelizar las aplicaciones.
Espacio de memoria.
Es más sencilla cuando se emplea memoria caché distribuida porque cada núcleo\r\núnicamente gestiona los datos de su memoria privada.
Sí y además proporciona un rendimiento superior al presentado en un núcleo\r\nhomogéneo.
Aumentando la frecuencia de reloj si la aplicación es secuencial y aumentando el\r\nnúmero de núcleos si la aplicación es paralela.
Menor volumen de comunicación en el bus de comunicación, reduciendo el cuello de botella y permitiendo en un futuro implantar más núcleos.
En el multiprocesador.
No, porque al ser NUMA cada núcleo tiene su propia memoria.
Un tipo de topología no ortogonal
Es simétrica
más eficiente que Core 2 Quad
2.2 Ghz
La columna
Tres: el temporal, el principio de velocidad de acceso y el global
En el mapeado directo
Coherencia 
256 fallos y 86 reemplazamientos con TLB, 253 fallos y 89 reemplazamientos sin TLB
Modified, si se aplica el protocolo MSI
Sí
A partir de la configuración en anillo, añadiendo a cada nodo enlaces con los nodos que tengan una distancia potencia de 2.
En sustituir cada vértice del cubo por varios anillos, normalmente con un número igual de nodos que de dimensiones del cubo.
Los que tienen una red de interconexión estática porque los conmutadores no cambian las conexiones entre los nodos.
A anillo.
Las estaciones de reserva
La tasa de aciertos de los mejores predictores dinámicos de saltos no supera en ningún caso el 80%
El salto retardado es una técnica dinámica de saltos
En la ejecución especulativa las instrucciones deben ser emitidas en orden y ejecutadas en orden
No debe fijarse en nada, el load se puede ejecutar sin que ello suponga problema alguno
Ninguna respuesta es cierta
RAW
Es falso (siempre y cuando no haya anticipación).
Ejecutar una operación o calcular direcciones de memoria
La velocidad de ejecución de cada instrucción
No existirían tales dependencias, puesto que se asegura que una instrucción no puede comenzar antes de que termine la anterior.
No.
Para solventar riesgos de datos
Que la máquina se base en el ensamblador
Registros (vectoriales y escalares), unidades funcionales (vectoriales y escalares), y unidad de carga y almacenamiento.
En el acceso simultáneo a memoria.
No es vectorizable, porque hay una dependencia en S1
Puede ser mayor que la longitud de los registros vectoriales
Nada, ya que se permite la adicción y eliminación dinámica tanto de unidades funcionales como de registros en las máquinas vectoriales.
Porque el programador del compilador vectorial tiene una concepción distinta del vector que el programador del compilador secuencial.
Reducción vectorial
Los gastos de arranque pueden siempre despreciarse
No es una variante de ninguna arquitectura.
Ninguna de las anteriores es correcta.
Repercutiría negativamente en el rendimiento de un único thread.
Dependencias de datos.
Duplica el proceso actual y a continuación duerme el proceso que creó\r\nel último proceso.
Tiene un tipo de escritura directa.
CISC
NUMA
SIMD
También se le conoce como  MMX-2
Ninguna es cierta
Modo inmediato
Memoria segmentada
CISC
Ninguna es cierta
Múltiples flujos de instrucciones sobre un único flujo de datos
No tiene repertorio de instrucciones propio
No, todos los núcleos deben ser procesadores secuenciales.
Una topología de red.
Pila de ejecución.
Únicamente necesita conocer qué núcleos acceden en un determinado instante de\r\ntiempo a los datos en caso de que se emplee memoria compartida.
Un procesador con HyperThreading con 2.8GHz. de frecuencia.
La aceleración obtenida empleando un procesador multinúcleo está determinada\r\npor la cantidad de código paralelizable del programa a ejecutar.
Menor volumen de comunicación en el bus interno del núcleo, reduciendo el cuello de botella y permitiendo en un futuro implantar más núcleos.
Salto retardado
El víctim buffer
L1 y L2
Programar un sistema que asegure la prioridad de las ejecuciones aún cuando se\r\ndispongan de varios núcleos.
Emplear núcleos heterogéneos que agilicen los accesos a memoria en threads de la misma aplicación.
Un tipo de topología de árbol
Es compleja de implementar
menos eficiente que Core 2 Duo
El Quad tiene un mayor consumo energético
El nivel
Sólo uno
En el mapeado asociativo por conjuntos
Recuperación de fallo
256 fallos y 89 reemplazamientos con TLB, 253 fallos y 86 reemplazamientos sin TLB
Invalid, si se aplica el protocolo MSI 
Sólo si es encadenada
A partir de la configuración en hipercubo, añadiendo a cada nodo enlaces con los nodos que tengan una distancia potencia de 4.
En sustituir cada vértice del cubo por un anillo, normalmente con un número igual de nodos que de dimensiones del cubo.
El que se introdujo para aliviar el problema grave consistente en el cuello de botella que se produce cerca de la raíz en los árboles de estrella.
A malla.
Las estaciones de reserva y la etapa de graduación
El reorder buffer
Búsqueda, decodificación y escritura
Ninguna de las respuestas es correcta
Se bloquea la emisión de instrucciones hasta que haya hueco
WAW
Es cierto, puesto que el sw escribe en la posición de memoria antes de que ésta sea leída por el lw.
Acceder a memoria (lectura o escritura)
La productividad
Si, puesto que las máquinas segmentadas siempre emiten una instrucción por ciclo.
Para solventar riesgos de datos y control
No, es indiferente 
Registros vectoriales, unidades funcionales vectoriales y unidad de carga y almacenamiento.
El acceso a una palabra en un único ciclo de memoria.
No es vectorizable, porque hay una dependencia en S2
Combinar el entrelazado de los procesadores en los buses en m vías y operar con los n buses en paralelos, para seleccionar m • n palabras por ciclo.
Que comienzan a surgir conflictos entre las diferentes operaciones vectoriales.
Depende de la forma de codificar del programador, y si el compilador detecta correctamente todas las posibles vectorizaciones.
Sentencias ejecutadas condicionalmente y matrices dispersas
Ninguna es cierta
Se comparten recursos de forma alterna.
Montecito es un procesador de un núcleo y dos subprocesos de hardware por núcleo.
Cuatro.
La sobrecarga del ciclo de reloj al disponer de más instrucciones.
En eventos fijos y provocados por eventos.
Cada 4 hilos son agrupados en un “hilo-grupo” en donde comparten el\r\npipeline.\r\n
Todas las afirmaciones son falsas.
RISC
Cada instrucción nunca se ejecutan en una unidad funcional
UMA
MISD
Sólo existirían dependencias cuando el registro es el destino de dos operaciones (un lw seguida de otra cualquiera no lw), y habría que incluir una burbuja.
Registro - Registro
Ninguna es cierta
Se tiene el mismo tiempo de acceso a las palabras de memoria de zonas múltiplos de 128 KB
"JE","JBE" y "JNE"
VLIW
4
Múltiples flujos de instrucciones sobre múltiples flujos de datos
Aquel procesador que posee una unidad de procesamiento y varios núcleos de caché.
No, un hilo no puede ser paralelizado mediante ILP.
Un bus.
Ninguno de los anteriores.
No, pero siempre mantienen igual los componentes externos del núcleo.
Sí porque núcleos de carácter específico tienen la capacidad de ejecutar todo el\r\nrepertorio de instrucciones empleado por el procesador.
Los procesadores multinúcleo aceleran de forma automática la ejecución de una\r\naplicación de manera transparente al programador.
Ninguno.
En el dual core.
Sí, porque acceden a la vez a la misma posición de memoria.
Sus nodos no siempre tienen el mismo número de vecinos
Generalmente es ineficiente
menos eficiente que Core 2 Quad
Sus nodos tienen K vecinos
Los predictores de torneo son un tipo de predictores que deciden la predicción escogiendo entre varios tipos de predictores
A diferencia del Quad, el X4 es un procesador monolítico
Los fallos de caché por colisión no dependen del mapeado
El tiempo medio que está una palabra en caché antes de ser reemplazada
253 fallos y 89 reemplazamientos con TLB, 256 fallos y 86 reemplazamientos sin TLB 
Shared, si se aplica el protocolo MSI
Sólo si no es encadenada
Que se cumpla que |j − i| = 2r
Reemplazando cada vértice del hipercubo adimensional por un anillo de k nodos.
Una forma práctica de implementar árboles con canales de mayor ancho de banda en la vecindad del nodo raíz.
A toro. 
El Common Data Bus y múltiples unidades funcionales
La caché de instrucciones
Aparecen de la misma manera que en los procesadores segmentados
Los predictores de saltos no tienen gran influencia en una máquina superescalar, ya que la misma es capaz de ejecutar instrucciones de manera especulativa
Los predictores locales tienen en cuenta el resultado de otros saltos a la hora de realizar sus predicciones
Mediante el sondeo del bus donde se escriben los resultados de la ejecución de las instrucciones, el Common Data Bus (CDB)
Gracias al campo “Ready”, que se actualiza sondeando el Common Data Bus
Ninguna
No es cierto aun habiendo anticipación o burbujas.
Escribir en el banco de registros
La complejidad del hardware
Este tipo de dependencias no se darían si la máquina realiza renombrado de registros, si no se podrían evitar anticipando el resultado de la primera instrucción a la etapa 2 de la segunda instrucción. 
Si, si tenemos en cuenta que no existen instrucciones de salto en dicha máquina que bloqueen el cauce del programa.
Para resolver los riesgos de control
La máquina podría delegar o no en el compilador la resolución de dependencias y así asegurar una ejecución correcta, pero el compilador debe basarse en la máquina para conseguir siempre el mejor rendimiento.
Registros (vectoriales y escalares), unidades funcionales (vectoriales y escalares).\r\n
El acceso a varias palabras en un único ciclo de memoria.
No, ya que los datos que están recogidos en los registros (vectoriales o escalares) proceden de la memoria, y el resultado se almacena ocasionalmente en memoria y en un registro para aprovecharlo en operaciones posteriores.
Indica la longitud de los registros vectoriales
Que si se utilizan pocas unidades funcionales, puede ocurrir que se colapse la máquina al necesitar más unidades funcionales de las disponibles.
No, porque un programador conoce mejor el programa que ha codificado y es capaz de obtener con mejores resultados las partes a vectorizar del mismo.
Son inexistentes
El rendimiento de las máquinas escalares segmentadas puede ser superior si el nivel de vectorización no es muy alto
Una variante de la arquitectura paralela.
Combina tecnología multicore y multithreading.
16Kb, 64Kb, 10Mb.
Repercutiría positivamente en el rendimiento de un único thread.
En cambios de contextos.
Cada 6 hilos son agrupados en un “hilo-grupo” en donde comparten el\r\npipeline.\r\n
2 de lectura y 3 de escritura.
NUMA
Ninguna es cierta
Ninguna es cierta
SISD
Ninguna es cierta
Es una extensión para la arquitectura Intel que permite realizar operaciones SIMD
Modo directo
Conversión
EPIC
3
Emplean un conjunto de instrucciones diseñadas para explotar el paralelismo\r\ndel hardware\r\n
Aquel sistema compuesto por varios procesadores.
Si, cada núcleo puede ser a la vez HyperThreading.
Cada uno de los núcleos de ejecución mantiene una pequeña memoria donde\r\nalmacena los datos empleados únicamente por el núcleo en cuestión.
No están conectados.
Porque los multiprocesadores no tienen la capacidad para realizar las tareas de un PC.
No, pueden variar tanto los componentes internos como externos del núcleo.
Un procesador multinúcleo con 2 núcleos a 2GHz. cada uno.
La presencia de código secuencial en una aplicación no supone una limitación en la\r\naceleración que se puede obtener en la ejecución de la misma.
Destreza del programador, librerías de funciones optimizadas e inteligencia de\r\ncompiladores de lenguajes de alto nivel.
Todas son correctas.
No, porque al ser de paso de mensajes cada hilo tiene su propio tiempo de acceso a\r\nmemoria.
Es un bus
Desde 2.60 GHz hasta 3.00 GHz
El X4 tiende a calentarse más
Cachés con correspondencia directa, caché totalmente asociativa y caché asociativa por conjuntos. 
Normalmente dos o tres
El tiempo acceso a memoria caché 
Paralelismo a nivel de bucles
El common data bus (CDB)
Ninguna de las afirmaciones es correcta
Cuando Qj y Qk valen cero
Mediante el sondeo de los registros que contendrán los operandos fuente
Gracias al campo “Address”, que se actualiza cuando el resultado de la instrucción es calculado en alguna unidad funcional
No se podría realizar register renaming, y por tanto, una instrucción al entrar en una estación de reserva no podría realizar el sondeo del Common Data Bus (CDB)
WAR
Es cierto si hay anticipación o burbujas
Obtener la instrucción de la memoria utilizando el CP
Si, si el número de etapas es muy pequeño
Igual que el caso de la máquina segmentada de 5 etapas. 
Si, si tenemos en cuenta que la máquina posee un mecanismo de prevención de riesgos de datos en el que no pierde ningún ciclo.
Para aumentar el rendimiento de la máquina
Restricciones de las instrucciones que pueden ir en el hueco y capacidad de predicción en tiempo de ejecución
Registros (vectoriales y escalares), unidades funcionales escalares, y unidad de carga y almacenamiento.
El acceso a varias palabras en varios ciclos de memoria.
No es vectorizable, porque hay dependencias en S1 y S2 
Combinar el entrelazado de los módulos en m vías dentro de los buses, y operar con los n buses en paralelos, para seleccionar m • n palabras por ciclo.
Sólo si el número de bancos de memoria es igual al tiempo de acceso a memoria en ciclos de reloj
Sí, ya que utiliza técnicas de descubrimiento de áreas a vectorizar bastante eficientes capaces de detectar áreas que no puede detectar el programador.
Son frecuentes, debido a que se desconoce el comportamiento de los saltos
Mayor velocidad en la búsqueda y decodificación de las instrucciones, en comparación con los procesadores vectoriales
Se comparten recursos de forma asíncrona.
Combina tecnología paralela y multicore.
32Kb, 512Kb, 16Mb.
64.
Que se ejecute un flujo de instrucciones de hardware por procesador.
Todos los hilos son independientes.
3 de lectura y 2 de escritura.
Ninguna es cierta
Almacena el byte más significativo en la dirección más baja de memoria
Incorpora nuevas instrucciones 
Todas son ciertas
Non-Unique Management Access
Se introdujo en la familia Intel a partir de los procesadores Pentium II
Ninguna es cierta
Lógicas
UMA
6
Código de operación
No pueden ejecutar instrucciones en paralelo
Aquel procesador que posee varias unidades de procesamiento en un mismo chip.
Si, cada núcleo puede usar un paralelismo ILP (Superescalar, VLIW, etc.).
Uno de los núcleos de ejecución mantiene una memoria de gran tamaño donde se\r\nalmacenan los datos empleados por todos los núcleos del procesador.
Un anillo.
Porque el software de usuario no está preparado para ejecutar en multiprocesadores.
Reduce los recursos inutilizados cuando se emplea memoria físicamente distribuida\r\nporque la memoria no utilizada en un instante de tiempo puede ser empleada por el\r\nresto de los núcleos de ejecución.
No porque los núcleos específicos únicamente pueden ejecutar aquellas\r\ninstrucciones para las que han sido diseñados.
Es necesario paralelizar completamente una aplicación para poder obtener una\r\naceleración superior en un procesador multinúcleo.
Capacidad de intercambio de threads del sistema operativo.
No depende de con que esté programada, sino como esté programada.
No, porque al ser de paso de mensajes cada hilo tiene reservado su propio espacio de\r\ndirecciones.
Todos sus nodos tienen el mismo número de vecinos
Un N-cubo K-ario con K = 2
Desde 2.80 GHz hasta 3.00 GHz
El Quad tiene una mayor frecuencia de reloj
Cachés con correspondencia directa, caché asociada al tamaño y caché asociativa por conjuntos
Al menos tres niveles
Cuanto más grandes los bloques, menos fallos forzosos y más fallos de colisión
El tiempo que tarda en copiarse un bloque de memoria principal a caché
253 fallos y 86 reemplazamientos con TLB, 256 fallos y 89 reemplazamientos sin TLB 
El reorder buffer y la etapa de graduación
No se garantiza de ninguna forma, al final de la ejecución el registro F6 contiene (R2)
Ninguna de las respuestas es correcta
Ninguna de las anteriores
Ninguna respuesta es correcta
Sí se podría realizar register renaming; el RegisterStat sólo es un buffer intermedio utilizado para acelerar la ejecución
RAW
Es cierto aun sin anticipación o burbujas.
6
Si, si el número de etapas es muy grande
Sí, si el código con el que fue evaluada no generaba bloqueos.
Para aumentar la velocidad de ejecución de las instrucciones de salto
Restricciones de las instrucciones que pueden ir en el hueco y pipelines más largos
El acceso a una palabra en varios ciclos de memoria.
No, ya que los datos que están recogidos en los registros vectoriales únicamente proceden de la memoria, y se almacena el resultado en el registro vectorial correspondiente.
Indica la longitud de la operación vectorial que se está realizando 
Nada, ya que existe un valor por defecto del número de unidades funcionales.
Tanto la vectorización manual como la automática obtienen resultados similares, sin distinción.
No segmentan las operaciones, al contrario que los procesadores vectoriales
El Niagara utiliza 16 threads y grano grueso.
Es una máquina compuesta por 8 procesadores interconectados.
32Kb, 256Kb, 8Mb.
32.
Que los procesadores sean paralelos en ejecución.
Light CPU.
Los hilos se agrupan según requerimientos de la aplicación.
4 de lectura y 1 de escritura.
Es una arquitectura avanzada de VLIW de alto rendimiento
Almacena el byte menos significativo en la dirección más alta de memoria
Bajo coste
Empaquetado de instrucciones
Registro - Memoria
El inverso de la latencia
Modo indirecto
Salto
NUMA
5
Operandos fuente
Tienen un repertorio de instrucciones más simple que los procesadores RISC
Aquel procesador que simula la ejecución de varios procesos.
A medida que la frecuencia disminuye.
El procesador mantiene una memoria de gran tamaño donde se almacenan los datos\r\nempleados por todos los núcleos de ejecución.
El lenguaje de programación empleado.
Por el alto coste y el sistema de refrigeración que requieren los multiprocesadores frente a los multinúcleo.
Núcleos de similares características entre sí.
Depende del número de aplicaciones que el procesador esté ejecutando en un\r\ndeterminado instante de tiempo.
Se emplea el protocolo MESI, el cual da soporte más eficientemente a write-back en\r\nlugar de write-through de caché de CPU.
Empleo de HyperThreading.
10.
No, porque al ser de paso de mensajes cada núcleo tiene su propia memoria y los\r\ndatos que comparte es mediante rutinas de envío y recepción.
Sus nodos tienen n vecinos
Una malla N-dimensional con K distinto para cada N
Athlon 64 X2
Desde 2.90 GHz hasta 3.30 GHz
Alta
Cachés con correspondencia directa, caché totalmente asociativa y caché asociativa por palabras
Piramidal
Cuanto más pequeños los bloques, menos fallos forzosos y más fallos de colisión
El tiempo que se ahorra utilizando critical-word-first
128 conjuntos
Owned, si se aplica el protocolo MOSI
No
Que se cumpla que |j + i| = 2r
Reemplazando 2 vértices del hipercubo adimensional por un anillo de k nodos.
El que tiene dos niveles con un alto grado de nodo.
número de enlaces afecta el coste de la red.
Paralelismo a nivel de thread
Buffer de predicción de saltos
Se ejecuta primero ADDF pero se retrasa la escritura de su resultado hasta que escriba su resultado LF
Cuando Qj y Qk valen distinto de cero
No es necesario retardarlas, las excepciones pueden ser lanzadas en la etapa de ejecución
Gracias al campo “Qj”, que indica que la instrucción está lista para ser ejecutada
En una máquina con pocas etapas de segmentación
El comportamiento de la máquina es el mismo pero sólo se podría emitir como máximo una instrucción por ciclo de reloj
WAW
Ejecutar una operación o calcular direcciones de memoria\r\n
Si, si el número de etapas es coherente
Se necesitaría una nueva estrategia para tratar las dependencias de control, ya que ahora cambia la implicación de los saltos de la etapa IF (máquina segmentada de 5 etapas) a la etapa 1 de la nueva máquina.
Cuando la unidad de anticipación y la unidad de control no puedan solventar un riesgo de bloqueo del cauce.
Todas las anteriores
Pipelines más largos, capacidad de predicción en tiempo de ejecución, emisión de múltiples instrucciones por ciclo
Sin utilizar cachés y sin segmentar los accesos a memoria
No, ya que los datos que están recogidos en los registros (vectoriales o escalares) proceden de la memoria, y el resultado se almacena en memoria y previamente en un registro para aprovecharlo en operaciones posteriores.
Combinar el entrelazado de los buses en los módulos en m vías y operar con los n módulos en paralelos, para seleccionar m • n palabras por ciclo.
Si el número de bancos de memoria es, como mínimo, tan grande como el tiempo de acceso a memoria en ciclos de reloj
SISD
Son inevitables
Reduce la limitación de la duración del ciclo de reloj característica de los procesadores segmentados
Combina hardware multithreading con tecnología superescalar.
16.
Que una instrucción se pueda dividir en ambos cores.
Un objeto de CPU repetido y dependiente del SO y del sistema.
1 de lectura y 4 de escritura.
Permite el empaquetamiento de instrucciones
Almacena el bit menos significativo en la dirección más baja de memoria
Incorpora nuevos tipos de datos
Tamaño variable de los datos en las instrucciones
Memoria - Memoria
Su objetivo es acelerar el procesamiento de aplicaciones de comunicación y multimedia
Modo base + desplazamiento
Aritméticas
MIPS
Sí
Operando destino
Ninguna es cierta
Paralelismo a nivel de thread.
Level CPU.
A medida que la integración disminuye.
El procesador mantiene una memoria de gran tamaño donde cada núcleo posee una\r\nparte proporcional de la misma para almacenar los datos que está empleando.
El algoritmo empleado.
Porque los multinúcleo son mucho más veloces.
Pentium D.
Los 3 por igual.
Se emplean protocolos basados en directorios que mantienen un directorio\r\ncentralizado de bloques que hay en las cachés.
Generación adecuada de código orientado a threads.
No, porque no se pueden combinar ambas librerías.
No se mejora el rendimiento.
Es un caso particular de los ciclo cubos conectados
Un N-cubo K-ario con K distinto para cada N
Opteron X2
Desde 2.80 GHz hasta 3.20 GHz
Normal
Cachés con correspondencia indirecta, caché totalmente asociativa y caché asociativa por conjuntos
De red
Cuanto más pequeños los bloques, más fallos forzosos y más fallos de colisión
No se registran reemplazamientos
256 conjuntos
Que el número de líneas del directorio puede ser limitado
Sí
Que se cumpla que |j − i| = 4r
Reemplazando cada lado del hipercubo adimensional por un cubo de k nodos.
El que tiene un nodo raíz conectado a un cierto número de nodos descendientes.
La propiedad de simetría afecta a la escalabilidad y a la eficiencia en el rutado.
La etapa de emisión
Predecir el salto como no tomado
Las máquinas superescalares realizan una planificación estática de instrucciones
Cuando Qj y Qk valen menos uno
Porque dicha excepción puede estar generada por una instrucción que no se debería de haber ejecutado porque un salto anterior fue mal predicho
Que no haya otros stores pendientes de ejecutarse y anteriores en el código que accedan a la misma dirección
Siempre en 2 estados
Ninguna
El de la suma
8
Acceder a memoria (lectura o escritura)
No, pero sí se obtendría un valor aproximado (siempre peor al ideal)
Al reducir las etapas IF y ID en una sola, la predicción de saltos se realiza en un único ciclo, por lo que para la siguiente instrucción se sabe la dirección de salto y no se daría situaciones en las que se desconozca el flujo del programa. 
Cuando la unidad de anticipación no pueda solventar un riesgo de bloque del cauce
En una máquina con muchas etapas de segmentación
Ninguna complicaría más el uso de saltos retardados
Una unidad escalar segmentada y una unidad vectorial
Sin utilizar cachés y segmentando los accesos a memoria
Es vectorizable si el compilador no relee A en S2 viendo que hay dos accesos seguidos
Combinar el entrelazado de los buses en los módulos en m vías y operar con los n módulos en paralelos, para seleccionar m / n palabras por ciclo.
Si el número de bancos de memoria es menor que el tiempo de acceso a memoria en ciclos de reloj
MISD
Sí, debido a que una operación vectorial equivale a ejecutar un conjunto de operaciones sobre múltiples datos de forma simultánea, con el correspondiente ahorro de tiempo. 
Todas las anteriores son ciertas
El Niagara utiliza 32 threads y grano fino.
Combina hardware multithreading con tecnología SMP.
16Kb, 256Kb, 12Mb.
128.
Desperdicio horizontal (horizontal waste).
Un objeto de CPU único e independiente ante el SO y el sistema.
Más de 10 Gb/s.
Posee Branches “Fully Pipelined”
Almacena el byte más significativo en la dirección más alta de memoria
Dimensiones reducidas
Operación de forma condicional
Ninguna es cierta
EPIC
El tiempo de acceso a memoria varía según el lugar donde se encuentre localizado el dato
N = número de instrucciones del programa\r\nn = número de ciclos que tarda en ejecutarse una instrucción\r\nt = tiempo de ciclo\r\n
PlayStation
No
Instrucción siguiente
RISC
Paralelismo a nivel de thread simulado.
A medida que el calor aumenta.
Tecnología de comunicaciones que ofrece un bus avanzado de alta velocidad y alto desempeño y que aumenta el rendimiento del procesador.
El compilador empleado.
Lines CPU.
En ILP se ejecutan las instrucciones en paralelo, mientras que en TLP lo que se ejecutan en paralelo son tareas.
Núcleos con características específicas.
Los procesadores multinúcleos habitualmente se fabrican empleando únicamente\r\nnúcleos de carácter específico.
Se emplean protocolos basados en directorios donde las transiciones entre estados\r\npara la actualización de los datos se realiza de forma independiente para cada núcleo.
El de CELL, ya que al ser un procesador híbrido y el programa estar optimizado para su uso, las tareas que se realizarán de forma específica y ofrecerá un rendimiento mayor.
Sí, porque se puede aprovechar MPI para paralelizar entre procesadores y OpenMP para paralelizar entre núcleos.
Sí, porque acceden a la vez a la misma posición de memoria.
Es una topología regular y simétrica
Ninguna respuesta de las anteriores es correcta
Turion X2
Tener menor rendimiento que el AMD FX-62
Baja
Escritura directa y postescritura
Paralela
Cuanto más grandes los bloques, más fallos forzosos y más fallos de colisión
No utilizan algoritmos de reemplazamiento
32 conjuntos
Las máquinas superescalares no implementan ningún tipo de paralelismo
Aquella donde el hardware reorganiza la ejecución de instrucciones para reducir las esperas manteniendo el resultado esperado por el programador
Cuando se emite LF se anota que ella va a escribir su resultado en F6; como a continuación se emite ADDF, se sobreescribe la anotación para indicar que será ADDF quien escriba finalmente su valor en F6, por lo tanto, LF no escribirá nunca su resultado en F6
Ninguna de las anteriores
Que no haya otros loads pendientes de ejecutarse y anteriores en el código que accedan a la misma dirección
En 2^n/2 estados
WAR y WAW
El del load
7
Escribir en el banco de registros
Es apropiado utilizar una arquitectura HARDVARD, es decir, memorias separadas para código y datos.
Cuando no quede más remedio
CISC
Si, si al desenrollarlo podemos reordenar instrucciones independientes entre si que antes del desenrollado no lo eran.
Utilizando cachés y sin segmentar los accesos a memoria
Registro - Registro
RISC
Es vectorizable si el compilador almacena A y lo relee en S2; y se consigue una ejecución correcta
Esta combinación nunca ocurre
Si la operación que se realiza no es de doble precisión
SIMD
Sólo en algunas ocasiones.
Sólo cuando el valor de A(i) sea menor que B(i)
El Niagara utiliza 32 threads y grano grueso.
Emisión múltiple.
Aumento del número de conflictos de caché. 
Desperdicio vertical (vertical waste).
Un objeto de CPU único y dependiente ante el SO y el sistema.
Igual a 20 Gb/s.
Todas son ciertas
Ninguna es cierta
Bajo consumo de energía
Branches “Fully Pipelined” (sin pérdidas de ciclos al ejecutar los branches)
Non-Uniform Management Access
Todas son ciertas
Todas son ciertas
N = número de instrucciones del programa\r\nn = número de etapas del pipeline\r\nt = tiempo de ciclo\r\n
FPU
Todas son ciertas
CISC
Paralelismo a nivel de proceso.
A medida que la frecuencia aumenta.
Tecnología similar al HyperThreading inventada por AMD.
Ninguna de las anteriores.
ILP puede paralelizar programas secuenciales, cosa que no puede hacer TLP.
Core 2 Duo.
Porque generan más calor que los de carácter generalista.
La proximidad de los núcleos en el mismo dispositivo, permitiendo que las señales\r\nno se degraden en exceso.
Soporta hasta 32 hilos.
El procesador Niagara (UltraSPARC T1) implementa ILP (Instruction Level Parallelism).
El del homogéneo, ya que al ser un procesador homogéneo y el programa estar optimizado para su uso, cualquier procesador puede realizar las tareas y ofrecerá un rendimiento mayor.
Sí, porque se puede aprovechar OpenMP para paralelizar entre procesadores y MPI para paralelizar entre núcleos.
Porque se reducen los tiempos de acceso a las distintas posiciones de memoria.
Es la generalización de un N-cubo K-ario
a,b y c
Ninguna de las anteriores
Tener un rendimiento similar al AMD FX-62
Muy Baja
Postescritura y escritura previa
No existe una estructura para la memoria caché
Iguales en todos los casos
No requieren políticas de escritura
16 conjuntos
Que será necesario un sistema de ficheros
Sí, pero sólo si sigue la arquitectura Harvard
Que se cumpla que |j + i| = 4r
Reemplazando cada vértice del hipercubo adimensional por un cubo de k nodos.
El que tiene un diámetro resultante grande y constante.
La anchura de banda biseccional puede mejorarse con un canal más ancho.
Paralelismo a nivel de instrucción
Ninguna respuesta es correcta
El algoritmo de Tomasulo nunca llegó a ser implementado en una máquina real; es usado solamente en simulación, con fines académicos
Los valores de los operandos fuente de la instrucción en esa estación de reserva si ya se conocen
Se seguirán emitiendo instrucciones como si el salto fuera tomado, y si finalmente no se toma, se vacían las estaciones de reserva
Que no haya otros loads ni stores pendientes de ejecutarse y anteriores en el código que accedan a la misma dirección
En n/2 estados
WAR y RAW
Ninguno de los anteriores
10
Obtener la instrucción de la memoria utilizando el CP
Es preferible disponer de complejos modos de direccionamiento para acelerar el acceso a memoria.
Determinar las relaciones de dependencia entre estas instrucciones, adecuar los recursos hardware a la ejecución de varias instrucciones en paralelo y diseñar estrategias que permitan determinar cuando una instrucción está lista para ejecutarse.
Cuando la unidad de control no pueda solventar un riesgo de bloqueo del cauce. 
En una máquina sin estrategias de control
No, puesto que el número de instrucciones que se ejecutarán será siempre el mismo
Utilizando cachés y segmentando los accesos a memoria
Sería vectorizable si se emplea la opción de la respuesta anterior, pero no se conseguiría una ejecución ordenada
CISC
No, ya que la multiplicación escalar de un vector fila con un vector  columna da un escalar y en este caso se almacena en un registro escalar.
MIMD
Siempre es preferible ejecutar una secuencia de operaciones escalares equivalentes.
Sí, la suma de dos elementos siempre es vectorizable\r\n
La tecnología SMT se basa en la arquitectura paralela.
La longitud de las palabras es de 64 bits.
Repercutiría negativamente en el rendimiento de un único thread.
Desperdicio vertical y desperdicio horizontal.
Logic CPU.
Un objeto de CPU repetido e independiente del SO y del sistema.
Más de 20 Gb/s.
EPIC
Multiple Instruction Stream, Multiple Data Stream
Incorpora nuevos registros
Todas son ciertas
Ninguna es cierta
El inverso de la productividad
El número de programas ejecutados por unidad de tiempo
N = número de ciclos que de cada etapa del pipeline\r\nn = número de etapas del pipeline\r\nt = tiempo de ciclo\r\n
Ninguna de las anteriores
EPIC
VLIW
Paralelismo a nivel de instrucción.
Tipo de topología entre procesadores en un sistema multiprocesador.
Mediante una ejecución paralela.
Un procesador ILP puede ser un componente de un procesador TLP.
Núcleos homogéneos y núcleos específicos.
Porque no pueden ejecutar todas las instrucciones del repertorio de instrucciones.
No. la ley de Amdahl limita a un máximo de aceleración de p veces con respecto a la\r\nversión secuencial.
0 %.
No, porque el rendimiento sería el mismo que programando con uno sólo de ellos.
Porque se reduce considerablemente la comunicación entre núcleos.
Es la generalización de una malla n-dimensional
a,b y d
1
Tener mayor rendimiento que el AMD FX-62
Aproximadamente un 25%
Escritura directa y escritura indirecta
Fallo forzado
Más grandes cerca del procesador, para mejorar el paralelismo
Incorporan su propia política de coherencia de caché
Añadiendo un buffer de instrucciones entre L1 y L2
Que el encadenamiento de punteros entre directorios puede generar bucles infinitos
No, pero sólo si sigue la arquitectura Harvard
El grado de cada nodo es d = 2n + 2 y un diámetro D = n/2.
En que el grado de nodo es siempre 6, independientemente de la dimensión del hipercubo, por lo que resulta una arquitectura escalable.
El que se suele utilizar en sistemas con un supervisor que hace de nodo periférico.
Sí, siempre
Aquella donde el compilador reorganiza la ejecución de instrucciones para reducir las esperas manteniendo el resultado esperado por el programador
Los índices de las estaciones de reserva cuyas instrucciones producirán los operandos fuente de nuestra instrucción
Ninguna de las respuestas es correcta
Que no haya otros stores pendientes de ejecutarse y posteriores en el código que accedan a la misma dirección
En n-1 estados
WAW y RAW
El de la resta
9
Decodificar la instrucción y leer del banco de registros
El problema de las dependencias en el pipeline las debe resolver el compilador.
Las mismas que la respuesta (a) más el diseño de técnicas de acceso a memoria/registro y registro/memoria eficientes
13 ciclos con anticipación, 17 sin anticipación
En una máquina con anticipación
Sí, más que en los procesadores superescalares o vectoriales
Vectorización de bucles.
No es vectorizable en ningún caso
Se deben hacer agrupaciones de registros para abarcar estos vectores
No, porque en el caso de que no queden registros vectoriales libres, se almacena en uno escalar.
Realizan operaciones sobre vectores
Nunca
No, porque A(i+1) depende de A(i)
Es hyperthreading una implementación de la tecnología SMT.
SMT hace uso de planificación estática.
 Ninguna de las anteriores es correcta
No repercutiría en una mejora significativa del rendimiento. 
Ninguna de las anteriores es correcta.
La alimentación eléctrica del procesador.
Igual a 30 Gb/s.
Multi Instruction Single, Multi Data Stream
Procesado en tiempo real de señales de alta frecuencia
2^n
El número de instrucciones ejecutadas en el pipeline por unidad de tiempo
N = número de ciclos que de cada etapa del pipeline\r\nn = número de instrucciones del programa\r\nt = tiempo de ciclo\r\n
PC convencional
VLIW
EPIC
Coprocesador matemático (FPU).
Proceso generado a partir de otro proceso.
Es proporcional al número de núcleos del procesador empleado.
Mediante software.
Todas son correctas.
Intel 7i.
Porque son más caros de fabricar que sus homólogos generalistas.
Sí. Los núcleos pueden ejecutar menos cantidad de código en la versión paralela que\r\nen la versión secuencial debido a los fallos de caché, optimizaciones del compilador,\r\netc.
El del homogéneo, ya que al ser un procesador homogéneo y el programa estar optimizado para su uso, las tareas que se realizarán de forma específica y ofrecerá un rendimiento mayor.
Desconocido.
Porque el compilador puede realizar una planificación estática de los accesos a memoria.
N-cubo K-aria
b,c y d
8
Ninguna de las anteriores
Aproximadamente un 10%
Escritura directa y borrado directo
El procesador Niagara implementa TLP (Thread Level Parallelism).
Fallo de capacitación 
Los buses son demasiado lentos y por eso no se usan en las memorias caché
Verdadero
Basándose en la arquitectura Harvard (generalmente en L1)
No presenta problemas, pero es más eficiente combinada con Snooping
No
El grado de cada nodo es d = 2n − 1 y un diámetro D = n x 2.
En que el grado de nodo es siempre 3, independientemente de la dimensión del hipercubo, por lo que resulta una arquitectura no escalable.
Conectando todos los nodos sin eliminar ningún enlace.
Con la utilización de técnicas segmentadas en el encaminamiento, la reducción del diámetro de la red es un objetivo primordial.
Sólo en el caso en el que la máquina superescalar haga ejecución especulativa
Una técnica que permite que las instrucciones sean ejecutadas en orden en el procesador aunque el compilador las haya desordenado para optimizar el almacenamiento de las mismas
El predictor gselect-best consigue una tasa de aciertos del 100%
Se seguirán emitiendo instrucciones como si el salto fuera no tomado, y si finalmente no se toma, se vacían las estaciones de reserva
Salto, Load, Store y aritméticas
A un predictor que utiliza el comportamiento de los m últimos saltos para elegir entre 2^m  predictores de n bits para un salto concreto
SISD
WAR
El del load
8
Ejecutar una operación o calcular direcciones de memoria
Los procesadores deberían construirse con un número elevado de registros de datos.
Las mismas que la respuesta (a) más el diseño de técnicas para pasar valores de una instrucción a otra.
12 ciclos con anticipación, 18 sin anticipación
Que la máquina segmentada tenga muchas etapas
Sí, si las mayoría de las instrucciones del bucle son saltos
Una unidad vectorial
Asignación de registros vectoriales.
Operaciones en punto flotante exclusivamente.
Se genera código de manera que cada operación vectorial se realiza para un tamaño menor o igual que la MVL 
Sí, siempre y en cualquier caso.
No permiten que se realicen operaciones vectoriales simultáneamente
El número de instrucciones se reduce
Sí, porque A(i) y B(i) no son dependientes entre sí
La tecnología SMT no explota TLP.
SMT es capaz de ejecutar más de una instrucción por ciclo.
Una arquitectura cerrada.
Ninguna de las anteriores es correcta.
Que una instrucción se ejecute en un core u otro de forma alternativa.
El procesador Niagara es un procesador superescalar.
El ancho de banda.
Single Instruction Stream, Single Data Stream\r\n\r\n
VLIW
Multiple Instruction Single, Multi Data Stream
Todas son ciertas
Tipo R(register), Tipo I(immediate) y Tipo J(jump)
Registro - Memoria
El número de instrucciones ejecutadas en una etapa del pipeline por unidad de tiempo
Aritméticas
Los programas acceden a una porción relativamente pequeña del espacio de direcciones en un determinado instante de tiempo
REST
Ninguna es cierta
Unidad de control.
El aumento de la frecuencia de reloj.
Tecnología para la ejecución multihilo que sirve para apoyar al HyperThreading.
Las multihilo.
El 100%.
Núcleos con capacidad de ejecutar cualquier tipo de instrucción.
MPI.
Sí. Los algoritmos paralelos pueden llegar a ejecutarse hasta 2p veces más rápido que\r\nsus homólogos secuenciales debido a las mejoras obtenidas al poseer un sistema de\r\ngestión de memoria que reduce los fallos de caché.
50 %.
100.
Del modelo de consistencia que se esté empleando.
Matriz Sistólica
Ninguna respuesta de las anteriores es correcta
4
No implementa caché para los SPEs
Aproximadamente un 60%
"read-back" y "critical word first"
Fallo de ubicación
Más pequeñas y rápidas cuanto más cerca del procesador
Falso
Necesita un bus de coordinación de memorias caché
La memoria caché no es la más económica de la jerarquía
El grado de cada nodo es d = 2n + 2 y un diámetro D = n x 2.
En que el grado de nodo es siempre 3, independientemente de la dimensión del hipercubo, por lo que resulta una arquitectura escalable.
Desconectando todos los nodos y eliminando algunos enlaces.
Malla Illiac
En ningún caso
Ninguna de las respuestas es correcta
Ninguna afirmación es correcta
El algoritmo de Tomasulo siempre usa ejecución especulativa
Salto, Alu y Store
A un predictor que elige entre 2^n predictores, cada uno de los cuales tiene m entradas para un salto concreto
SIMD
IF
El de la suma
7
Acceder a memoria (lectura o escritura)
Generalmente, evitar que ocurra
Ninguna de las anteriores. 
10 ciclos con anticipación, 13 sin anticipación
Que no haya instrucciones independientes del salto a tomar
t1,t2,t3,t4,t5,t6,t7 si hay anticipación; t2 y t3 si no hay anticipación
Varias unidades vectoriales
Optimización de código.
Operaciones en punto flotante, simple precisión, lógicas y enteras.
Sólo realiza operaciones vectoriales
Es de tipo SIMD según la clasificación de Flynn
El patrón de acceso a memoria es lineal
Es la fuente de sobrecarga más importante
Ninguna es correcta.
SMT no posee emisión múltiple.
Una arquitectura abierta.
SMT mediante multithreading de grano grueso.
Definimos threads preferidos cuya ejecución intenta prolongarse en el tiempo todo lo posible.
El procesador Niagara es un SMT.
El control de voltajes y frecuencias de reloj en el procesador.
Tiempo de ejecución sin utilizar la mejora/ Tiempo de ejecución utilizando la\r\nmejora
RISC
Multi Instruction Simple, Multi Data Simple
Los procesadores Pentium II son los primeros que lo soportan
2^(n-1)
Memoria - Memoria
El inverso de la aceleración
Registro-registro
Lógicas
Ninguna es cierta
En la pila
Múltiples instrucciones sobre múltiples flujos de datos
Memoria caché.
Característica hardware que permite ejecución paralela.
Es de 2.
Asignando a cada hilo sólo los recursos que necesiten e intercambiándolos en el procesador cuando ambos precisen de los mismos recursos.
El 75%.
Intel Atom.
OpenMP.
Sí. El código paralelo generado por el compilador es más eficiente que su versión\r\nsecuencial.
Depende de la frecuencia de reloj empleada y la política de coherencia de caché\r\nempleada.
3, 10 ó 100.
Del sistema de conexión entre núcleos empleado en el procesador.
Ciclo Cubo Conectado
Topología de malla
6
No implementa ejecución fuera de orden
Aproximadamente un 90%
"early-restart" y "critical word first"
Fallo de caché
MESI
Verdadero, en arquitecturas Harvard
Mediante sofisticados algoritmos de búsquedas
Mientras una memora caché escribe, las demás deben espiar el contenido escrito
La memoria caché contiene un subconjunto disjunto de la información almacenada en memoria principal
El grado de cada nodo es d = 2n − 1 y un diámetro D = n/2.
Añadiendo un víctim buffer a cada nivel de caché
En que el grado de nodo es siempre 3, dependiendo de la dimensión del hipercubo, por lo que resulta una arquitectura escalable.
Desconectando todos los nodos sin eliminar ningún enlace.
Router
Sólo si se emite más de una instrucción por ciclo
Una técnica de planificación dinámica que permite la ejecución de instrucciones fuera de orden cuando hay suficientes recursos y no hay dependencias de datos
Un predictor de torneo
Un valor booleano que indica si los operandos fuente de la instrucción ya han sido calculados
Se dejan de emitir instrucciones hasta que se calcule si el salto se toma o no
8, 16 y 32 bits
MISD
ID
Ninguno de los anteriores
1 – 3 – 4 – 2 
Escribir en el banco de registros
Generalmente, la detección y resolución mediante la detención del pipeline
La ejecucion del programa tarda más del doble (CPI > 2) que si se hubiera utilizado estrategias para evitar riesgos de control.
10 ciclos con anticipación, 17 sin anticipación
Bucles muy grandes
Todos si hay anticipación; t1, t2 y t3 si no hay anticipación
Permitir el acceso segmentado a los distintos módulos de memoria paralelos.
Generación de código de seccionamiento.
Operaciones en simple precisión, lógicas y enteras.
Se emplean las opciones b) y c) simultáneamente
Sí, si el resultado es un vector.
Está compuesto por una unidad escalar y una unidad vectorial
No hay retrasos de memoria por la espera  de un operando vectorial, ya que se encuentra en un registro vectorial
Sería despreciable para un vector infinito
Se comparten recursos en intervalos de tiempo alternos.
Las máquinas superescalares son más eficientes que las SMT.
Una arquitectura semi-abierta.
SMT mediante multithreading de grano fino.
Combinamos tecnologías multithreading de grano fino y grano grueso.
Todas las respuestas anteriores son verdaderas
Tiempo de ejecución utilizando la mejora/ Tiempo de ejecución sin utilizar la\r\nmejora
Ninguna es cierta
SIMD
Tipo R(results), Tipo A(arithmetic) y Tipo J(jump)
Ninguna es cierta
Ninguna es cierta
Registro-Memoria
Salto
De anillo
Los programas acceden a una porción amplia del espacio de direcciones en un determinado instante de tiempo
En el acumulador
SISD
Único flujo de instrucciones sobre un único  flujo de datos
Todos los anteriores.
El aumento del calor generado.
No se mejora nada empleando un procesador multinúcleo.
Las secuenciales.
El 50%.
Generan una gran cantidad de calor.
Las 2 son adecuadas.
El tráfico por el bus principal del procesador se reduce.
Ninguno de los 2.
Un alto coste de la sincronización entre los núcleos, ya que trabajan a distinta frecuencia.
Del tipo de núcleos empleado.
Barrel Shifter
Topología N-cubo K-aria
4
No implementa predicción de saltos
tiene un núcleo principal PowerPc que maneja 124 núcleos auxiliares pequeños de 6 bits
"read-back" y "first in-first out"
Una palabra contiene bloques. Cada palabra va en una línea de caché
MOSI
Falso, en arquitecturas Harvard
I en el procesador 1, M en el procesador 2, I en el procesador 3 
Tiene un control de estados, como el MSI
La política de reemplazo de datos en memoria caché es crítica para el rendimiento de un sistema
Mayor que la del anillo completamente conectado.
El que tiene sólo un nodo, sin conexión con nodos descendientes.
Conectando todos los nodos y eliminando algunos enlaces.
Anillo
MEM
Gracias a la ejecución fuera de orden
Una vez
Un predictor dinámico de 2 bits
A un predictor que utiliza una tabla de m * n entradas para almacenar las predicciones de los saltos
Se escribe la dirección de destino en el contador de programa
Ninguna respuesta es correcta
MIMD
EX
El de la division
2 – 1 – 4 – 3  
Obtener la instrucción de la memoria utilizando el CP
Generalmente, la anticipación 
La ejecución del programa tarda más que si hubiera utilizado estrategias para evitar riesgos de control, pero menos del doble (2 > CPI > 1).
10 ciclos con anticipación, 17 sin anticipación
Que la máquina segmentada tenga un número de etapas pequeño
t1,t2,t3,t4,t6,t7 y t9 si hay anticipación, t2 y t3 si no hay anticipación
No se consigue ninguna mejora.
El cálculo de cada resultado es independiente de los cálculos de los resultados anteriores
Operación en punto flotante, lógica y entera.
Realiza operaciones escalares y vectoriales
Acceden a memoria con un patrón conocido
Todas son ciertas
Retrasa la ejecución de convoys sucesivos
Se comparten recursos en intervalos de tiempo pequeños.
DDR2.
Todas son falsas.
Todas son igual de eficientes.
Aumentar la profundidad del pipeline del procesador.
3 ciclos.
Tiempo de ejecución de una instrucción en una etapa del pipeline/ Tiempo de\r\nejecución total\r\n
Están agrupadas en contenedores denominados bundles de 128 bits de longitud
MIMD
Tipo L(logic), Tipo A(arithmetic) y Tipo J(jump)
Latencia o tiempo de respuesta
La productividad por la aceleración
El número máximo de veces que se repite una misma instrucción en un programa
Conversión
Los programas tienden a ejecutarse en un pequeño espacio de tiempo
En un banco de registros de propósito general
SIMD
Único flujo de instrucciones sobre múltiples flujos de datos
Porque requieren mayor cantidad de accesos a memoria que sus homólogas multithreading.
La miniaturización de los componentes.
Es proporcional al número de segmentos de código paralelizable.
Las paralelas.
No se consume menos energía en un procesador multinúcleo.
Programando la aplicación de forma secuencial.
Ninguna es adecuada.
Se requiere una mayor cantidad de información para gestionar la coherencia de caché\r\nentre los diversos núcleos.
No se puede alcanzar aceleración de 30.
No existirá ningún problema, este procesador funcionará perfectamente.
Barrel Shifter
De la organización de la jerarquía de memoria caché empleada.
De estrella
Topología de anillo
6
Todas las anteriores 
tiene 1024 núcleos
"read-back" y "early restart"
Un bloque contiene palabras. Cada bloque va en una línea de caché
Basado en directorios
12 en LRU y 9 en óptimo
I en el procesador 1, E en el procesador 2, I en el procesador 3
Tiene una versión mejorada, el Snarfing
La memoria caché es de rápido acceso y baja capacidad
Igual que la del anillo completamente conectado.
El que tiene un nodo raíz conectado a un cierto número de nodos descendientes.
Tipo de arquitectura unida multidimensional, diseñada para la realización de algoritmos específicos fijos.
Toro 2D
Gracias al uso de estaciones de reserva
Una técnica de planificación estática que usan las máquinas segmentadas
Los valores de los operandos fuente de la instrucción que se encuentra almacenada en esa estación de reserva
Sólo se usan para instrucciones de acceso a memoria y contienen la dirección de acceso
En las estaciones de reserva
Hace planificación estática de instrucciones
El de la suma
1 – 3 – 2 – 4 
Decodificar la instrucción y leer del banco de registros\r\n
Los códigos de programas RISC constan de menos instrucciones que sus equivalentes en CISC.
La diferencia en el tiempo de ejecución es pequeña si el número de instrucciones totales del código del programa es relativamente pequeño, no así para códigos grandes.
12 ciclos con anticipación, 18 sin anticipación
Implementación de una circuitería de anticipación simple
t1, t2, t3, t4, t6, t7 y t9 si hay anticipación, t1, t2 y t3 si no hay anticipación
Complicar la memoria con la consiguiente sobrecarga a la máquina.
El cálculo de cada resultado depende de los cálculos de los resultados anteriores
Nunca se pueden determinar en tiempo de compilación
No se puede usar en aplicaciones científicas
El ancho de banda es menor en comparación con el resto de instrucciones
El cálculo de cada resultado depende de los resultados anteriores\r\n
Suele ser de 1 o 2 ciclos
Se comparten recursos en intervalos de tiempo grandes.
SDRAM.
SMT mediante multithreading de grano fino con uso de hilos “preferidos”.
Ninguna de las anteriores.
2 ciclos.
Tiempo de ejecución total /Tiempo de ejecución de una instrucción en una\r\netapa del pipeline\r\n
Pueden ser ejecutadas en una o más unidades funcionales
Ninguna es cierta
2*(n-1)
Productividad
Ninguna de las anteriores
Memoria-Registro
Una unidad independiente de la CPU con registros propios
Los programas tienden a ejecutarse en un amplio espacio de tiempo
Todas son ciertas
MISD
Múltiples flujos de instrucciones sobre un único flujo de datos
Porque no pueden ser ejecutadas en paralelo en los distintos núcleos de ejecución.
Sinónimo de proceso.
Mayor aprovechamiento de los recursos.
No consigue ejecutar varios hilos.
El aumento en el tiempo de acceso a posiciones de memoria.
Porque solo trabaja al máximo el núcleo que soporte la aplicación mientras que el\r\nresto de los núcleos disminuyen su consumo.
El rendimiento de los núcleos no es óptimo debido a sus características generalistas.
Es falso, los núcleos específicos consumen más energía.
Los mecanismos de comunicación entre los distintos núcleos de ejecución del\r\nprocesador son iguales.
Se obtiene una aceleración de un millón.
Colapsamiento en el bus de comunicación interno debido a las altas frecuencias a la que trabajan dichos procesadores.
La aparición de overhead en los accesos a memoria que hacen referencia a posiciones almacenadas remotamente.
Topología de matriz lineal
2
Power Processor Element (PPE)
tiene un núcleo principal PowerPc que maneja 1024 núcleos auxiliares pequeños de 8 bits
En que se recupera la palabra requerida en cuanto es copiada de memoria principal a caché
La implementación de Intel sobre SMT.
Una palabra contiene bloques. Cada bloque va en una línea de caché
Spooning 
11 en LRU y 9 en óptimo
S en el procesador 1, M en el procesador 2, S en el procesador 3
Será igual que una de mapeado directo si k = m
Menor que la del anillo completamente conectado.
El que tiene una configuración en hipercubo, añadiendo a cada nodo enlaces con otros nodos.
Tipo de arquitectura segmentada unidimensional, diseñada para la realización de algoritmos específicos fijos.
Gracias al renombramiento de registros
Ninguna vez
Un predictor estático de 2 bits
Ninguna de las respuestas es correcta
Si se acertó la predicción se vacía el reorder buffer, en caso contrario no se hace nada
En el CDB
No soporta ejecución especulativa
WB
El de la multiplicacion
3 – 1 – 4 – 2 
Ejecutar una operación o calcular direcciones de memoria
La filosofía RISC se basa en que, para mejorar el rendimiento del ordenador, es bueno minimizar el número de ciclos de reloj necesarios para ejecutar las instrucciones. 
El nuevo CPI tendría un valor aproximado de 0,8333, es decir, 1 – 1/6, emperando 1/6 la ejecución del programa. 
10 ciclos con anticipación, 15 sin anticipación
Implementación de una circuitería de control simple
Todas son ciertas
t1,t2,t3,t4,t5,t6,t7 si hay anticipación; t2, t3, t5 y t6 si no hay anticipación
Incluir la memoria principal de la máquina al lado del procesador vectorial.
El requerimiento de ancho de banda de las instrucciones es elevado
Siempre se pueden determinar en tiempo de compilación
Filas, excepto en Fortran 
No tiene un conjunto de registros escalares
Una instrucción equivale a ejecutar un bucle completo de instrucciones ordinarias
Una sola instrucción especifica gran cantidad de trabajo
Número de componentes del vector y velocidad de CPU
Se comparten recursos en intervalos de tiempo fijos.
Utiliza SMT y TMT.
De 1 ciclo.
Cooperativa y Comparativa
Simple Instruction Stream, Simple Data Stream
Todas son ciertas
MISD
Tipo R(register), Tipo M(memory) y Tipo J(jump)
Aceleración o speed-up
Explicitly Parallel Instruction Computer
Memoria-Memoria
Una unidad dependiente de la CPU con registros propios
Un sistema empotrado
Número de instrucciones concurrentes y en los flujos de datos disponibles
MIMD
Técnica de segmentación que consiste en dividir la ejecución de la instrucción\r\nen bloques independientes que se ejecutan en paralelo\r\n
Las aplicaciones secuenciales si pueden aprovechar las características de los procesadores multinúcleo.
Las posibilidades de la programación en paralelo.
Solo se puede ejecutar una aplicación en cada instante de tiempo.
Ninguna de las anteriores es cierta.
El 30%.
La gran cantidad de espacio que ocupan.
No existe motivo alguno para que los núcleos específicos consuman más energía.
Aumenta los recursos no reutilizados al emplear el mismo espacio de direcciones por\r\ntodos los núcleos de ejecución del procesador.
Sí, porque puede que nunca se aprovechen todos los núcleos del procesador.
No se podrá dar esta situación, ya que todos los núcleos de un procesador deben trabajar a la misma frecuencia.
Que ambas son simétricas
Más de 6
Controlador de Acceso Directo a Memoria
tiene dos núcleo principal PowerPc 
En que se lee de memoria principal además de caché, para garantizar la coherencia
Un bloque contiene palabras. Cada palabra va en una línea de caché
Nunca
10 en LRU y 9 en óptimo
S en el procesador 1, E en el procesador 2, S en el procesador 3
Será igual que una de mapeado directo si k = L
No tiene complejidad porque no está conectado a un anillo.
Soporta las tecnologías Hyper-Threading y de virtualización.
Todas las anteriores.
El que tiene una configuración en anillo, añadiendo a cada nodo enlaces con otros nodos.
Tipo de arquitectura segmentada multidimensional, diseñada para la realización de algoritmos específicos fijos.
Estrella
Gracias al reorder buffer
Una técnica que complementa al algoritmo de Tomasulo para considerar la ejecución especulativa
Los índices de las estaciones de reserva que contienen las instrucciones que producirán los valores de los operandos fuente de nuestra instrucción
Ese caso no podría darse, por lo que el funcionamiento sería correcto
Se saca del reorder buffer y se pasa a la siguiente instrucción
En la caché de instrucciones
Tiene 7 unidades funcionales
IF
El del store 
Tiempo instrucciones segmentadas / Tiempo instrucciones no segmentadas 
Acceder a memoria (lectura o escritura)\r\n
La arquitectura RISC indica que incrementando los modos de direccionamiento permite acceder más fácilmente a los datos y, por lo tanto, proporciona una ejecución más rápida.
Predicción de saltos, anticipación y bloqueos.
10 ciclos con anticipación, 16 sin anticipación 
Implementación de una reordenación de código a nivel software
Los mismos para las dos máquinas
Ejecutar programas cortos con muchos datos activos
Aumentan los riesgos de control que surgen de los saltos de los bucles
A veces se pueden determinar en tiempo de compilación, y hay tests que las detectan
Columnas, excepto en Fortran
No, ya que este tipo de acceso ha sido diseñado para aprovechar el acceso segmentado en paralelo de un conjunto vectorial de datos y el alto ancho de banda.
No existen los riesgos de control
Tiempo de arranque, y velocidad de iniciación
SMT explota TLP.
DDR.
No existe penalización.
Predicción de saltos estática.
Competitiva y Equitativa.
Stream Instruction Single, Stream Data Single
1
SISD
Ninguna es cierta
Es un repertorio de instrucciones diseñadas por AMD
Gestión de memoria avanzada y unidad de punto flotante
Se tiene el mismo tiempo de acceso a todas las palabras de memoria
Una unidad independiente de la CPU sin registros propios
Un array de procesadores
Número de procesadores 
Único flujo de instrucciones sobre único flujo de datos
Técnica de segmentación que consiste en dividir la ejecución de la instrucción\r\nen bloques independientes que se ejecutan en secuencial\r\n
Porque los procesadores multinúcleo trabajan a frecuencias de reloj bajas.
Porque no permite aumentar la frecuencia de reloj.
Disminución del calor desprendido.
Multinúcleos con igual frecuencia y con distinta frecuencia.
Porque todos los núcleos se reparten el trabajo de forma similar.
Creando hilos para aquellas tareas con gran carga de trabajo o de interfaz con el usuario.
Porque poseen menor circuitería debido a su finalidad reducida en el proceso de\r\nejecución.
No posee información para establecer el orden de ejecución de los fragmentos de\r\ncódigo de las aplicaciones.
Los fallos de caché afectan gravemente al rendimiento de la aplicación que puede dar\r\nlugar a una aceleración mínima.
El que tiene segmentación es incompatible con los demás porque no es ILP.
La complejidad en los protocolos de coherencia de caché.
N-cubo K-aria
Que ambas son regulares
Arquitectura del Pentium M
64 KB de caché L1 y 1 MB de caché L2
Crear núcleos homogéneos (todos los núcleos idénticos) 
En que se lee de L2 además de L1 para garantizar la coherencia de caché
N
Siempre
9 en LRU y 9 en óptimo
Una memoria caché y una memoria principal por procesador
Será igual que una completamente asociativa si k = 1
Array paralelo, anillo, anillo acorde de grado 4 y desplazador de barril.
No tienen ciclos.
L1 donde se guardan sólo datos. 
El renombrado de registros .
Tipo de arquitectura segmentada multidimensional, diseñada para la realización de algoritmos específicos variables.
Toro 
Campo “Inst.” con valor 3
Dos veces
Un predictor correlativo
El renombrado de registros solucionaría el problema
Sólo una
En los buffers de loads y stores
El tamaño de la tabla de predicción de saltos utilizada es de 2MB.
ID
Verdadero
Tiempo instrucciones no segmentadas / Tiempo instrucciones segmentadas
Escribir en el banco de registros
RISC defiende que las dependencias de datos, en los procesadores segmentados, se deben resolver mediante hardware, ya que es más rápido.
Repertorio de instrucciones.
El tamaño variable de las instrucciones provoca que primero haya que averiguar qué tamaño tiene la instrucción para luego poder decodificarla.
Implementación de una reordenación de código a nivel hardware
t2,t3,t5,t6 y t7 si hay anticipación; t2, t3,t5y t6 si no hay anticipación
Repartir las localizaciones contiguas de memoria de forma vertical.
Las operaciones aritméticas, pero no los accesos a memoria
A veces se pueden determinar en tiempo de compilación, pero debido a que se trabaja con muchos datos, es un proceso muy caro y se realiza en etapas posteriores
Filas, en todos los lenguajes
Sí, ya que se puede reducir al caso de un procesador con varias módulos de memoria.
Vector-Memoria
Velocidad de iniciación, tiempo de arranque y número de componentes del vector
SMT explota ILP.
Utiliza TMT y no SMT.
Las estaciones de reserva.
De 2 ciclos.
Predictor de saltos de 2 bits.
4 ciclos.
Stream Instrucion Simple,Stream Data Simple
2
SISD
Tienen implementado a nivel de hardware el algoritmo de la Transformada Discreta de Fourier\r\n
2*n
Rendimiento
Extension Parallel Instruction Complex
Varía el tiempo de acceso según donde se localice el dato
Si se referencia una posición de memoria, las direcciones “cercanas” tienden a ser referenciadas poco después
Un clúster
Número de instrucciones concurrentes y tipo de acceso a memoria
Único flujo de instrucciones sobre múltiples flujos de datos
Técnica de paginación que consiste en dividir la memoria en bloques\r\ndependientes\r\n
Aproximadamente cada 18 meses se duplica el número de transistores en un circuito\r\nintegrado.
Porque aumenta el consumo energético.
Se pueden ejecutar hasta x aplicaciones en el mismo instante de tiempo.
Multinúcleos homogéneos, heterogéneos e híbridos.
Porque los núcleos del procesador van intercambiándose la ejecución de la aplicación.
El elevado número de transistores empleados en su fabricación.
Porque son capaces de ejecutar un menor número de instrucciones.
Necesita conocer qué núcleos no están haciendo nada para darles trabajo.
La aceleración obtenida está acotada por el sistema de coherencia de caché.
Al manejar los datos de diferentes maneras existirán inconsistencias en estos.
No existen problemas importantes.
De anillo
Que, en ambas, el número de vecinos es igual para cada nodo
Arquitectura del Pentium 4
Synergistic Processor Elements (SPEs)
Crear núcleos especializados (por ejemplo específicos para en gráficos) 
"Read-back" no está relacionado con la caché
k
Cuando la palabra está al principio del bloque de memoria
68 en cada caso
Una memoria caché por procesador y una memoria principal común
Será igual que una completamente asociativa si k = L
Array lineal, anillo, anillo acorde de grado 4 y desplazador de barril.
Cada nodo tiene distintos padres.
Con la interconexión variable y el funcionamiento síncrono de los nodos.
Anillo
L1 donde se guardan datos e instrucciones. 
Campo “Qk” con valor -1
Ninguna de las anteriores
Los índices de los registros que contienen los valores de los operandos fuentes de la instrucción
La máquina debería parar la emisión de instrucciones hasta que se supiera el resultado del salto (si es tomado o no), por lo que el predictor sería inútil
Dos
No, solo es necesario ese hueco después de haber ejecutado la instrucción en alguna unidad funcional
Redes de gran tamaño\r\n
EX
Falso
Tiempo instrucciones no segmentadas / Número de etapas de segmentación
Obtener la instrucción de la memoria utilizando el CP
Riesgos estructurales, de control y de datos.
El número máximo de etapas en el cual se puede segmentar la ejecución de las instrucciones es dependiente del tipo de instrucción a segmentar.
Porque sólo tiene que anticipar un resultado
Todos si hay anticipación; t2, t3, t5, t6 y t7 si no hay anticipación
Ejecutar programas cortos con pocos datos activos
Los accesos a memoria, pero no los cálculos de direcciones efectivas
No, ya que esto es un error de tipos incompatibles.
No ya que depende totalmente de la forma en que ha sido codificado el programa original.
No, porque siempre se precisa de una memoria caché vectorial para multiprocesadores vectoriales.
Reunir-Esparcir
Los operandos son vectores y el resultado es un escalar
Ninguna de las anteriores 
SMT explota ILP y TLP.
DDR3.
Nivel I.
De más de 2 ciclos.
Predictor de salto con correlación.
Cooperativa y Competitiva.
El aumento del rendimiento debido a una mejora está limitado por el tiempo\r\nque se utiliza dicha mejora\r\n
4
SIMD
Tienen implementado a nivel de hardware sumas y multiplicaciones, por lo que la operación se lleva a cabo en un ciclo de reloj\r\n
Es una extensión de MMX
Fue diseñado como mejora de la extensión MMX
Extended Parallel Instruction Cache
Se tiene el mismo tiempo de acceso a las palabras de memoria de zonas múltiplos de 64 KB
Un tipo de procesador que sólo utilizan las máquinas con arquitectura CISC
Ninguna es cierta
Número de procesadores y número de flujos de datos disponibles
EPIC
Técnica de paginación que consiste en dividir la memoria en bloques\r\nindependientes\r\n
Cada año de reduce a la mitad el número de transistores en un circuito integrado.
Porque disminuye el tiempo de vida del procesador.
Se puede ejecutar un número fijo de aplicaciones en cada instante de tiempo.
Multinúcleos con o sin hyperthreading.
La aceleración que se puede obtener no depende del número de núcleos del\r\nprocesador multinúcleo empleado.
Creando hilos siempre que se pueda paralelizar.
Thread.
Son quienes paralelizan el código para la ejecución en paralelo en procesadores\r\nmultinúcleo.
Sí, porque los retardos ocasionados por la sobresaturación del bus y las sincronizaciones de caché pueden mermar tanto el rendimiento que lo haga menos eficiente.
El procesador con arquitectura VLIW no es compatible con los demás.
Mediante protocolos de escritura por invalidación.
De árbol
Ninguna respuesta de las anteriores es correcta
Arquitectura del Pentium 2
Arquitectura del Pentium M
Combinación de los dos (múltiples núcleos homogéneos rodeados de núcleos especializados) 
Arquitectura Von Neumann
N * k
Cuando la palabra está en la mitad del bloque de memoria
68 en FIFO, 35 en LRU y 31 en óptimo
Una memoria caché y una memoria principal comunes
El primero
Array paralelo, anillo hexagonal, anillo de grado 4 y desplazador de barril.
Un nodo con descendientes es un nodo hoja.
Con la interconexión fija y el funcionamiento síncrono de los nodos.
Hipercubo
Campo “ROB” con valor 1
Posee cuatro procesadores lógicos.
Cinco veces
El número de instrucciones que se han emitido antes y después que la instrucción
Más de tres
Solamente las excepciones se verían afectadas, el resto de instrucciones se ejecutarían correctamente
Tres
Sí, además de tener un hueco libre en alguna estación de reserva
Redes en las que se quiere una gran escalabilidad\r\n\r\n
MEM
Depende
1 / Número de etapas de segmentación
Decodificar la instrucción y leer del banco de registros\r\n
No, si la máquina implementa anticipación o permite insertar burbujas
Número de etapas.
Al existir un número elevado de etapas existen mayores probabilidades de generar riesgos estructurales, de control y de datos.
Porque la complejidad del hardware es menor en los RISC
No existe diferencia alguna.\r\n
No existe tal tipo de entrelazado de memoria.
Las operaciones aritméticas, los accesos a memoria y los cálculos de direcciones efectivas
Sí, ya que opera el primer elemento del vector con el escalar, y el resto del vector  se desecha.
Columnas, en todos los lenguajes
No, ya que este tipo de acceso ha sido diseñado para aprovechar el alto ancho de banda de los multiprocesadores vectoriales.
Vector-Escalar
Los operandos son escalares y el resultado es un vector
El tiempo de arranque es despreciable
Ninguna de las anteriores.
Pentium 4 HT.
Predicción estática de saltos.
Nivel II.
Problemas de desperdicio horizontal (horizontal wasted).
Predictor con buffer de destino de saltos (BTB).
1 ciclo.
El aumento del rendimiento debido a una mejora está limitado por el tiempo\r\nque no se utiliza dicha mejora\r\n
8
MIMD
Tienen implementado a nivel de hardware el algoritmo de la Convolución
También se le conoce como  MMX-2
Corrigió errores de diseño de la extensión MMX
Unidad de procesamiento en paralelo
“ROUND”, “FLOOR” y “CEIL"
Si se referencia una posición de memoria, las direcciones “alejadas” tienden a ser referenciadas poco después
Los DSPs
En las instrucciones de conversión
VLIW
Ninguna es cierta
Cada par de años se cuadruplica el número de transistores en un circuito integrado.
Porque limita el número de núcleo del procesador.
Alternativa al aumento progresivo de la frecuencia de reloj.
Las 3 anteriores.
La aceleración que se puede obtener es de 2 independientemente del número de\r\nnúcleos del procesador multinúcleo empleado.
Dividiéndola en tantos hilos como núcleos posea el procesador.
Aquel cuyos núcleos pueden ejecutar distintos tipos de instrucciones dependiendo de\r\nla aplicación ejecutada.
Establecen la granularidad de los segmentos de código ejecutados en paralelos por los\r\ndiferentes núcleos de ejecución para adaptarse a las características específicas de los\r\nmismos.
La aceleración puede ser inferior que empleando un número menor de núcleos\r\ndebido al coste del traspaso de información entre los distintos núcleos de ejecución.
No habrá problema, se trata de un multinúcleo híbrido.
Mediante protocolos donde todos los nodos no necesariamente han de conocer las actualizaciones.
Cada nodo tiene al menos un enlace cruzando cada dimensión
De toro
Sus nodos pueden tener grado igual a 2
Ninguna de las anteriores
Arquitectura del Pentium 4
Toda las anteriores
Arquitectura Harvard 
N / k
Las inclusivas se basan en la arquitectura Von Neumann y las exclusivas usan Harvard
239 en cada caso
Una memoria principal común, sin memoria caché
El segundo
Array lineal, anillo, anillo acorde de grado 6 y desplazador de barril.
La ejecución de los hiperprcosesos en la CPU.
La distancia entre cada nodo hoja y la raíz es siempre la misma.
Con la interconexión fija y el funcionamiento asíncrono de los nodos.
Campo “Qj” con valor 0
Un algoritmo de programación dinámica para resolver la multiplicación de matrices
Tiene una tasa de aciertos del 100%
Se produce un riesgo estructural y no se emiten más instrucciones
Sí, pero siempre hay huecos disponibles, ya que sólo se emite una instrucción por ciclo
Redes donde los patrones de comunicación son predecibles\r\n
WB
Verdadero
Cuando se tiene que tomar una decisión basada en los resultados de una instrucción mientras las otras se están ejecutando. 
Ejecutar una operación o calcular direcciones de memoria
Sí, si la máquina permite insertar burbujas. Sólo se pierde un ciclo.
Al existir un número elevado de etapas existen mayores probabilidades de generar riesgos de datos y de control, pero no estructurales ya que la propia arquitectura provee un numero de unidades funcionales suficientes para cada una de las etapas de la segmentación.
Porque los RISC proveen reordenación de código previo a la ejecución del mismo
La primera carga operaciones almacenadas en memoria y guarda el resultado en memoria, y la segunda realiza las mismas operaciones pero en registros tanto de tipo vectorial como escalar.
Ejecutar programas largos con pocos datos activos
Los cálculos de direcciones efectivas, pero no los accesos a memoria
Elimina las dependencias correspondientes a riesgos WAW y WAR
Sí, ya el compilador vectoriza todo como cree conveniente.
No contienen dependencias
Reducción de vectores
Uno de los operandos es un escalar, y el resultado es un vector
De 6 a 12 ciclos
Sus dimensiones reducidas.\r\n
Utiliza SMT y no TMT.
La baja capacidad de proporcionar instrucciones por parte de un thread para mantener ocupados todos los recursos de la máquina.
No se puede compartir recursos
El bajo rendimiento de un sistema debido a la alteración de uno de sus\r\ncomponentes está limitado por el número de dichos componentes que se\r\nnecesitan\r\n
Multiple Instruction Stream, Single Data Stream
MISD
Todas son ciertas
Los procesadores Pentium III son los primeros que lo soportan
Usa los mismos registros y formatos de instrucciones básicos de MMX
Extension Parallel Instruction Computer
Se tiene el mismo tiempo de acceso a las palabras de memoria de zonas múltiplos de 128 KB
El procesador tiende a acceder a posiciones de memoria que han sido utilizadas recientemente
Las máquinas MIPS
En las instrucciones aritméticas
Múltiples flujos de instrucciones sobre un único flujo de datos
CISC
Realizar un mejor reparto de la carga al paralelizar.
Se ejecuta una sola aplicación en todos los núcleos de ejecución en cada instante de\r\ntiempo.
Dentro del núcleo.
La aceleración que se puede obtener es de p veces.
Es simple debido a que todos los núcleos tienen las mismas características.
Es aquel procesador que posee un conjunto de núcleos generalistas idénticos\r\nrodeados de muchos núcleos especializados.
El programador de la aplicación.
Sí, siempre.
Evitar las cachés en los núcleos para que no existan problemas de coherencia en caché.
No es necesario controlar las actualizaciones.
Sus nodos no pueden enumerarse usando sus coordenadas en el espacio n-dimensional
De matriz lineal
Sus grados tienen que tener grado menor que 2
No existe tal problema
PowerPC de 64 bits
podemos mantener la refrigeración clásica sin modificación
Arquitectura Turing
Son tres: tamaño de la caché, tamaño de los bloques y conjuntos asociativos de la caché
Las inclusivas mantienen copias de los bloques en cada nivel de caché; las exclusivas no
68 en FIFO y LRU, 35 en óptimo
Esperando a que haya una entrada libre en el reorder buffer para insertar la instrucción que tiene la dependencia
Un algoritmo de planificación dinámica que permite la ejecución fuera de orden teniendo en cuenta las posibles dependencias de datos
Permite elegir entre varios predictores a la hora de predecir un salto 
Se fuerza el vaciado de las unidades funcionales para poder seguir ejecutando las instrucciones posteriores
Las instrucciones del bucle se ejecutarán de manera secuencial, sin ningún paralelismo, ya que podría afectar al resultado final del programa
Ninguna respuesta es correcta
Redes donde los patrones de comunicación son impredecibles\r\n
Pertenece a la famila 8000 de Intel.
Si, existen numerosas formas de averiguar que ocurre en el procesador.
IF
Falso
Acceder a memoria (lectura o escritura)
Sí, si la máquina permite insertar burbujas. Se pierden dos ciclos.
Como alternativa de anticipación y control.
El mayor tiempo de todas las instrucciones
Porque el tamaño de instrucción es constante 
La primera carga operaciones almacenadas en memoria y guarda el resultado en memoria, y la segunda realiza las mismas operaciones exclusivamente en registros vectoriales.
Repartir las localizaciones contiguas de memoria de forma horizontal.
Se complica la jerarquía de memoria de la máquina y ralentiza el funcionamiento de la misma.
Sí, ya que se opera el escalar con cada elemento del vector.
No, ya que si el compilador dispone de vectorización automática, vectorizará aquellas partes que considere vectorizables y dejará igual las que no.
El código 1 no contiene dependencias, pero el código 2 sí 
La longitud de los vectores
Ninguna de las anteriores
De 1 a 6 ciclos
Todos son falsas.
Pentium 2 MMX.
Ninguna de las anteriores.
Nivel III.
El uso de bancos de registros lo suficientemente grandes para mantener múltiples contextos.
Sí, pero solo existe la técnica de hint@pause.
La utilización de la instrucción hint@pause, el mayor numero de veces.
El bajo rendimiento de un sistema debido a la alteración de uno de sus\r\ncomponentes está limitado por el número de dichos componentes que no se\r\nnecesitan\r\n
16
Cache Coherent Uniform Memory Access
MIMD
Permite una gran mejora de la decodificación MPEG2
Todas son ciertas
Dos unidades vectoriales
"AND","OR" y "XOR"
El procesador tiende a acceder a posiciones de memoria que no han sido utilizadas recientemente
Las máquinas POWER PC
En las instrucciones lógicas
REST
VLIW
El rendimiento de los circuitos integrados se duplica cada 18 meses.
SIMD porque el procesador no explota el paralelismo en las instrucciones ni en flujos\r\nde datos.
Todas son correctas.
Fuera del núcleo.
Si, HyperThreading no se puede integrar en un núcleo.
Es difícil debido a que el programador debe realizar el equilibrado de carga de trabajo\r\nentre los núcleos de ejecución.
Parallel.
El compilador.
No, nunca.
Cambiar el bus de comunicación por una red, para evitar la saturación de éste.
Son controladas mediante hardware por un dispositivo específico externo al procesador. 
Sus enlaces pueden ponerse de tal manera que se produzca un desplazamiento en más de una dimensión
SIMD
Sus nodos tienen grado mayor que 2
La información viaja demasiado lenta para los procesadores
Arquitectura del Core 2 Duo
podemos mantener la refrigeración clásica aumentando su tamaño o potencia
Arquitectura Shelving
Son dos: velocidad de acceso y tamaño del bus
Las inclusivas usan write-through y las exclusivas write-back
4 en LRU y 2 en óptimo
Computadores de memoria compartida
Los dos tienen el mismo número
En sustituir cada vértice del cubo por un anillo, normalmente con un número igual de nodos que de dimensiones del cubo.
Cuando la distancia entre cada nodo hoja y la raíz es la misma.
Con la interconexión fija y el funcionamiento síncrono de los árboles.
Haciendo que la instrucción que tiene la dependencia espere para ejecutarse hasta que sus operandos hayan sido calculados
El salto debe ser tomado
Que la instrucción de esa estación de reserva no tiene operandos fuente
Se fuerza el vaciado de las estaciones de reserva para poder continuar la emisión de instrucciones
En este caso no se hará “register renaming”, ya que el contador del bucle debe estar almacenado siempre en el mismo registro
Elimina las dependencias correspondientes a riesgos WAW
Pocas y sencillas Instrucciones, pocos y sencillos modos de direccionamiento, modelo de ejecución registro-registro.
ID
Depende
Cuando una instrucción depende del resultado de una instrucción previa que todavía está en el cauce.
Escribir en el banco de registros
Localizar la UC en la primera etapa, cambiar las líneas de control a su etapa correspondiente y fusionar etapa de ejecución con escritura retardada.
Porque permiten anticipar el valor correcto de un registro que todavía no ha sido escrito.
El tiempo medio de todas las instrucciones
Aumento de la velocidad de ejecucion de una instruccion
SISD
Ejecutar programas largos con muchos datos activos
Se realiza un entrelazado de memoria que permite acceder a varias posiciones consecutivas en distintos módulos de memoria.
Elimina las dependencias correspondientes a riesgos WAR
Sí, pero solo sobre aquellas sentencias que suponen un aumento de rendimiento significativo.
El código 1 contiene dependencias, pero el código 2 no
Las dependencias de control
Se pueden ejecutar múltiples instrucciones en paralelo
Más de 12 ciclos
No utiliza Ninguno.
En CMP se aumenta  el número de hilos en ejecución, no así en SMT.
Implementar la técnica de renombrado de registros.
Implementación de la técnica de renombrado de registros.
Es una arquitectura avanzada de RISC de alto rendimiento\r\n
Multiple Instruction Single, Simple Data Stream
Cache Cache Uniform Management Access
Es una extensión de MMX
Todas son ciertas
Salto Condicional
Pipeline
Throughput
Simplificar el hardware de la máquina al máximo para mejorar sus\r\nprestaciones\r\n
Ninguna es cierta
Ninguna es cierta
Múltiples flujos de instrucciones sobre múltiples flujos de datos
EPIC
Aumento de velocidad.
Controlar el false sharing.
Las que requieren gran interoperabilidad con los usuarios.
Tanto dentro como fuera del núcleo.
La aceleración depende de si el algoritmo es intrínsicamente secuencial o no.
Debe considerar el número de núcleos en ejecución en cada instante de tiempo.
Un procesador con núcleos tradicionales acompañados de núcleos destinados al\r\nprocesamiento gráfico.
La máquina virtual del lenguaje empleado.
Sí, porque MPI realiza una paralelización de forma generalista y por ello debería funcionar igual en cualquier procesador que se pruebe.
Mejorar el controlador de memoria para evitar fallos en caché.
Es ortogonal
MIMD
Sus nodos internos tienen grado igual a 2 y los terminales grado igual a 1
El cuello de botella del ancho de banda está limitado por el acceso a memoria
3.8 Ghz
es necesario acudir a técnicas novedosas
Entre el procesador y la memoria principal 
Son dos: número de niveles y número de procesadores
4 en LRU y ninguno en óptimo
Computadores de memoria anidada.
Ninguna de las memorias implementa conjuntos
En sustituir cada vértice del cubo por un anillo, normalmente con un número mayor de nodos que de dimensiones del cubo.
Cuando no todas las ramas del árbol tienen la misma longitud.
Con las redes estáticas.
Monitorizando las estaciones de reserva hasta que se calcula el resultado para poder hacer uso de él
Que los operandos fuente de la instrucción en esa estación de reserva valen cero
Se produce una situación insostenible provocada por la mala programación y la ejecución se aborta
Ninguna de las respuestas es correcta
Pocas y sencillas instrucciones, fácil decodificación de la instrucción. 
EX
Cuando se tiene que tomar una decisión basada en los resultados de una instrucción mientras las otras se están ejecutando. 
En la etapa EX, ya que los multiplexores de anticipación de la ALU están en esta etapa
Cambiar las líneas de control a su etapa correspondiente y las etiquetas de control, fusionar etapa de ejecución con escritura retardada.
Todas las anteriores. 
Bloqueo.
Las inclusivas solo tienen un nivel de caché, mientras que las exclusivas normalmente 2 o 3
El menor tiempo de todas las instrucciones
Aumento de la velocidad de ejecución de un programa
La primera carga operaciones almacenadas en memoria y guarda el resultado en registros, y la segunda realiza las mismas operaciones pero en registros tanto de tipo vectorial como escalar.
Repartir las localizaciones contiguas de memoria de forma vertical y horizontal.
No tiene ningún efecto.
No, ya que devuelve un resultado erróneo.
Teniendo una separación entre elementos y un número de bancos que sean relativamente primos entre sí
Tanto el código 1 como el código 2 contienen dependencias
Los riesgos estructurales de las operaciones
No es necesario asegurarse de que no ocurren dependencias
Suele ser alta en los procesadores vectoriales
Todos lo utilizan.
Todas las respuestas son ciertas.
Cuenta con 8 unidades funcionales que se encuentran en un solo chip.
Depende de si el sistema se encuentra libre u ocupado el procesador.
La cooperación entre los procesos.
No permite tamaño variable de los datos en las instrucciones
Multiple Instruction Single, Single Data Stream
Cache Coherent Uniform Memory Access
MISD
Es una extensión del juego de instrucciones MIPS y permite un alto rendimiento en el procesado de gráficos 3D\r\n
Latencia
Throughput
Ninguna es cierta
UMA
Temporal y espacial
En las instrucciones de salto
Ninguna es cierta
RISC
Mejor aprovechamiento de los recursos.
MIMD porque los procesadores multinúcleo emplean múltiples flujos de instrucciones\r\nsobre un mismo flujo de datos.
Las que requieren grandes cantidades de datos.
En ninguna de las anteriores.
No, un procesador multinúcleo puede tener cada uno de sus núcleos con HyperThreading y a su vez puede formar parte de un sistema de multiprocesadores.
Implica conocer las características específicas de cada uno de los núcleos de ejecución.
Runnable.
El compilador o un intérprete inteligente implementado por software o hardware.
Sí, porque al tener el mismo número de núcleos el rendimiento con cualquier programa es el mismo.
No existirán problemas, el único límite es el tamaño de los componentes.
Malla, Ciclo Cubo Conectado e Hipercubo
SISD
a
La información viaja demasiado rápido para los procesadores
4.0 Ghz
no hace falta refrigerar, los procesadores multinúcleo no se calientan
Entre la memoria principal y la memoria virtual
Son cuatro: tamaño de la caché, tamaño de los bloques, tamaño del bus y número de niveles
2^3 accesos simultáneos
2 en LRU y 2 en óptimo
Computadores de memoria desdibujada
Eso es falso. NRU resetea ambos bits
En sustituir cada vértice del cubo por un anillo, normalmente con un número menor de nodos que de dimensiones del cubo.
Cuando todas las ramas del árbol tienen distinta longitud.
Con el barrel shifter.
Renombrando los registros destino de las instrucciones
El salto no debe ser tomado
Es el predictor de salto que se utilizó en las primeras máquinas superescalares
Reorder buffer y estaciones de reserva
Sí
Fácil decodificación de la instrucción, pocos y sencillos modos de direccionamiento
MEM
Cuando la circuitería no puede soportar la combinación de instrucciones que se quiere ejecutar en el mismo ciclo
En la etapa ID, que es cuando se decodifica la instrucción
Localizar la UC en la primera etapa y cambiar las líneas de control a su etapa correspondiente.
Una vez se termine de decodificar la instrucción en la etapa IF, se tiene que que obtener el registro o valores necesarios para calcular la dirección de salto antes de la epata EX (se necesita una unidad ALU extra en la etapa ID).
Se calcula usando la frecuencia de aparición de la instrucción en un código y el tiempo de la instrucción.
Aumento del rendimiento de la máquina
MISD
Repartir las localizaciones contiguas de memoria de forma vertical.
Predicción.
El acceso a los datos es más veloz a causa del entrelazado de memoria.
No tiene solución
Sí, ya que se puede establecer un compromiso entre ambos total.
La dependencia de datos
No se requieren cachés para los datos
Es el tiempo por cada resultado una vez la operación vectorial está en ejecución
Por tener un sistema de refrigeración integrado.
UltraSparc T1.
En SMT se aumenta necesariamente la profundidad del pipeline del procesador, no así en CMP.
Graduar de forma independiente instrucciones de varios threads.
El Niagara no cuenta con nivel de caché 3.
Una prioridad mayor que a los hilos no especulados.
SISD
Multiple Instruction Simple, Simple Data Stream
Cache Cooperative Uniform Memory Access
SISD
Uniform Memory Access
Con operandos inmediatos
Todas son ciertas
Latencia
Ampliar el hardware de la máquina al máximo para permitir nuevas\r\nfuncionalidades\r\n
Referencial y temporal
SISD, SIMD, MISD, MIMD
Tiempo antiguo*[(1 - fracción mejorada) + (fracción mejorada/aceleración mejorada)]
UMC
Aumento de la eficiencia de la aplicación.
Gestionar la prioridad de los hilos.
Las que emplean multihilo.
Sí, siempre.
Si, los sistemas multiprocesadores no admiten procesadores multinúcleo.
Homogéneos.
Es aquel procesador que emplea núcleos destinados al procesamiento gráfico para la\r\nejecución del repertorio de instrucciones tradicional.
No, nunca.
Superescalar.
10.
N-cubo K-aria, Malla y Matriz lineal
MISD
b
Le hacen falta más transistores
4.6 Ghz
Ha comenzado a ser construido con la tecnología de fabricación de 85nm 
Entre la memoria virtual y los discos duros
Elimina el último bloque que fue referenciado
3 accesos simultáneos
3 en LRU y 1 en óptimo
No existe esa estructura de memoria 
Para minimizar el número de escrituras en memoria principal
En sustituir cada vértice del cubo por varios anillos, normalmente con un número igual de nodos que de dimensiones del cubo.
Cuando la raíz tiene el mismo número de nodos.
Con las redes de interconexión.
Renombrando los registros destino de las instrucciones
Garantizando la ejecución en orden de las instrucciones de carga de datos
Un algoritmo de planificación estática que permite la ejecución fuera de orden teniendo en cuenta las posibles dependencias de datos
Que los operandos fuente de la instrucción en esa estación de reserva son direcciones de memoria
Son todas falsas, menos la a.\n
Son todas falsas.
Ninguna respuesta es correcta
El 50% de las veces predecirá que el salto se toma y el otro 50% que no se toma
Ninguna de las respuestas es correcta
El bucle puede verse automáticamente desenrrollado gracias al “register renaming” y a la predicción de saltos
En la etapa de emisión
Acceso eficiente a memoria
WB
Cuando una instrucción depende del resultado de una instrucción previa que todavía está en el cauce.
En la etapa MEM, que es cuando se necesita el dato a anticipar
Ninguna de las anteriores.
Puesto que ya se tiene el valor del PC y el campo inmediato, se añade un sumador (ALU) a la etapa ID. Para decidir si saltar o no se puede hacer con una comparación de dos registros durante la etapa ID para ver si son iguales (OR-Exclusiva de dus bits y AND-Lógica de los resultados).
Duplicar la velocidad de la etapa de ejecución (ALU)
Ninguna de las anteriores
SIMD
Vectoriales, SIMD; Superescalares, MIMD
Elimina las dependencias correspondientes a riesgos WAW y RAW
Con suficientes bancos para evitar conflictos en el caso de separación unidad
No, ya que al intentar aumentar la consecución de un objetivo, disminuye la del otro.
En ocasiones el cálculo de un resultado es independiente de los cálculos anteriores
Se amortiza la latencia de memoria con la carga de un gran número de elementos
Normalmente es  uno por ciclo de reloj
Montecito es un procesador de cuatro núcleos y dos subprocesos de\r\nhardware por núcleo.\r\n
Búsqueda, ejecución, memoria y escritura.
En CMP se ejecuta cada hilo durante un número fijo de ciclos de reloj, no así en SMT.
Optimizar la ocupación de las unidades funcionales.
Cuenta con 6 unidades funcionales que se encuentran en un solo chip.
El Niagara tiene 3Mb de memoria cache a nivel 2.
La carga de trabajo y de sus características de tiempo de ejecución
Single Instruction Stream, Multiple Data Stream
Están agrupadas en contenedores denominados bundles de 64 bits de longitud
Cache Coherent Uniform Management Access
También se le conoce como  MMX-2
Es una unidad funcional independiente que comunica con la FPU
Ambas son ciertas
Latencia
"JE","JBE" y "JNE"
NUMA
Referencial y espacial
Anticipación de decisión por hardware.
Predicción de saltos, ejecución especulativa y planificación dinámica.
Un flujo de datos es el flujo secuencial de datos requeridos por el flujo de direcciones. \n\n
SISD, se define como un único flujo de instrucción y de datos. \n
CrayX, Crag-MP.\n\n
Para conseguir un sistema con mayor rendimiento se utilizan simultáneamente varios procesadores. \r\n\r\n
El principal motivo que llevó al desarrollo de sistemas multiprocesadores fue disminuir la segmentación.
Los sistemas multiprocesador incrementan la velocidad de operación. 
Todas las respuestas anteriores son verdaderas excepto la b. 
Las respuestas a y b son verdaderas.
Modelo de Flynn.\n\n
Modelo de Gustafson.\n
Modelo de Von Neumman. \n
Todas las respuestas anteriores son ciertas.\r\n \r\n
Sólo es cierta la respuesta a.\n
Es cierta tanto la b como la d.
La Ley de Amdahl se basa en una carga de trabajo variable, en la que al incrementarse el número de procesadores en un computador paralelo la carga fija se distribuye entre más procesadores para la ejecución paralela. \n\n
La escalabilidad es el principal objetivo de la Ley de Gustafson. \n
El tiempo de respuesta es el objetivo principal de la Ley de Amdahl. \n
El modelo de Sun y Ni se aplica a problemas escalables limitados por la capacidad del procesador. \n
La a es falsa y la d también.\n
Todas son ciertas.\n
Ninguna es cierta.
Es el valor normalizado del speed – up, respecto a la cantidad de procesadores. \n\n
Es  En = Sn / N.\n
Es el número de programas ejecutados en un determinado tiempo.\n\n
Nos dice cuánto más rápido se ejecuta una tarea.\n
Son todas falsas.\n
La a y b son ciertas.\n
Son todas ciertas.
Verdadero.\n\n
Falso.
La clasifican más popular es la de Von Neuman.\n\n
La clasificación más popular es aquella que se basa en la clasificación atendiendo al flujo de datos y de procesadores en un sistema.\n
La clasificación más conocida es la de Amhdal.\n
Son todas ciertas.\n
En los SISD sólo se puede ejecutar una única instrucción. \n
Las instrucciones en los SISD se ejecutan de forma secuencial, no pudiendo haber superposición de varias instrucciones. \n
El MIMD es de flujo de instrucciones simples, y un flujo de datos múltiple. \n
El SIMD sólo puede ejecutar una instrucción simple y un flujo de datos múltiple. \n
Los MIMD pueden ser segmentados. \n
El MISD se define como un flujo múltiple de instrucciones y un único flujo de datos. En donde, las instrucciones actúa sobre el mismo y único trozo de datos. \n
A los MIMD se les conoce como computadores serie escalares. \n
Las repuestas correctas son b, c, f, g y h.\n
Son todas falsas.\n
Son todas ciertas.
Según el modelo de memoria compartida los multiprocesadores se encuentran subdivididos en UMA, NUMA y COMA.\n\n
Según el modelo de memoria compartida los multiprocesadores son sólo los de tipo NUMA.\n
Los tipos UMA y COMA no son tipos de multiprocesadores.
Los multiprocesadores de memoria compartida y multicomputadores son los esquemas de diseño más usados en los computadores.\n\n
Los multiprocesadores se basan en el tipo de red conocida como red indirecta, aunque también puede ser de tipo de red directa.\n
Los multiprocesadores se basan en el tipo de red dinámica.\n
Son todas falsas.
La red de interconexión no puede comunicar la memoria principal con el procesador, sólo lo hace con la memoria cache. 
Los computadores matriciales utilizan el bus como medio de comunicación. 
Las redes de interconexión únicamente pueden ser clasificadas de un modo. \n\n
Los multiprocesadores usan la red como medio de comunicación. \n
Los protocolos de las redes de interconexión son comunes e independientes del tipo de sistema. \n
La memoria está totalmente separada de los procesadores. \n
Cada procesador posee su propia memoria cache. \n
Los multicomputadores usan el bus como medio de comunicación. \n
La b, c, f y h son falsas.
Topología, encaminamiento, mecanismos de control de flujo y estrategias de conmutación.\n\n
Topología, encaminamiento, mecanismos de control de flujo, estrategias de conmutación y modelado del retardo.\n
Topología, encaminamiento, mecanismos de control de flujo y formato del datagrama.\n
Son todas ciertas.\n
Son todas falsas.\n
La b es más precisa que la a.\n
La a es la más precisa.
Redes multibus.\n\n
Redes cruzadas.\n
Redes clos.\n
Redes multietapa.\n
Redes basadas en clusters.\n
Redes jerárquicas.\n
Son todas tipos de redes híbridas.\n
Ninguna es de tipo híbrida.\n
La a, la f y la e si lo son.
Redes multibus.\n\n
Redes cruzadas.\n
Redes clos.\n
Redes multietapa.\n
Redes basadas en clusters.\n
Redes jerárquicas.\n
Son todas tipos de redes híbridas.\n
Ninguna es de tipo híbrida.\n
La a, la f y la e si lo son.
Topologías Regulares.\n\n
Medio Compartido.\n
Topologías Irregulares.\n
Barras Cruzadas.\n
Todas son correctas.\n
Ninguna es correcta.\n
La a y la c son correctas.
Medio Compartido, Barras Cruzadas y Buses.\n\n
Buses y Barras Cruzadas.\n
Medio Compartido, Barras por Buses e Interconexión Multietapa.\n
Barras Cruzadas e Interconexión Multietapa.\n
Medio Compartido.\n
Todas son ciertas.\n
Todas son falsas.\n
La b y la e son ciertas.\n
La d y le e son ciertas.
El Futurebus (IEEE 896) es un estándar de bus de datos ideado para reemplazar todas las conexiones de bus local en una computadora.\n\n
Es un bus poco conocido pero por sus altas prestaciones está llamado a sustituir a los populares buses VME, EISA, Multibus, etc. \n
Sus características lo hacen ideal para cualquier tipo de sistema, desde sistemas multiprocesadores con soporte para cache hasta sistemas con funcionalidades de tiempo real, pasando por los sistemas tolerantes a fallos.\n
Todas son ciertas.\n
Ninguna es cierta.
El mayor ancho de banda y capacidad de interconexión se consigue con la red de barra cruzada.\n\n
Una red de barra cruzada se puede visualizar como una red de una sola etapa de conmutación. \n
Los conmutadores de cada cruce dan las conexiones dinámicas entre cada par destino-fuente.\n
Todas son ciertas.\n
Todas son falsas.\n
La respuesta a y b son ciertas.
CrayX, CrayY-MP. \n
DEC GIGAswitch y Myrinet.\n
GIGAswitch y Myrinet. \n
Tanto la a como la d lo son.\n
Tanto la b como la c lo son.\n
Tanto la b como la d lo son.\n
Todas son ciertas.\n
Todas son falsas.
Dos.\n\n
Cinco.\n
Cuatro.\n
Siete.\n
Uno.\n
Todos los que se deseen.\n
Ninguno.
Dos.\n\n
Cinco.\n
Cuatro.\n
Siete.\n
Uno.\n
Todos los que se deseen.\n
Ninguno.
Son una clase de redes No Bloqueantes.\n\n
Son una clase de redes de Interconexión Multietapa.\n
Son una clase de redes Bloqueantes.\n
Son una clase de redes Reconfigurables.
En estas redes existe múltiples caminos entre cada origen y destino.\n\n
En estas redes sólo existe dos caminos entre cada origen y destino.\n
En estas redes sólo existe un único camino entre cada origen y destino.\n
En estas redes puede que no exista camino entre cada origen y destino.\n
Todas son falsas.\n
Todas son ciertas.
Bloqueantes, Multietapa y Reconfigurables.\r\n\r\n\r\n
Bloquenates y No Bloqueantes.\n\n
Multietapa y Reconfigurable.\n
Bloqueantes, Unidireccional, Multietapa y Reconfigurable.\n
Reconfigurable.\n
Todas son ciertas.\n
Todas son falsas.\n
La respuesta b y c son ciertas.\n
La respuesta b y e son ciertas.
MINs unidireccionales.\n\n
MINs bidireccionales.\n
MINs multidireccionales.\n
Todas son ciertas.\n
Todas son falsas.\n
La respuesta a y b son ciertas.\n
La respuesta a y la c son ciertas.
Omega, Inversa y Cubo.\n\n
Gamma, Cuadrática y Compacta.\n
Mariposa y de Línea Base.\n
La respuesta a y la b son redes multietapa.\n
La respuesta a y la c son redes multietapas.\n
Todas son redes multietapa.\n
Ninguna es red multietapa.
Verdad. Pero sólo en ocasiones se pueden considerar similares.\n\n
Verdad. Las redes MIN Bidireccionales línea base pueden considerarse como una red Benes plegada.\n
Falso.
Se puede incrementar añadiendo más enlaces en paralelo en los conmutadores cercanos al conmutador raíz.\n\n
Se puede incrementar añadiendo más enlaces cruzados en los conmutadores cercanos al conmutador raíz.\n
Se puede incrementar añadiendo más enlaces en serie en los conmutadores cercanos al conmutador raíz.
Son redes en las que algunas fuentes pueden conectarse con cualquier destino libre sin afectar a las conexiones en curso.\n\n
Son redes en las que cualquier fuente puede conectarse con cualquier destino libre sin afectar a las conexiones en curso.\n
Son redes en las que cualquier fuente puede conectarse con cualquier destino libre afectando a las conexiones en curso.\n
Son redes en las que cualquier fuente puede conectarse con algunos destinos libre sin afectar a las conexiones en curso.
Red de Clos.\n\n
Red de Benes.\n
Red de Stendhal.\n
Todas son redes reconfigurables.\n
Ninguna es una red reconfigurable.
Red de Clos.\n
Red de Benes.\n\n
Red de Stendhal.\n
Todas son redes reconfigurables.\n
Ninguna es una red reconfigurable.
Máquina con registros vectoriales.\n\n
Máquina con registros escalares.\n
Máquina vectorial memoria-memoria.\n
Ninguna de las anteriores.
Cray X-MP. \n\n
Cray Y-MP.\n
Cray-1.
La CM-5 fue utilizada por la NASA Jet Propulsion Laboratory (JPL) para procesar los datos de un transbordador espacial en expediciones de alta resolución de imágenes de color de la superficie de la tierra. Esta máquina dejó de usarse en 1994.\n\n
La CM-5 fue utilizada principalmente en campos de investigación para el gobierno y en los negocios internacionales, así como por las universidades con fines educativos. Dejó de usarse en 1993.\n
Ninguna de la anteriores es cierta 
La CM-5 fue utilizada para procesar los datos meteorológicos alcanzando predicciones de 72 horas hasta el año 1994.
Redes regulares Crossbar.\n\n
Redes multietapa unidireccionales. \n 
Redes multietapa bidireccionales.
Las redes Clos son redes multietapa no bloqueante.\n\n
Las redes Myrinet tienen una alta latencia. \n
El Cray XMP permitió 72 horas de predicciones meteorológicas y el estudio de hidrodinámica en 2D. \n
Ninguna es cierta.
En los host.\n\n
En los conmutadores.\n
Ethernet y Autonet son compatibles sin ningún tipo de cambios.\n
Puede hacerse tanto en los host como en los conmutadores.
Los bucles.\n\n
Fallos en la traducción de direcciones.\n
Evita los giros abajo/arriba.\n
La respuesta a y c son correctas.
Las redes Myrinet se caracterizan por su alta velocidad, alta disponibilidad y baja latencia.\n\n
Las redes Myrinet actualmente superan latencias de tres microsegundos.\n
Las redes Myrinet no se usan en la actualidad.
La empresa que diseñó Myrinet es Myricom que comenzó en 1994 con su primera aplicación Myrinet.\n\n
Los paquetes Myrinet son de longitud variable.\n
En la CPU se concentra el procesamiento de las comunicaciones.\n
La respuesta a y c son correctas.
Magerit, Marenostrum y Altamira.\n\n
Sólo Magerit y Marenostrum.\n
Todos los supercomputadores de la red Española de Supercomputación.\n
Sólo los supercomputadores de la red Española de Supercomputación que están en el TOP500.
Core Duo
Pentium 4
Pentium D
Core 2 Duo
Dos procesadores Pentium 3
Cuatro procesadores Pentium 4
Dos procesadores Pentium 4
Cuatro procesadores Pentium 2
Año 2000, tres versiones
Año 2007, cuatro versiones
Año 2004, dos versiones
Año 2006, dos versiones
Intel
Sun Micro System
AMD
IBM
Intel\n
Sun Micro System\n
AMD\n
IBM
2) Marenostrum ha duplicado su capacidad en los últimos años.
Red escalable hasta 512 procesadores, 128 puertos y ancho de banda de 15Gbytes.
3) La 1 y 2 son correctas.
4) Ninguna es correcta.
Intel\n
Sun Micro System\n
IBM\n
Ninguna de las anteriores
Intel\n
AMD\n
IBM, Sony y Toshiba\n
IBM e Intel
8
2\n
4\n
6
Procesador vectorial\n
Coordinar el trabajo de los SPEs\n
Procesador para el consumo energético\n
Ninguna de las anteriores
Unidad de cálculo vectorial\n
Unidad de coordinación del trabajo\n
Bus de interconexión de elementos\n
Ninguna de las anteriores
la temperatura no aumenta\n
aumenta de manera mínima\n
disminuye la temperatura\n
aumenta mucho la temperatura
La primera generación alcanzaba los 512Mbps y la segunda generación los 1280Mbps.
Red escalable hasta 512 procesadores, 128 puertos, ancho de banda de 15Gbytes y reloj de 50MHz.
Red escalable hasta 512 procesadores, 128 puertos y ancho de banda de 13Gbytes.
sólo depende del numero de núcleos
depende del numero de núcleos y la forma en que se administren\n
ninguna de las anteriores
sólo depende de la forma en que se administren los núcleos
cambiar a la programación organizada\n
cambiar a la programación estructurada\n
cambiar a la programación en paralelo\n
ninguna de las anteriores
Tamaño de la caché de datos, velocidad de reloj y conjunto de instrucciones.
Alta disponibilidad, alto rendimiento y baja latencia.
Todas las respuestas anteriores son correctas.
Alta escalabilidad, memoria distribuida y con características similares a las de una máquina SIMD.
La a y la b son correctas.
Intel Core Duo\n
Intel Core 2 Duo\n
Intel Core i7\n
Intel Core 2 Quad
Todas ellas son redes multietapa bloqueantes unidireccionales.
Son computadoras MIMD con memoria virtual compartida
Todas pueden ser escalables hasta 512 procesadores.
La b y la c son correctas.
Intel Core 2 Duo\n
Intel Gulftown\n
Intel I7\n
Intel Core2
El acceso a los datos remotos.
Poca escalabilidad.
La topología unidireccional.
La a y la b son correctas.
Ninguna de las siguientes son ciertas\n
Se está estudiando refrigerar los móviles con agua\n
No es posible incluir procesadores multinúcleo en los móviles\n
Se esta estudiando el incluir procesadores multinúcleo en los teléfonos móviles
En las redes Clos no siempre es posible establecer una comunicación sin cambiar conexiones en curso.
Las redes Clos pueden tener las mismas funcionalidades que las redes cross-bar aumentando las etapas.
Al aumentar las etapas en las redes Clos aumenta la latencia.
Todas son falsas.
Mediante hidrógeno líquido\n
Mediante líquido y mediante ventiladores de estado sólido\n
Haciendo que la carcasa imite a un refrigerador convencional\n
Ninguna de las anteriores
Prohíbe los giros abajo/arriba.
Que depende del árbol de distribución.
No garantiza rutas óptimas.
La traducción de las direcciones.
Myrinet alcanzaba en el año 2000 los 1280Mbps y actualmente alcanza 10Gbps.
PC sobremesa\n
La segunda generación alcanzaba los 1280Mbps y actualmente los 10Gbps.
Portátil\n
Servidor\n
Dispositivo móvil
Memrom
Conroe\n
WoodCrest\n
El tamaño de los paquetes no influye en la latencia de las redes Myrinet.
Ninguna de las anteriores
La disminución del tamaño de los buffer disminuye el retardo medio de encolado.
Gigabit Ethernet presenta bajas prestaciones en sistemas paralelos. 
Los conmutadores Myrinet pueden llegar a tener 16 puertos con un ancho de banda de 8 bits.
Memrom\n
Conroe\n
WoodCrest\n
Ninguna de las anteriores
Las redes de baja latencia como Myrinet incrementan las prestaciones a partir de un número de nodos.
Gigabit Ethernet presenta un cuello de botella en sistemas paralelos.
Myrinet utiliza módulos de compatibilidad con Gigabit Ethernet.
Myrinet utiliza un procesador específico.
Memrom\n
Conroe\n
WoodCrest\n
Ninguna de las anteriores
Las redes de baja latencias son las redes Myrinet, ya que ofrecen las mejores prestaciones en la actualidad.
Las redes Infiniband son redes de baja latencia y ofrecen mejor compatibilidad que las redes Myrinet.
Opteron\n
Turion\n
Las redes Myrinet ofrecen mejor compatibilidad mientras que las redes Infiniband obtienen mejores prestaciones.
Athlon 64\n
Ninguna de las anteriores
Athlon 64 X2\n
Opteron X2\n
Turion X2\n
Radica principalmente en el alto rendimiento de los procesadores.
Ninguna de las anteriores
Mediante chips integrados en la tarjeta de red.
Descargando a la CPU del procesamiento de las comunicaciones.
La respuesta b y c son correctas.
ServerNet fue desarrollado por Intel.
ServerNet es una máquina SIMD.
Las redes ServerNet obtienen bloqueos nulos.
Ninguna es correcta.
Una lectura por un procesador P de una posición X, que sigue de una escritura de P a X, sin que ningún otro procesador haya escrito nada en X entre la escritura y la lectura de P.
Una lectura por un procesador de la posición X, que sigue una escritura por otro procesador a X, devuelve el valor escrito si la lectura y escritura están suficientemente separados y no hay otras escrituras sobre X entre los dos accesos.  
7) La 1 y la 4 son ciertas
El coste de una red de este tipo es O(N M C), donde N son las entradas, M las salidas de la red y C el número de conmutadores.
Ninguna de las anteriores es cierta. \t
Las escrituras a la misma posición están serializadas, es decir, dos escrituras a la misma posición por cualquiera dos procesadores se ven en el mismo orden por todos los procesadores. Por ejemplo, si se escriben los valores 1 y 2 en una posición, los procesadores nunca pueden leer el valor 2 y luego el 1.
La a y c son ciertas.
Son todas ciertas.
Son todas falsas.
En post escritura no se produce incoherencia entre caches, pero si en la escritura inmediata. 
En post escritura se produce incoherencia entre caches, pero no entre cache y memoria principal. 
La escritura inmediata siempre evita la incoherencia entre cache y memoria cuando se escribe en cache. 
En escritura inmediata se opta por no provocar incoherencia entre cache y memoria principal cuando se escribe en cache. 
Son todas ciertas.
Existe el peligro de que un proceso vea el nuevo dato en su caché que se ha escrito en una dirección, mientras que el otro proceso vea aún el valor del dato antiguo.
Existe el peligro de que un proceso vea el nuevo dato en su caché que se ha escrito en una dirección, mientras que el otro proceso vea aún el valor que se encuentra en la memoria principal.
Existe el peligro de que un proceso vea el nuevo dato en su caché que se ha escrito en una dirección, y que el otro proceso vea ese nuevo dato, ya que no existe incoherencia.
Son todas falsas.
Son todas ciertas.
La b y c no son del todo ciertas.
Los problemas que se pueden dar en sistemas con múltiples componentes con caché, son dos tipos, puede haber incoherencia entre las distintas caches, así como problemas de coherencia entre la memoria cache y la memoria principal.
La escritura inmediata es aquella en la que cuando se modifica un dato en la cache de un procesador únicamente se escribe sobre dicha cache, es decir no se realiza una copia inmediata sobre la memoria principal.
La escritura post escritura es aquella que permite la actualización de la memoria principal cada vez que se modifica una dirección en cache de un procesador.
Existen varios modos de implementaciones que se pueden llevar a cabo para mantener la coherencia.
El uso de caches locales hace que se produzca incoherencia.
La implementación de memoria compartida, de cache privada con protocolos de escucha y caches privadas con directorios compartidos son modos de implementaciones para mantener la incoherencia.
No existe ventaja alguna al usar la post escritura en vez de la escritura inmediata.
La ventaja es que el uso de la post escritura permite que no aparezcan incoherencias entre cache y memoria principal también cuando se escribe en cache. 
La ventaja es que el uso de la post escritura permite que no aparezcan incoherencias entre cache y memoria principal cuando se escribe en cache o en memoria principal.
La competencia por el acceso al directorio (contención), así como los largos tiempos de búsqueda, son alguno de los inconvenientes de este esquema.
El uso de protocolos de directorios distribuidos son mejores, ya que producen menos contenciones.
En MSI para actualizar la memoria principal se hace uso de la post escritura.
En MSI para mantener la coherencia se usa la escritura inmediata. 
Los protocolos de espionaje usan las transferencias broadcast, mientras que los protocolos de directorios realizan sus transferencias punto a punto.
Son todas falsas, tienen errores.
Son todas falsas, menos la a, b, c y e.
Son todas ciertas.
El bit de presencia indica la existencia en las caches de copias del bloque de memoria. 
El bit de inconsistencia única muestra que cache tiene permiso para actualizar la línea. 
El bit de validación por su parte indica si es o no válida la copia del bloque de memoria.
El bit de privacidad sirve para indicar si la copia tiene permiso de escritura.
Son todas ciertas.
Son todas falsas.
Si.
No.
La red de interconexión va a ser el modo a través del cual se van a comunicar los diferentes elementos del sistema, memoria con procesador, etc.
Las redes de interconexión juegan un papel importante en el rendimiento de los computadores paralelos modernos.
La escalabilidad es uno de los factores que intervienen en el diseño de las redes de interconexión, es decir implica que al incrementarse el número de procesadores, el ancho de banda de la memoria, la E/S y la red deben incrementarse proporcionalmente.
La simplicidad es uno de los factores que intervienen en el diseño de las redes de interconexión, que llevan a mayores frecuencias de reloj lo que implica un mayor rendimiento.
La restricciones físicas es uno de los factores que intervienen en el diseño de las redes de interconexión, sería deseable que la red permitiese la conexión de un gran número de componentes a la vez que mantuvieran las latencias de comunicación bajas.
La fiabilidad y reparabilidad es uno de los factores que intervienen en el diseño de las redes de interconexión, una red de interconexión ha de ser capaz de enviar información de forma fiable.
Son todas ciertas.
Todas tienen algún fallo, que hacen que no sean totalmente correctas.
Son todas falsas.
La a, b, c, e y f son correctas.
El coste de una red de este tipo es O(N M), donde N son las entradas y M las salidas de la red.
El coste de una red de este tipo es O(N M), donde N son las salidas y M las entradas de la red.
El coste de una red de este tipo es O(R), donde R es una ponderación de N (entradas) y M (salidas).
Es el número de ciclos de espera entre operaciones con dependencias 
Es el tiempo que se tarda en ejecutarse todas las instrucciones
Es el tiempo que se tarda en ejecutar una instrucción
El apoyo a la planificación de peraciones cuando el retardo del salto queda expuesto por la arquitectura
La asistencia a los mecanismos de predicción dinámicos y la determinación\nde los caminos de código más frecuentes
Disminución de los tiempos de espera debidos al retardo
Las opciones a y b
Una arquitectura para diseño de CPUs que implementa una forma de paralelismo a nivel de instrucción\n
Una arquitectura para diseño de CPUs que trabaja con multithreading
La evolución de la arquitectura EPIC
Una implementación del salto retardado
Tienen juegos de instrucciones muy complejos en cuanto a número de instrucciones diferentes, pero muy simples en cuanto al tamaño de las   instrucciones  \n
Aprovecha la información a priori de un salto para predecir en tiempo de \n    ejecución su comportamiento\n
Aprovecha la información a priori de un salto para predecir en tiempo de    \n    compilación su comportamiento\n
Es una técnica del compilador no aconsejable para mejorar el paralelismo a \n    nivel de instrucción\n
No trabajan con instrucciones 
El número de instrucciones diferentes y el tamaño de las mismas es muy simple 
Ninguna de las anteriores es cierta
El tiempo que tardan en ejecutarse las instrucciones 
Las máquinas superescalares no implementa paralelismo a nivel de instrucción\n
La diferencia únicamente tiene que ver con la complejidad del hardware
Compilador en tiempo de compilación
Cómo se construyen las instrucciones y cómo se lleva a cabo su planificación
procesador en tiempo de ejecución
La realiza en una primera fase la realiza el compilador y luego en ejecución el procesador.
Ninguna de las anteriores son correctas
La planificación recae exclusivamente en el compilador 
La planificación recae a partes iguales entre el compilador y software 
La planificación recae en el compilador y es apoyada por software
La planificación recae exclusivamente en el software
Es una tarea sencilla aunque presenta ciertas desventajas 
por la cual se solapa la ejecución de múltiples instrucciones.
Es una tarea muy compleja y presenta ciertas desventajas
Es una tarea muy compleja pero su portabilidad hace el proceso factible
Es una tarea sencilla pero su poca portabilidad hace que el proceso no sea \nfactible\n
para separar las instrucciones de datos de las de saltos.
Ninguna de las anteriores son correctas
No presenta ningún problema ya que el compilador los tiene en cuenta sin \nañadir mayor complejidad a su diseño \n
Vuelven inservible la utilidad del compilador 
El compilador debe realizar la planificación siempre teniendo en cuenta el peor caso, lo que se traduce en una pérdida importante de rendimiento\n
No pueden darse problemas de este tipo en un procesador VLIW 
Compilador en tiempo de compilación
procesador en tiempo de ejecución.
La realiza en una primera fase la realiza el compilador y luego en ejecución el procesador.
Ninguna de las anteriores son correctas.
Simplifica la arquitectura hardware y reduce el número de instrucciones de \nlos programas \n
Simplifica el compilador y reduce el número de instrucciones de los   \nprogramas \n
Presenta una alta compatibilidad hacia atrás y simplifica la arquitectura \nhardware al no tener que planificar código\n
Complica la arquitectura hardware y aumenta el número de instrucciones \nde los programas \n
El hardware
Ninguno, todo son ventajas 
El software (compilador). 
El hardware y el software
Requiere de compiladores complejos y presenta una compatibilidad hacia \natrás nula\n
Aumenta el número de instrucciones y presenta una compatibilidad hacia \natrás nula\n
Es el número de instrucciones que pueden ejecutarse por ciclo en las\nmáquinas que implementan el paralelismo a nivel de instrucción \n
Superescaleres
Segmentadas
VLIW
 Ninguna de las anteriores son correctas \t\n
Aprovecha la información a priori de un salto para predecir en tiempo de \nejecución su comportamiento \n
Aprovecha la información a priori de un salto para predecir en tiempo de    \ncompilación su comportamiento\n
Es una técnica del compilador no aconsejable para mejorar el paralelismo a \nnivel de instrucción\n
Ninguna de las anteriores es cierta
Computación de Instrucciones paralelas explícitas 
Computación de Instrucciones paralelas enumeradas.
Ciclo independiente paralelo de ejecución
Ninguna de las anteriores es correcta. 
Ninguna 
Distintas iteraciones del bucle precisan del uso de distintos registros para \nevitar riesgos de datos\n
Para eliminar operaciones de incremento del registro base para acceder a \nmemoria, debe integrarse esa operación en el propio acceso a memoria como Offset\n
Las opciones b y c
Planificación dinámica y estática.
Planificación  dinámica e híbrida
Planificación estática e híbrida
Ninguna es cierta 
Si se desenrolla demasiadas veces, llega un momento en que no se incrementa el paralelismo 
El tamaño del código aumenta conforme se desenrolla y se puede dejar a la máquina sin registros disponibles
Limitaciones propias de los registros
Las opciones a y b
1
2
3
4
Es un desenrollado de bucles llevado al extremo 
Es la combinación entre desenrollado de bucles y planificación de bloques \nbásicos \n
Ninguna es cierta
Trace Cache, Especulación y Predicción.
Predicción y Trace Cache
No
Especulación y Predicción
Dependiendo de las optimizaciones locales que se realicen
Ninguna es cierta
Se obtiene un código más corto y el rendimiento es más alto
Se obtiene un código más corto y aumenta la portabilidad del mismo 
en replicar a las unidades funcionales del procesador para lanzar a ejecutar varias instrucciones en varios ciclos. 
Si, ambas técnicas intentan reducir la sobrecarga de operaciones debida a\ndemasiados saltos o incrementos de contadores \n
en replicar las estaciones de reservas del procesador para lanzar a ejecutar varias instrucciones en cada ciclo.
Si, ambas técnicas intentan reducir el tiempo que el bucle no está ejecutándose \na pleno rendimiento \n
en replicar las unidades funcionales del procesador para lanzar a ejecutar varias instrucciones en cada ciclo
No, el desenrollado se centra en la reducción del tiempo que el bucle no está\nejecutándose a pleno rendimiento mientras que el software pipeling se centra en \nla reducción de la sobrecarga de operaciones \n
La realización de un conjunto de instrucciones como paquete (bundle en inglés).
La utilización de conjunto de instrucciones de carga especulativa como un tipo de búsqueda anticipada de datos. 
La realización de un conjunto de instrucciones como paquete (bundle en inglés). y la utilización de una instrucción de carga especulativa como un tipo de búsqueda anticipada de datos
Ninguna de las anteriores es cierta
No es necesario
Hacer más rápido el caso más frecuente
El procesador debe duplicar el estado de cada thread para permitir este tipo de \nejecución: bancos de registro, PC, tabla de páginas,…\n
El hardware debe adaptarse para permitir un cambio de contexto rápido entre \nthreads\n
Las opciones b y c
VLIW
Superescalares
EPIC
Segmentada
La única diferencia es el nombre, que hace referencia al número de threads que\nintervienen en la ejecución\n
El MT de grano fino utiliza un thread por cada instrucción lo que permite \nocultar la pérdida de rendimiento debida a burbujas cortas o excesivamente \nlargas. El MT de grano grueso, al usar un único thread el máximo tiempo \nposible, es limitado a la hora de recuperarse en estas pérdidas de rendimiento\n
Aumentar el número de ocurrencias de saltos. 
El MT de grano fino no suele ralentizar la ejecución de un único thread \nmientras que el MT de grano grueso si\n
Las opciones b y c
Decrementar la ejecución especulativa. 
Incrementar el número de ocurrencias de los saltos y e decrementar la ejecución especulativa de instrucciones. 
Si,  ya que un único thread no suele ser capaz de proporcionar suficientes\ninstrucciones como para mantener ocupados todos los recursos de la máquina\n
No, es una evolución obligada de MT pero no aporta ninguna mejoría significativa \n
No, ya que un único thread puede mantener ocupados todos los recursos de la \nmáquina\n
Si, ya que había que evolucionar las máquinas superescalares para asegurar su \ncontinuidad en el tiempo\n
cargar instrucciones y datos a la CPU antes de que sean actualmente necesitados o no si no lo son 
cargar instrucciones y datos a la CPU cuando ésta los necesite.
almacenar datos a la memoria cuando la CPU se lo indique. 
Ninguna de las anteriores es correcta.
La ampliación de los registros físicos y la unión lógica del reorder buffer con los\nthreads que se ejecuten\n
El renombrado de registros y la graduación independiente de instrucciones
Aumento del tamaño de los bancos de registros y la separación lógica del reorder buffer para cada thread\n
Ninguna de las respuestas anteriores es cierta
que se van a usar futuramente y limitar los cálculos innecesarios.\n
que estás usando actualmente, y esto limita los cálculos innecesarios.\n
que se están usando actualmente, como también futuramente para así disminuir casi totalmente los cálculos innecesarios.\n
Ninguna de las anteriores es correcta.\n
La sobrecarga en el ciclo de reloj y el uso de bancos de registros lo \nsuficientemente grandes para mantener múltiples contextos \n
La sobrecarga en el ciclo de reloj y el aumento de la latencia debido a los \nconflictos de caché \n
La imposibilidad de realizar renombramiento de registros debido a que el banco de registros no es suficientemente grande \n
Todos son ciertas
Es la capacidad de dividir una instrucción y ejecutarla.\n
Es la capacidad potencial de un grupo de instrucciones de poder ser ejecutadas en paralelo. \n
es la capacidad potencial de agrupar en un conjunto, un subconjunto de instrucciones y ejecutar ese conjunto.\n
 Es la capacidad potencial de agrupar a un conjunto de instrucciones, ejecutarlas en paralelo, siempre y cuando estas sean del mismo tipo de operación.\n
superescalar + segmentado = supersegmentado,\n
segmentado + segmentado = supersegmentado \n
segmentado + sub-segmentado = supersegmentado\n
Ninguna de las anteriores son correcta\n
Cuando la ejecución previa requiere menos tiempo que el que requería en la ejecución posterior.\n
Cuando la ejecución previa requiere menos espacio que el que requería en la ejecución posterior. \n
Cuando la ejecución previa requiere menos espacio y tiempo que el que requería en la ejecución posterior\n
Ninguna de las anteriores es correcta.\n
 Captura de la instrucción\t\n \n
Si los operandos de entrada están disponibles, la instrucción es enviada a la correspondiente unidad funcional. Si alguno de ellos no lo están, el procesador inserta burbuja hasta poder utilizar el dato \t\n
 La instrucción es ejecutada por la unidad funcional adecuada.\t\n \n
Se envía el resultado a cola.
Más pequeño es más rápido.
Genera 9 fallos de página.
Cada proceso tiene su tabla de páginas y en algunos sistemas se almacena una tabla de páginas por proceso en memoria principal.
Cada proceso tiene su tabla de páginas y puede suceder que cada proceso ocupe una cantidad de memoria virtual considerable, con lo que la tabla aumenta, para resolver el problema, se almacenan las tabla de páginas en memoria virtual.
Cada proceso tiene su tabla de páginas y puede suceder que cada proceso ocupe una cantidad de memoria virtual enorme, con lo que la tabla aumenta, para resolver el problema, algunos sistemas usan un esquema a dos niveles de tablas de páginas, donde cada entrada del directorio de páginas señala a una tabla de páginas.
Si el sistema almacena la tabla de páginas de cada proceso en memoria virtual, el sistema obliga a que, al menos una parte de la tabla de páginas del proceso en ejecución esté en memoria principal, incluyendo la entrada de la tabla de páginas para la página actualmente en ejecución.
Todas las respuestas anteriores son ciertas.
Si un elemento es referenciado, es muy probable que vuelva a ser referenciado en un periodo corto de tiempo
Si un elemento es referenciado, los elementos cercanos a él tenderán a ser referenciados pronto
Todas las respuestas son ciertas
Únicamente las respuestas 1 y 3 son ciertas
A la tabla de páginas de cada proceso cuando esta se almacena en memoria principal, en vez de en memoria virtual, esta técnica se utiliza cuando las tablas son reducidas.
A la tabla de páginas en el que a la dirección virtual se le aplica una función de hashing, para que la estructura de datos sólo necesite tener el tamaño del número de marcos físicos de la memoria principal.
A la tabla de páginas en el que a cada entrada del marco de página se le aplica una función de hashing, más los bits de control de protección, de esta manera se ahorra espacio y tiempo de traducción ya que con una operación tenemos todos los datos referentes a la entrada.
Las memorias más grandes tienen más retardo y necesitan más niveles para decodificar las direcciones
Todas las respuestas anteriores son falsas.
Las memorias más pequeñas son más rápidas pero son más caras de implementar
Aumentar la funcionalidad del Sistema operativo hace que se dedique más hardware a otro tipo de componentes
Todas las respuestas son ciertas
Es la cantidad de información máxima que se transfiere a través del bus de datos
Es la unidad mínima de información que puede estar presente o no en uno de los niveles de la jerarquía
Es la unidad de información máxima que se puede asignar a un proceso
Es la cantidad de información que se gestiona en un nivel de la jerarquía
Todas las repuestas son falsas
Consiste en utilizar una tabla de páginas invertida. Esta solución dificulta la traducción ya que se tiene que se debe recorrer cada entrada de la tabla de páginas buscando la página que corresponde con el código hash especificado y no como antes que el código era el índice de la tabla. 
Las respuestas 3 y 4 son ciertas
Consiste en utilizar una caché de traducción de direcciones, denominada TLB (búfer de traducción adelantada) que contiene las traducciones de las direcciones de memoria utilizadas hace menos tiempo.
Consiste en mejorar las técnicas de gestión de reemplazo y de ubicación, de esta forma se reduce la frecuencia de fallos.
Las respuestas 1 y 2 son correctas.
Todas las respuestas con ciertas.
El ancho de banda está relacionado con el tiempo de transferencia de datos de memoria
El tiempo que tarda el sistema en reemplazar un bloque de memoria de un nivel inferior cuando ocurre un fallo se denomina tiempo de transferencia
Al aumentar el tamaño de las páginas se reduce la fragmentación interna y se favorece el tiempo de carga de los procesos.
La latencia está relacionada con el tiempo de acceso a la memoria
Al aumentar el tamaño de página disminuye la tasa de fallos hasta un cierto umbral, ya que cada página individual contendrá posiciones cada vez más distantes de cualquier referencia reciente.
Al aumentar el tamaño de las páginas se reduce el tamaño de la tabla de página, y se aumenta la eficacia de las escrituras en la memoria secundaria.
La Penalización de fallo es el tiempo que tarda el sistema en reemplazar un bloque de memoria desde un nivel inferior en caso de fallo
El TLB constituye una herramienta eficaz para aumentar el rendimiento cuando los tamaños de página son grandes, debido a que reducen la búsqueda en la tabla de páginas.
8) La 2 y 3 son ciertas
La implementación de jerarquía de memoria tiene como objetivo incrementar el rendimiento del sistema
Las técnicas de gestión de los niveles de memoria están ubicadas en el sistema operativo de la máquina y tienen el objetivo aumentar el rendimiento
Las respuestas 1, 3, 4 y 5 son ciertas, el resto falsas
Todas las respuestas son ciertas
La frecuencia de fallo aumenta.
La frecuencia de fallo aumenta hasta un umbral a partir del cual comienza a disminuir.
A la dirección virtual 622 le correspondería la página 1 con un desplazamiento igual a 110.
La frecuencia de fallo se mantiene hasta que el tamaño de bloque es tan grande que deja huecos de memoria que no se utilizan.
A la dirección virtual 622 le correspondería la dirección física 1646.
No genera fallos de página.
Genera 7 fallos de página.
Genera 6 fallos de página.
Dada la cadena de referencia 2,3,4,2,1,5 y usando la política de sustitución óptima, tras la primera referencia (pág. 2) se producirá un fallo de página y se seleccionará como víctima a ser remplazada la página 9.
La frecuencia de fallo aumenta hasta que el tamaño de bloque es tan grande que deja huecos de memoria que no se utilizan.
Todas las respuestas son falsas.
Dada la cadena de referencia 2,3,4,2,1,5 y usando la política de sustitución óptima, tras la segunda referencia (pág. 3) se producirá un fallo de página y se seleccionará como víctima a ser remplazada la página 5.
Todas las respuestas anteriores son correctas.
Alguna de las respuestas anteriores no es correcta.
Están relacionadas con la decisión de cuando se debe cargar una página en memoria principal.
La paginación multinivel es una de las alternativas más utilizadas.
Las alternativas más comunes son paginación por demanda y paginación previa.
La paginación previa no puede utilizarse cuando un proceso se carga por primera vez.
La paginación previa es una forma de reemplazo.
Es la cantidad de información máxima que se transfiere a través del bus de datos.
Es la unidad mínima de información que puede estar presente o no en uno de los niveles de la jerarquía.
En un esquema que utiliza la paginación por demanda, se tiene en cuenta el principio de cercanía de referencias a la hora de cargar páginas del disco.
Es la unidad de información máxima que se puede asignar a un proceso.
Ninguna de las anteriores es cierta.
Es la cantidad de información que se gestiona en un nivel de la jerarquía.
Tanto 1 como 2 son correctas.
Todas las repuestas son falsas.
Tanto 1 como 3 son correctas.
Las respuestas 1, 3 y 5 son correctas.
Las respuestas 3 y 4 son ciertas.
Sí, normalmente el tamaño se respeta para facilitar el acceso desde la CPU.
No, el tamaño aumenta a medida que se aleja de la CPU.
Depende del diseñador de la máquina.
Todas las repuestas son falsas.
Algunas de las políticas se basan en reemplazar la página que tenga una menor probabilidad de ser referenciada en un futuro cercano mientras que otras reemplazan la página con mayor probabilidad de ser referenciada.
Están estrechamente relacionadas con el principio de localidad temporal.
Las respuestas 1 y 3 son ciertas.
Interesa que la política sea lo más elaborada y sofisticada posible para obtener los mejores resultados.
Cualquier página cargada en memoria puede ser reemplazada.
Si una página está cargada en un marco que está bloqueado no podrá ser reemplazada.
Las respuesta 1 y 4 son correctas.
Las respuestas 2 y 5 son correctas.
Las respuestas 1 y 5 son incorrectas.
Se utiliza la frecuencia de fallos ya que es una medida independiente de la arquitectura.
Se utiliza el tiempo de acceso medio a la memoria ya que es una medida indirecta que da una buena aproximación.
Se utiliza el tiempo de acierto, que en el caso de las memorias caché determina el tiempo de ciclo de la CPU.
Todas las respuestas anteriores son falsas.
Todos los datos de un nivel en la jerarquía de memoria, se encuentran contenidos en los siguientes sucesivamente.
Una jerarquía de memoria es una combinación del principio de localidad y la tecnología.
Si se referencia un elemento de memoria, los elementos cercanos a él tenderán a ser referenciados pronto.
Todas las respuestas son ciertas.
Únicamente las respuestas 2 y 3 son ciertas.
Es la técnica de sustitución implementada que proporciona el menor número de fallos de página.
Es la técnica más implementada en los sistemas de memoria virtual actuales debido a su alto rendimiento.
Es una técnica que permite comparar el rendimiento de otros algoritmos realizables.
Es una técnica que sustituye la página que vaya a tardar más tiempo en ser referenciada en el futuro.
Las dos primeras respuestas son falsas. 
Memoria caché.
Registros.
Registros en un punto intermedio entre la caché y la CPU.
Todas las respuestas son ciertas.
La respuesta 1 y 2 son ciertas.
Ninguna de las anteriores es cierta.
Están administrados por el Sistema operativo.
Están administrados por la CPU.
Están administrados vía hardware.
Todas las respuestas son falsas.
Está administrada por el Sistema operativo de la máquina.
Está administrada por el hardware del sistema.
Está administrada por el sistema operativo, aunque algunas operaciones están implementadas en el hardware.
Es una técnica utilizada para buscar datos en memoria secundaria.
Todas las respuestas son falsas.
Tiene la ventaja de que proporciona un rendimiento aceptable utilizando una implementación muy sencilla.
Es una técnica de sustitución de páginas que elimina de memoria la última página usada (Last Recently Used).
Es una técnica de sustitución de páginas que elimina de memoria la página menos recientemente usada (Least Recently Used).
Es un algoritmo eficaz cuando la memoria principal es mayor que el conjunto de trabajo.
Ninguna de las anteriores es cierta.
Las respuestas 4 y 5 y 6 son correctas.
Las respuestas 2 y 5 y 6 son correctas.
Son RAM estáticas , es decir, que no es necesario refrescar los datos, ya que sus celdas mantienen los datos siempre que estén alimentadas.
Debido a su alto coste de fabricación y a su velocidad suelen utilizarse para implementar memorias caché.
Es una memoria de acceso aleatorio, es decir que se puede acceder a cualquier byte de memoria sin tener que acceder a los bytes precedentes.
Todas las repuestas son ciertas.
Únicamente las respuestas 1 y 3 son ciertas.
Es una memoria que se denomina dinámica, ya que para mantener un dato en memoria es necesario refrescarlo cada cierto tiempo.
Es una memoria de acceso aleatorio, es decir que se puede acceder a cualquier byte de memoria sin tener que acceder a los bytes precedentes.
Es una memoria que tiene la ventaja de contar con mayor densidad de celdas de memoria por conexiones.
Todas las respuestas son ciertas.
Únicamente las respuestas 1 y 2 son ciertas.
Direcciones físicas.
Direcciones virtuales.
Direcciones lineales.
Ninguna de las anteriores es cierta.
Es una técnica que permite al software usar más memoria principal que la que realmente posee el ordenador.
Es un tipo de memoria que constituye el tercer nivel de la jerarquía de memoria.
Es una técnica de gestión de memoria que gestiona los niveles de caché y memoria principal de la jerarquía.
Las respuestas 1 y 2 son ciertas.
Todas las respuestas son falsas.
para evitar la serialización innecesaria de las operaciones de los programas impuesta por la no reutilización de los registros de procesador.  \n
Ninguna de las anteriores es cierta.  \n
Es una técnica que se basa en el concepto de traza 
El compilador tiene todo el peso en la obtención del paralelismo a nivel de instrucción y la compatibilidad entre diferentes generaciones.\n
Se conoce también como "grado de multiprogramación".
Adecuar los recursos hardware a la ejecución de varias instrucciones en paralelo\n
Determinar las relaciones de dependencia entre estas instrucciones.\n
Diseñar estrategias que permitan determinar cuándo una instrucción está lista para ejecutarse.\n
Construir una estrategia de pasos de testigo para la ejecución de las instrucciones más prioritaria.\n
Emisión múltiple de instrucciones.\n \n\t\t
Ejecución especulativa.\n \n
Supersegmentación.\n 
Renombre de instrucciones.\n
No genera fallos de página.
Genera 7 fallos de página.
Genera 6 fallos de página.
Genera 9 fallos de página.
LOAD especulativo \t\t\n \n
Ninguna de las anteriores es cierta.
STORE especulativo.\t\t\n
LOAD y STORE especulativo. \t\t\n 
Ninguna de las anteriores \t\t\n\n
Saltos de predicados \t\n \n
Excepciones retardadas. \t\n \n
Instrucciones de saltos de múltiples instrucciones \t\n 
\nRenombrar de registros\t\n
Saca la página de la primera posición de la tabla de páginas.
Saca la página que lleva más tiempo cargada.
Para su implementación, el sistema operativo mantiene una lista de las páginas que están en memoria ordenadas por el tiempo que llevan residentes.
Es un algoritmo cuya implementación es bastante compleja.
Anular los efectos de la latencia de memoria.\n
Proporciona un rendimiento bastante bueno.
Aumentar los efectos de la latencia de memoria.\n
 Limitar los efectos de la latencia de memoria.\n
Solamente la 1 y la 3 son ciertas.
Ninguna de las anteriores son correctas.\n
Solamente la 2 y la 3 son ciertas.
La frecuencia de instrucciones ejecutadas es mayor que la frecuencia de reloj\n
Encontrar en cada ciclo varias instrucciones independientes entre sí que se puedan lanzar a ejecutar.\n
Restricciones reales: tipos de instrucciones que se pueden ejecutar simultáneamente y qué ocurre cuando aparecen riesgos.\n
Todas son correctas.\n
Es una técnica del compilador para explotar el ILP 
para evitar la serialización necesaria de las operaciones de los programas impuesta por la reutilización de los registros de procesador.\n\n
Es una implementación concreta de la planificación global 
para evitar la serialización innecesaria de las operaciones de los programas impuesta por la reutilización de los registros de procesador.  \n\n
Todas son ciertas
El compilador es el responsable de explotar el paralelismo haciéndolo explícito en el código máquina.\n
Se añade información sobre la dependencia entre instrucciones.\n
Predicación, Especulación y Multiway Branching
Multiway Branching, Trace Scheduling y Software pipelining
Todas la anteriores son ciertas. \n
Especulación, Predicación y Trace Scheduling
Ninguna es cierta 
El compilador tiene parte del peso en la obtención del paralelismo a nivel de instrucción y la incompatibilidad entre diferentes generaciones.\n
8) Son todas falsas.
El compilador tiene todo el peso en la obtención del paralelismo a nivel de instrucción y la incompatibilidad entre diferentes generaciones.\n
Ninguna de las anteriores es correcta.\n
Mecanismo hardware que establece condiciones a las operaciones de manera que se evalúen como parte de su ejecución 
Técnica del compilador que establece condiciones a las operaciones de manera que se evalúen como parte de su ejecución
Mecanismo hardware que planifica las operaciones de los dos caminos de un salto condicional simultáneamente 
Es una técnica que se considera como el contrapunto de la ejecución especulativa de las máquinas superescalares
Captura de la instrucción, envío de la instrucción a la unidad funcional para ejecutar, envío del resultado a la cola, escritura de la instrucción. \t\n 
Captura de la instrucción, envío de la instrucción a una cola en espera de la disponibilidad de los operandos, envío de la instrucción a la unidad funcional para ejecutar, escritura de la instrucción.\t\n \n
Captura de la instrucción, envío de la instrucción a la unidad funcional para ejecutar, envío del resultado a la cola, escritura de la instrucción.\t\n \n
Captura de la instrucción, envío de la instrucción a una cola en espera de la disponibilidad de los operandos, envío de la instrucción a la unidad funcional para ejecutar, envío del resultado a la cola, escritura de la instrucción.\t\n
Mecanismo hardware que establece condiciones a las operaciones de manera \nque se evalúen como parte de su ejecución\n
Mecanismo hardware que soluciona dos grandes problemas: el control de las \nexcepciones y la posibilidad de adelantar especulativamente la ejecución de \noperaciones de memoria que pueden tener conflictos de direcciones \n
Es una técnica que se considera como el contrapunto de la ejecución \nespeculativa de las máquinas superescalares\n
Ninguna es cierta 
Las máquinas VLIW realizan la planificación de forma estática mediante\nsoftware mientras que las superescalares  la realizan de forma dinámica\nmediante hardware\n
Las máquinas VLIW pueden escribirse en lenguaje ensamblador mientras que\nen las superescalares es inabordable\n
La frecuencia de reloj es mayor en las máquinas superescalares 
El compilador es más complejo en las máquinas VLIW 
La portabilidad del código es mejor en las máquinas superescalares 
En las VLIW se construyen instrucciones multioperación mientras que en las\nsuperescalares se aprovecha el mismo flujo del código secuencial para emitir \nmás de una instrucción por ciclo\n
Atraviesa los límites de los bloques básicos e incluso de subsiguientes iteraciones de un bucle \n
Presta especial atención en preservar las dependencias de datos y de control
En muchas ocasiones requiere que se añada un código de resguardo
Todas son ciertas 
Las opciones a y c
Predicación, Perfiles y Predicción en una única dirección 
Predicción en una única dirección, Predicción basada en el programa y Perfiles 
Salto retardado, Predicción en una única dirección y Trace Scheduling
Ninguna es cierta 
Genera 9 fallos de página.
Genera 7 fallos de página.
Genera 6 fallos de página.
No genera fallos de página.
Ninguna de las anteriores es cierta.
Es una esquema del que han partido muchos algoritmos diseñados con el fin de para aproximarse al rendimiento de la estrategia LRU sin introducir mucha sobrecarga.
Es un esquema en el que los marcos candidatos a ser reemplazados son considerados como un buffer circular con un puntero asociado.
Su forma más sencilla asocia un "bit de modificación" a cada marco, de tal manera que cuando se carga por primera vez una página en un marco, su bit de modificación se pone a uno.
Es un esquema en el que cualquier marco con el "bit de modificación" puesto a uno es descartado como candidato para ser reemplazado.
Todas las anteriores son ciertas.
Las respuestas 1 y 2 son ciertas.
Las tres primeras respuestas son ciertas.
Genera 7 fallos de página.
Genera 9 fallos de página.
Genera 6 fallos de página.
No genera fallos de página.
Ninguna de las anteriores es cierta.
Todas las anteriores son ciertas.
Una vez cargadas las cuatro primeras páginas en memoria, tras la referencia al resto de las páginas de la lista se producirán 4 fallos de página si se utiliza FIFO como algoritmo reemplazo.
Supongamos que se utiliza el algoritmo del reloj como algoritmo de reemplazo de páginas, y que se ha hecho referencia a todas las páginas de la lista. Si a continuación se necesita la página 3 se producirá un fallo de página.
Supongamos que se utiliza el algoritmo del reloj como algoritmo de reemplazo de páginas, y que se ha hecho referencia a todas las páginas de la lista. Si a continuación se necesita la página 7 se expulsará a la página 4.
Ninguna de las anteriores es cierta.
La más antigua y cuyo bit de uso es 0.
La más antigua y cuyo bit de uso es 1.
La que no ha sido utilizada durante un periodo de tiempo mayor y su bit de uso es 0.
La que no ha sido utilizada durante un periodo de tiempo mayor y su bit de uso es 1.
Ninguna de las anteriores es cierta.
Cada página que se carga se sitúa en una lista de páginas modificadas.
Es una estrategia que mejora el rendimiento de la paginación y permite el uso de una política de reemplazo más sencilla.
En redes multibus y basadas en cluster.
Cada página reemplazada se asigna a una lista de páginas libres en el caso de que no haya sido modificada.
Esta estrategia mantiene la página a reemplazar en memoria, de tal manera que si el proceso hace referencia a dicha página, se devuelve al conjunto residente del proceso con un coste pequeño.
Las páginas modificadas son reescritas por bloques, en lugar de una en una.
El número de operaciones de E/S se reduce significativamente.
Todas las anteriores son ciertas.
La 2, 3, 4, 5 y 6 son ciertas, pero no la 1.
Sólo las respuestas 2, 4, 5 y 6 son ciertas.
Por encima de un determinado tamaño, la asignación de memoria adicional a un proceso en particular tendrá efectos notables en el porcentaje de fallos de página para ese proceso, debido al principio de cercanía.
La gestión del conjunto residente se encarga de asignar el número de marcos de página que se va a asignar a cada programa instalado en el computador.
Existen dos tipos de políticas de gestión del conjunto residente, la política de asignación fija y la política de asignación variable.
Se denomina conjunto residente al conjunto de páginas de un proceso a las que se va a hacer referencia en el próximo intervalo de tiempo.
La política de asignación fija otorga a cada proceso un número fijo de páginas. En caso de que se produzca un fallo de página se puede reemplazar cualquiera de las páginas.
Ninguna de las anteriores es cierta.
Las afirmaciones 3, 4, 5 y 6 son ciertas. 
La política de asignación variable permite asignar a priori a cada proceso un número de páginas en función del tamaño del mismo. Así, procesos diferentes pueden tener asignados un número de páginas distinto.
Asignación variable y alcance local.
Asignación variable y alcance global.
Asignación fija y alcance local.
Asignación fija y alcance global.
Todas las relaciones anteriores son posibles.
Todas las relaciones son posibles excepto la 4.
Sólo las relaciones comentadas en 2 y 3 son posibles.
El conjunto de trabajo de un proceso en un instante virtual t y con parámetro A, denotado por W(t, A), es el conjunto de páginas a las que el proceso ha hecho referencia en las últimas A unidades de tiempo virtual.
El "tiempo virtual" representa el tiempo que transcurre mientras que el proceso está realmente en ejecución.
El tamaño del conjunto de trabajo a lo largo del tiempo para un valor fijo de A puede variar.
Una medida real del conjunto de trabajo para cada proceso es impracticable.
Un proceso puede ejecutarse sólo si su conjunto de trabajo está en memoria principal, esto es, si su conjunto residente incluye a su conjunto de trabajo. 
Tanto el tamaño como el contenido del conjunto de trabajo cambiarán con el tiempo.
Incorpora implícitamente el control de carga.
Todas las afirmaciones anteriores son ciertas.
Sólo las cinco primeras son ciertas.
Si la tasa de fallos de página de un proceso es inferior a un umbral mínimo, el sistema al completo puede beneficiarse asignando un tamaño menor para el conjunto residente del proceso, sin perjudicar al proceso.
Si la tasa de fallos de página de un proceso es superior a un umbral máximo, el proceso puede utilizar un conjunto residente mayor sin degradar el sistema.
El algoritmo exige que se asocie un "bit de uso" a cada página de memoria. El bit se pone a 1 cuando se accede a la página.
No rinde de un modo adecuado durante los periodos transitorios, cuando se produce un desplazamiento a una nueva ubicación.
Puede emplear dos umbrales: uno superior que se usa para activar la reducción del tamaño del conjunto residente y otro inferior, que se usa para provocar un crecimiento.
Al contrario del algoritmo del conjunto de trabajo no incorpora implícitamente el control de carga.
Todas las afirmaciones anteriores son ciertas.
Sólo las dos primeras afirmaciones con ciertas.
Las cuatro primeras afirmaciones son ciertas.
Evalúa el conjunto de trabajo de un proceso tomando muestras en función del tiempo virtual transcurrido.
El tamaño del conjunto residente puede disminuir tanto al principio como al final de un intervalo.
Reduce la proporción de páginas no usadas que abandonan el conjunto residente cuando aumenta la tasa de paginación.
Su implementación es bastante más compleja que la del algoritmo PFF.
Ninguna de las anteriores es cierta.
Las dos primeras afirmaciones son ciertas.
Es una manera alternativa de denominar a las políticas de reemplazo. Se encargan de determinar que página será la candidata a ser reemplazada cuando se produce un fallo de página.
Las alternativas más comunes son el vaciado por demanda y el vaciado previo.
Aborda el fenómeno conocido como "hiperpaginación".
Se encargan de determinar en qué momento hay que escribir en memoria secundaria una página modificada.
La manera de determinar si una página ha sido modificada desde que se cargo en memoria es mediante la utilización de un "bit de uso".
Con el vaciado previo, una página se escribe en memoria secundaria en cuanto es modificada. En ese mismo momento pasa a una lista de páginas modificadas de la que será suprimida en cuanto sea seleccionada por una estrategia de reemplazo.
Las respuestas 2 y 3 son correctas.
Las respuestas 2, 3 y 5 son correctas.
Se lleva a cabo una agrupación lógica de la información en bloques de tamaño variable denominados segmentos.
En el caso de que todos los segmentos sean de igual longitud el sistema degenera en un sistema paginado, con la salvedad de que en un sistema paginado el funcionamiento es absolutamente transparente al usuario, mientras que en un sistema segmentado no es así.
La longitud de cada segmento puede ser cualquiera desde 0 hasta el tamaño máximo. Se fija en el momento de la carga y no puede cambiar durante la ejecución.
Cada proceso tiene su propia tabla de segmentos.
Todas las afirmaciones anteriores son ciertas.
Las respuestas 1 y 2 y 4 son ciertas.
La dirección virtual consta de un número de segmento y un desplazamiento dentro del segmento.
La dirección virtual consta de un número de marco más un desplazamiento dentro del marco.
La dirección virtual consta de un número de segmento y una dirección física asignada.
Ninguna de las anteriores es cierta.
Tanto 1 como 3 son ciertas.
Se denominan “descriptores de segmentos”.
Contienen un campo que indica la longitud del segmento.
Contienen un campo denominado "dirección base del segmento" que indica la dirección de  memoria secundaria correspondiente al comienzo del segmento.
Contienen el campo "dirección base del segmento" en el caso de que el segmento no esté en memoria principal.
Todas las respuestas anteriores son ciertas.
Sólo las dos primeras respuestas son ciertas.
Elimina el problema de la fragmentación externa.
El programador debe despreocuparse de especificar los segmentos del programa, puesto que de esa tarea se encarga el sistema operativo.
Da soporte a métodos más potentes de protección y compartición en un espacio de direcciones.
La carga de un segmento en memoria principal no requiere la descarga de segmentos al disco.
Todas las anteriores son ciertas.
1) La mayor computadora Myrinet en España alcanza los 94,21 teraflops con 10.240 procesadores.
El algoritmo de fallo de segmento es más complicado que el de fallo de página.
Puede presentar hiperpaginación.
Puede requerir compactación tras haber extraído de la memoria múltiples segmentos víctimas.
Las respuestas 1, 2 y 3 son ciertas.
Sólo las respuestas 2 y 3 son ciertas.
ServerNet es el sucesor de Myrinet.
ServerNet es una red de baja latencia que empieza a popularizarse en la actualidad.
ServerNet se ha usado como modelo en otras redes de la actualidad.
Es un fenómeno que sucede como consecuencia del movimiento de particiones variables en la memoria que van dejando huecos de direcciones que no se utilizan.
Es un fenómeno que puede darse en cualquier sistema de memoria virtual: sistemas paginados, sistemas segmentados o sistemas con paginación y segmentación combinadas.
Es un fenómeno que puede resolverse reorganizando las particiones de manera contigua de forma que la memoria de los huecos pueda ser reasignada.
Es un fenómeno común en los sistemas paginados que realizan un uso ineficiente de la memoria debido a la aparición de porciones inutilizada al final de cada partición.
Las tres primeras son correctas.
Las respuestas 1 y 3 son correctas.
Utiliza conmutadores ASIC que permiten conexiones en cascada.
Utiliza procesadores superSPARC.
Utiliza procesadores RISC de 32 MB de memoria.
Ninguna es correcta.
Un programa puede acceder sólo a datos que estén en el mismo anillo o en un anillo más interno (con un número menor).
Un programa puede hacer llamadas a servicios que residan en el mismo anillo o en anillos más externos (con números mayores). 
Los anillos externos (de mayor numeración) son los que gozan de mayores privilegios.
Sistema tolerante a fallos de las redes ServerNet.
Los anillos internos (de menor numeración) son los que disponen de menos privilegios.
Servidores tolerantes a fallos en los que se basa ServerNet.
Todas las anteriores son ciertas.
Línea de servidores integrados en las redes ServerNet.
Todas las anteriores son falsas.
Ninguna es correcta.
A la dirección virtual compuesta por el número de segmento 0 y el desplazamiento 128 le corresponde la dirección física válida 628.
A la dirección virtual compuesta por el número de segmento 3 y el desplazamiento 558 le corresponde la dirección física válida 2634+558.
2) Cuando en la lectura de un dato se devuelve el valor del dato más recientemente escrito. 
Para la dirección virtual compuesta por el número de segmento 0 y el desplazamiento 950 se produce una violación de segmento.
1) Cuando se determina cuando un valor debe ser devuelto por una lectura. 
Todas las anteriores son ciertas.
3) Cuando se escribe un dato en una dirección de memoria por primera vez. 
Ninguna de las anteriores es cierta.
4) Cuando se modifica un dato de memoria en un sistema uniprocesador. 
5) Cuando un dispositivo de E/S modifica un dato de memoria principal que ha sido modificado previamente en la memoria cache de un procesador. 
6) La 2, 3 y 6 son falsas.
7) Son todas falsas.
8) Son todas ciertas.
Puede ocurrir la incoherencia sobre sistemas uniprocesadores. 
La incoherencia en sistemas multiprocesador se produce cuando un procesador lee un dato y a su vez otro procesador escribe sobre esa misma dirección, únicamente. 
Cuando se lee una dirección de memoria se devuelve el primer dato escrito sobre ella. 
Influyen las caches cuando se tiene un sistema que hace uso de múltiples procesos en un procesador. 
Son todas falsas.
Son todas ciertas.
En un sistema de particiones fijas.
En un sistema de particiones variables para solucionar el problema de la fragmentación interna.
En un sistema de memoria virtual paginada.
Ninguna de las anteriores es cierta.
El método de recubrimientos ("overlays") se puede utilizar en un sistema que no gestione memoria virtual.
1) Los sistemas multiprocesadores no producen falta de coherencia. 
2) La falta de coherencia se solventa generando accesos en orden a dicha dirección. 
El espacio de direcciones físicas depende del bus de direcciones del sistema.
3) Una forma de hacer que el sistema de un multiprocesador sea coherente consiste en asegurar que todas las actualizaciones o invalidaciones de una dirección parten del mismo punto del sistema de comunicación. 
4) Una forma de hacer que el sistema de un multiprocesador sea coherente consiste en asegurar que todas las actualizaciones o invalidaciones de una dirección parten del mismo punto del sistema de comunicación, y si es necesario no comenzar la nueva escritura hasta que se haya recibido reconocimiento de la escritura anterior. 
El espacio de direcciones virtuales está limitado por el tamaño de la memoria física.
5) Son todas correctas menos la 1.
La gestión de memoria basada en particiones fijas utiliza el método de compactación para resolver el problema de fragmentación externa.
6) Son todas correctas menos la 3.
7) Son todas correctas.
Todas las respuestas anteriores son verdaderas.
Sólo las dos primeras son verdaderas.
Es un sistema en el que el espacio de direcciones de un usuario se divide en varias páginas que a su vez se dividen en segmentos.
1) Las caches locales suponen algún beneficio cuando se comparten muchos datos. 
Es un sistema en el que cada dirección virtual está formada por un número de página y un desplazamiento. Desde el punto de vista del sistema, el desplazamiento de la página se ve como un número de segmento dentro de la página y un desplazamiento dentro del segmento. 
2) Las caches privadas son accesibles desde otros procesadores. 
Es un sistema en el cual cada proceso tiene asociada una tabla de páginas y varias tablas de segmento, una por cada segmento del proceso.
Cuando un proceso está ejecutándose, un registro contendrá la dirección de comienzo de la tabla de páginas para ese proceso.
3) En buses se puede implantar el protocolo de espionaje. 
Todas las anteriores son ciertas.
4) Extendiendo los requisitos del controlador de la cache y explotando las propiedades del bus, las lecturas y escrituras que son inherentes al programa se usan de forma implícita para mantener la coherencia de las caches mientras que la serialización del bus mantiene la coherencia.
5) Puede el bus contener de forma eficiente a un número elevado de procesadores. 
Ninguna de las anteriores es correcta.
6) Son todas correctas.
7) Son todas falsas excepto la 1, 4 y 5.
8) Son todas falsas excepto la 2 y la 3.
Longitud del segmento.
Bit de presencia.
Bit de modificación.
Bit de uso.
1) Puede más de una cache actualizar la línea en el directorio completo. 
2) En directorio limitado se usa el algoritmo de reemplazo. 
3) El protocolo de directorio jerárquico envía una petición que no puede ser servida por las caches en un directorio a otro cluster según determine el directorio. 
4) Si el bit de privacidad está activado, entonces indica que la copia del bloque de memoria es válida. 
5) Los tipos de directorio completo y directorio limitado se suele usar en directorio centralizado. 
6) Son todas ciertas, menos la 1 y la 4.
7) Son todas ciertas, menos la 1.
Los bits de presencia y modificación no son necesarios puesto que estos elementos se manejan a nivel de segmento.
La entrada de la tabla de páginas es, básicamente, la misma que se usa en un sistema de paginación pura.
No añaden bits de control puesto que estos están presentes en la tabla de segmentos asociada.
Ninguna de las anteriores es cierta.
El número máximo de páginas por segmento es 4.
1) Se usa la escritura por invalidación para la actualización de la memoria principal. 
La dirección física tiene un campo de desplazamiento de 6 bits.
2) Se usa la post escritura para la actualización de la memoria cache. 
La dirección virtual se compone de 2 bits para el número de página, 2 bits para el número segmento y 8 bits para el desplazamiento de segmento.
3) Se usa la escritura inmediata para la actualización de la memoria principal. 
4) Se usa la escritura por actualización para mantener la coherencia entre caches. 
5) Son todas falsas.
6) La 1 y 3 no son del todo ciertas.
7) La 3 y 4 son totalmente ciertas.
8) La 2 y 3 son falsas.
El tamaño de página es igual a 2^12 = 4096 palabras.
Todas las respuestas anteriores son correctas.
Ninguna de las anteriores es cierta.
Solamente las respuesta 1 y 3 son correctas.
El número de páginas por segmento es de 2^18 y el tamaño de la página de 2^8 bytes.
El número de páginas por segmento es 2^16 pero el tamaño de la página depende del tamaño del segmento.
El número de segmentos totales es de 2^2 y el tamaño máximo del segmento es de 2^24 bytes.
1) El nodo exclusivo es el único nodo que posee una copia del bloque en su cache en estado dirty. 
El número de páginas totales es de 2^18 pero el tamaño de la página depende del tamaño del segmento.
2) El nodo solicitante es el que emita la petición sobre el bloque. 
3) El nodo propietario es un nodo sin copia valida. 
Ninguna de las respuestas anteriores es correcta.
4) El nodo exclusivo es el único nodo que posee una copia del bloque en su cache en estado valido. 
5) La 3 y 1 son totalmente ciertas.
6) La 3 y 1 son totalmente falsas.
7) Son todas falsas.
Las políticas de ubicación, al igual que las de reemplazo son de vital importancia en un sistema de paginación.
En un sistema de paginación la eficiencia del hardware de traducción de direcciones y del hardware de acceso a memoria principal depende de la combinación de marco de página escogida.
Porque no hay sobrecarga sustancial en la comunicación entre los procesadores.
Juegan un papel importante en los multiprocesadores de acceso no uniforme a memoria (NUMA), en los cuales es conveniente una estrategia de ubicación automática que asigne las páginas a los módulos de memoria que proporcionen el mejor rendimiento.
Es improbable que se consiga que todos los procesadores realicen un trabajo útil sobre un mismo problema de forma simultánea. 
En un sistema de paginación combinada con segmentación, la política de ubicación es un aspecto fundamental del diseño.
El aumento es proporcional ya que todos los procesadores trabajan de forma simultánea todo el tiempo sobre una aplicación, y hay computación adicional o contención en los recursos que no afectan al trabajo realizado por los procesadores.
Todas las anteriores son ciertas.
Todas las respuestas son ciertas.
Ninguna es cierta.
Sólo las tres primeras son ciertas.
Los multiprocesadores se encuentra clasificados dentro de la clasificación de Flynn como de tipo MIMD.
Las arquitecturas MISD se pueden interpretar de dos formas, la primera es considerar la clase de máquinas que requerirían que unidades de procesamiento diferentes recibieran instrucciones distintas operando sobre los mismos datos. La otra forma, es como una clase de máquinas donde un mismo flujo de datos fluye a través de numerosas unidades procesadoras.
Políticas de reemplazo.
Los tipos MIMD se encuentran subdivididos en multiprocesadores, multicomputadores, multiprocesadores y máquinas de flujo de datos.
Políticas de vaciado.
Son todas ciertas.
Políticas de lectura.
Ninguna es cierta.
Políticas de ubicación.
Ninguna de las anteriores es cierta.
1) Son dos, espacio de memoria compartida y espacio de memoria distribuida.
Consiste en determinar el número de páginas que pueden ser reemplazadas en un periodo de tiempo A.
2) En los multiprocesadores sólo se pueden transmitir los datos mediante el uso de memoria compartida, ya que el uso de memoria distribuida hace que los datos se transfieran erróneamente debido a la falta de coherencia.
3) En los multiprocesadores sólo se pueden transmitir los datos mediante el uso de memoria distribuida, ya que el uso de memoria compartida hace que los datos se transfieran erróneamente debido a la falta de coherencia.
4) La 2 y la 3 son ciertas.
5) La 1 no es totalmente cierta.
Se incorpora implícitamente en la "estrategia del conjunto de trabajo".
Controla las situaciones en las que hay pocos procesos residentes en memoria, con lo cual habrá muchas ocasiones en las que todos los procesos estén bloqueados y se consumirá mucho tiempo en el intercambio.
Todas las anteriores son ciertas.
La respuesta 1 es falsa.
Sí
No
Sí
No
1) Son sistemas que poseen varios procesadores capaces de ejecutar procesos autónomamente.
El número de programas máximo que realizan operaciones de E/S.
2) No hay una unidad de control central.
3) La lógica de control está distribuida entre los distintos procesadores.
El número máximo de procesos en memoria principal.
4) Los procesadores comparten un espacio de memoria común.
5) El hardware es gestionado en su totalidad por el sistema operativo.
6) Granularidad del paralelismo.
7) Mecanismos de sincronización para el acceso a memoria.
8) Latencia de memoria.
El número máximo de programas que comparten variables.
9) Red de interconexión.
El número máximo de archivos en un directorio.
10) Gestión de los recursos del procesador.
Ninguna de las anteriores es correcta.
11) Cada procesador ejecuta su propio flujo de instrucciones.
12) Para aprovechar plenamente una arquitectura de n procesadores, es necesario tener al menos n hebras o procesos ejecutándose en paralelo.
13) Son todas características, pero de los multicomputadores.
14) Son todas falsas, menos la 1, 3, 5, 6, 7, 8, 9 y 11.
15) Son todas ciertas.
1) La clasificación según el rendimiento de los multiprocesadores es UMA, NUMA y COMA.
2) UMA: Memoria de acceso uniforme. La memoria física está uniformemente compartida por todos los procesadores, es decir, todos los procesadores tienen el mismo tiempo de respuesta a todas las palabras de memoria. Cada procesador puede tener su caché privada. La red de interconexión toma la forma de bus común, conmutador cruzado o red multietapa.
3) NUMA: Memoria de acceso no uniforme. Es un sistema de memoria compartida donde el tiempo de acceso no varía según el lugar donde se encuentre localizado el acceso. La memoria se divide en tantos bloques como procesadores haya, y cada bloque se une a un procesador mediante un bus.
4) COMA: Memoria de acceso sólo en caché. Cada bloque de memoria trabaja como un bloque de memoria principal.
5) La ventaja de los NUMA sobre los UMA es que el acceso a la memoria local es más rápido.
6) Son todas ciertas.
7) La 2, 3 y 4 son ciertas, pero no la 1.
8) Son todas falsas.
1) Los multiprocesadores pueden estar conectados con un único bus y conectados por una red.
2) En los multiprocesadores conectados por un solo bus, el tráfico por procesador y el ancho de banda del bus determinan el número de procesadores útiles.
Suspensión del proceso que lleva más tiempo activo puesto que introduce una sobrecarga.
3) En los multiprocesadores conectados por un solo bus, las caches replican los datos en sus memorias tanto para reducir la latencia de acceso a los datos como para reducir el tráfico de datos con la memoria en el bus. 
Suspensión del proceso con el mayor conjunto residente puesto que introduce una sobrecarga.
4) Si se quieren conectar múltiples procesadores se necesita más de un bus.
5) En los sistemas que usan como medio de comunicación una red, la memoria está conectada a cada procesador, mediante dicha red.
Suspensión de los procesos que se encuentran en un anillo más interno dentro del esquema de anillos.
6) La 1, 2 y 3 son ciertas.
Suspensión del proceso que se encuentra en la cabeza del buffer de procesos en ejecución gestionado por el sistema.
7) La 4 y 5 no son totalmente correctas.
8) La 4 y 5 son ciertas.
Suspensión de los procesos con la mayor ventana de ejecución restante.
9) Todas tienen algún fallo.
Suspensión del proceso mayor.
10) Son todas ciertas.
Todas las respuestas anteriores son ciertas.
Ninguna de las respuestas de la 1 a la 6 es cierta.
Sólo las respuestas 5 y 6 son ciertas.
Sólo las respuestas 3, 5 y 6 son ciertas.
Porque las tres características deseables de un bus son incompatibles: gran ancho de banda, latencia pequeña y gran longitud. Existe también un límite en el ancho de banda de los módulos de memoria conectados al bus, por lo que un único bus impone limitaciones en la práctica en el número de procesadores que se pueden conectar.
Porque las tres características deseables de un bus son incompatibles: gran ancho de banda, aceleración y gran longitud. Existe también un límite en el ancho de banda de los módulos de memoria conectados al bus, por lo que un único bus impone limitaciones en la práctica en el número de procesadores que se pueden conectar.
4) En multiprocesadores asíncronos, las redes MINs, el control centralizado y el encaminamiento basado en la permutación es inflexible. 
Porque las tres características deseables de un bus son incompatibles: gran ancho de banda, tiempo de respuesta y gran longitud. Existe también un límite en el ancho de banda de los módulos de memoria conectados al bus, por lo que un único bus impone limitaciones en la práctica en el número de procesadores que se pueden conectar.
Redes de medio común, redes dinámicas, redes indirectas, y redes híbridas.
Redes de medio común, redes buses, redes indirectas, y redes híbridas.
Redes de medio común, redes basadas en routers, redes indirectas, y redes híbridas.
Las redes de área local y los buses usados en la comunicación interna de los uniprocesadores y multiprocesadores.
Las redes de medio compartido se pueden dividir en dos grupos. En broadcast y unicast.
En barreras de sincronización y las basadas en el protocolo Snoopy.
En redes multietapa y barras cruzadas.
Las redes directas son una arquitectura útil para construir computadores paralelos de gran escala, ya que al aumentar el número de nodos en el sistema, el ancho de banda total de las comunicaciones, memoria y capacidad de procesamiento del sistema también aumenta. 
Las redes directas son una arquitectura útil para construir memorias paralelas de gran escala, ya que al aumentar el número de nodos en el sistema, el ancho de banda total de las comunicaciones, memoria y capacidad de procesamiento del sistema también aumenta. 
En lugar de proporcionar una conexión directa entre algunos nodos, la comunicación entre cualquier pareja de nodos se realiza a través de conmutadores. Cada nodo tiene un adaptador de red que se conecta a un conmutador. Cada conmutador consta de un conjunto de puertos. Cada puerto consta de un enlace de entrada y otro de salida. Un conjunto (posiblemente vacío) de puertos en cada conmutador están conectados a los procesadores o permanecen abiertos, mientras que el resto de puertos están conectados a puertos de otros conmutadores para proporcionar conectividad entre los procesadores. La interconexión de estos conmutadores define la topología de la red.
En lugar de proporcionar una conexión directa entre algunos nodos, la comunicación entre cualquier pareja de nodos se realiza a través de routers. Cada nodo tiene un adaptador de red que se conecta a un encaminador. Cada encaminador consta de un conjunto de puertos. Cada puerto consta de un enlace de entrada y otro de salida. Un conjunto (posiblemente vacío) de puertos en cada router están conectados a los procesadores o permanecen abiertos, mientras que el resto de puertos están conectados a puertos de otros routers para proporcionar conectividad entre los procesadores. La interconexión de estos routers define la topología de la red.
Un bus de sistema está formado por un conjunto de conductores conectados al bus. En el bus sólo puede haber una transacción a un tiempo entre una fuente (maestro) y uno o varios destinos (esclavos).
Un bus de sistema está formado por un conjunto de conductores para la transacción de datos entre procesadores, módulos de memoria, y dispositivos periféricos conectados al bus. En el bus sólo puede haber una transacción a un tiempo entre una fuente (maestro) y uno o varios destinos (esclavos). 
Un bus de sistema está formado por un conjunto de conductores para la transacción de datos entre procesadores, módulos de memoria, y dispositivos periféricos conectados al bus. En el bus puede haber transacciones a un tiempo entre las fuentes (maestros) y uno o varios destinos (esclavos).
Cierto
Falso
Sólo en algunos casos
En la mayoría de los casos
1) El multiprocesador basado en bus es uno de los sistemas multiprocesadores más utilizados en computadores de prestaciones altas.
2) El principal problema que tienen es su baja escalabilidad, lo que no permite tener sistemas con muchos procesadores de forma eficiente. 
3) Dependiendo del ancho de banda del bus y de los requisitos de los procesadores que incorpora, un bus puede albergar entre 1 y 3 procesadores de forma eficiente. 
4) Por encima de 4 procesadores, que dependen del procesador y el bus, el canal de conexión, en este caso el bus, se convierte en el cuello de botella del sistema.
5) Se suele utilizar el sistema basado en bus por su bajo coste y facilidad de diseño. 
6) Todas son falsas
7) Todas son ciertas
8) La 3 y la 4 son ciertas
9) La 2 y la 5 son ciertas
10) La 1 y la 2 son ciertas
1) Las redes de barras cruzadas permiten que cualquier procesador del sistema se conecte con cualquier otro procesador o unidad de memoria.
2) Muchos procesadores pueden comunicarse simultáneamente sin contención. 
3) Es posible establecer una nueva conexión en cualquier momento siempre que los puertos de entrada y salida solicitados estén libres. 
4) Un crossbar se puede definir como una redes conmutada con N entradas y M salidas, que permite hasta min{N;M} interconexiones punto a punto sin contención.
5) Todas son ciertas
6) Todas son falsas
Las redes de interconexión multietapa (MINs) conectan dispositivos de entrada a dispositivos de salida a través de un conjunto de etapas de conmutadores 
Cada conmutador es una red de barra cruzada. 
El número de etapas y los patrones de conexión entre etapas determinan la capacidad de encaminamiento de las redes.
Ninguna es cierta
Todas son ciertas
1) Las redes MINs no permiten acceder en paralelo a arrays almacenados en bancos de memoria. 
2) Si los conmutadores tienen el mismo número de puertos de entrada y salida, las redes MINs tienen el mismo número de puertos de entrada y salida. 
3) Las redes MINs no pueden configurarse con un número de entradas mayor que el número de salidas (concentradores) y viceversa (expansores). 
5) Las redes reconfigurables se comportan como redes bloqueantes cuando los accesos son asíncronos 
6) Todas son falsas
7) La 2, 3 y 5 son ciertas
8) Todas son ciertas
9) La 1, 3 y 4 son ciertas
1) Las redes Omega, Baraje y Barras cruzadas son MIN Bloqueantes 
2) La red de Intercambio está basada en la red Omega 
3) La red Baseline y Intercambio son prácticamente la misma red 
4) En la red Mariposa son necesarias "log2 a" etapas para realizar la interconexión 
5) La red Omega requiere en total n(log2 n)/2  conmutadores 
6) Todas son falsas
7) Todas son ciertas
8) La 2 y 5 son ciertas
9) La 1, 3 y 4 son ciertas
10) La 3 y 5 son ciertas
1) Las redes MIN Bidireccionales usan un conmutador bidireccional en donde cada puerto está asociado a un par de canales unidireccionales en direcciones opuestas.\n
2) La información entre conmutadores vecinos no puede transmitirse en direcciones opuestas.\n
3) El conmutador bidireccional soporta tres tipos de conexiones: hacia delante, hacia atrás y de vuelta.\n
4) Todas son ciertas.\n
5) Todas son falsas.\n
6) La 1 y la 3 son ciertas.\n
7) La 2 y 3 son ciertas.\n
En las redes MIN Bidireccionales, los caminos se pueden establecer cruzando etapas hacia delante, después estableciendo una conexión de vuelta, y finalmente cruzando los estados en dirección hacia atrás.\n
En las redes MIN Unidireccionales, los caminos se pueden establecer cruzando etapas hacia delante, después estableciendo una conexión de vuelta, y finalmente cruzando los estados en dirección hacia atrás.\n
En las redes MIN Bidireccionales, los caminos se pueden establecer cruzando etapas hacia atrás, después estableciendo una conexión de vuelta, y finalmente cruzando los estados hacia delante.\n
Sí. Una red no bloqueante permite implementar cualquier permutación entre entradas y salidas de la red.\n\n
Sí. Cualquier red regular, permite implementar cualquier permutación entre entradas y salidas de la red.\n\n
No. Ya que son completamente distintas.\n
Todas son falsas\n\n
Todas son ciertas
Las redes no bloqueantes presentan igual funcionalidad que una red de barras cruzadas, pero la latencia puede ser menor, debido a que las transferencias deben atravesar varios conmutadores.\n
Las redes no bloqueantes presentan igual funcionalidad que una red de barras cruzadas, pero la latencia puede ser mayor, debido a que las transferencias deben atravesar varios conmutadores.\n
Las redes no bloqueantes presentan igual funcionalidad que una red de barras cruzadas, pero las transferencias deben atravesar varios conmutadores, con igual latencia.\n
La memoria del VAX-11 y el TLB del VAX 11/780: ejemplo de memoria virtual paginada con posibilidad de habilitar segmentación.
Intel 80286/80386:  arquitectura con segmentación paginada.
Pentium de Intel: arquitectura con memoria virtual paginada.
Todas las respuestas anteriores son falsas.
Esta combinación proporciona protección a la vez que minimiza el tamaño de la tabla de páginas.
Cada espacio de direcciones de un proceso se divide en segmentos y se ubican en cualquier parte de la memoria.
El espacio de direcciones se divide primero en dos segmentos, uno para sistema y otro para procesos, a su vez el área de proceso está dividido en dos partes p0 y p1.
La traducción de direcciones se realiza utilizando los bits más significativos para calcular el segmento y el resto, para calcular el desplazamiento sobre dicho segmento.
Debido al gran tamaño de las tablas de páginas que se originan en este sistema, estas se almacenan exclusivamente en memoria virtual.
Existe una tabla exclusiva para la compartición que se encuentra en memoria física y que gestionan los usuarios, en esta se almacena la información común a varios procesos.
Todas las respuestas anteriores son falsas.
Las respuestas 1 y 3 son correctas y el resto falsas.
Las respuestas 2 y 4 son correctas y el resto falsas.
El objetivo de dividir el TLB es disminuir la frecuencia fallos global, favoreciendo el principio de localidad espacial.
El TLB esta dividido con el objetivo de evitar que las direcciones de los procesos ocupen más del 50% del TLB.
Dividir el TLB provoca que la frecuencia de fallos global aumente.
Dividir el TLB provoca que la frecuencia de fallos máxima del TLB disminuya en entornos con intensivos cambios de proceso.
Todas las respuestas anteriores son falsas.
Todas las respuestas son correctas excepto la 1.
Las máquinas 8086 implementaban segmentación, pero no tenían memoria virtual ni protección de ningún tipo, cada segmento tenía registro base pero no límite.
Las máquinas 80286 no evolucionan gran cosa respecto a las máquinas 8086 ya que no incluyen ningún tipo de protección, ni compartición.
Las máquinas 80286 evolucionan incorporando una distribución parecida de la memoria a las de las VAX, incorporando protección y compartición de datos.
Las respuestas 1 y 3 son correctas.
Todas las respuestas son falsas.
Soporta segmentación o segmentación /paginación combinadas en el caso de que se habilite la paginación en el registro correspondiente.
Es una implementación de memoria virtual segmentada simple.
Utiliza dos tablas, la LDT(tabla de descriptor local) y la GDT(tabla de descriptores global).
Las respuestas 1 y 3 son correctas.
Todas las respuestas son correctas.
Cuando el espacio de direcciones que utilizan los programas varía dinámicamente.
Cuando se utiliza la memoria secundaria para almacenar una copia del programa ejecutado.
Cuando el espacio de direcciones que utilizan los procesos es distinto que el espacio de direcciones físicas disponible en memoria principal.
Cuando se utiliza un sistema especial de almacenamiento para reducir los accesos a memoria secundaria.
Todas las respuestas son falsas.
Únicamente son ciertas las respuestas 3 y 4.
Reubicación.
Compartición y  protección.
Reubicación y organización lógica.
Todas las respuestas están incompletas.
Se asigna memoria a cada proceso de manera transparente al programador con el objetivo de maximizar el uso de la CPU, así los procesos comparten la memoria disponible.
Cada proceso tiene su propia tabla de segmentos, cuando todos los segmentos se encuentran en memoria principal, la tabla de segmentos del proceso se crea y se carga en memoria.
Se asigna un conjunto de bloques de memoria contiguos de un espacio de direcciones unidimensional.
Todas las respuestas son correctas.
Las respuestas 1 y 2 son correctas.
Surgió como solución a la limitación de la capacidad de la memoria principal para alojar múltiples procesos.
Surgió como respuesta a la necesidad de dar soporte a la multiprogramación, de esta forma, tenemos al menos dos espacios de direcciones contiguos de memoria para asignar a múltiples procesos.
Tiene como objetivo ofrecer un espacio de direcciones más allá del que puede ofrecer la memoria principal, esto unido a diversas técnicas de gestión de memoria aumenta el rendimiento de la máquina.
Es una estrategia soportada por el hardware del sistema, que se sitúa entre los niveles de caché y memoria principal de la jerarquía.
Todas las repuestas anteriores son ciertas.
Las respuestas 1 y 4 son ciertas.
Las respuestas 2, 4, 5 y 6 son falsas.
Espacio de direcciones virtuales.
Espacio virtual direccionable.
Conjunto de trabajo.
Conjunto virtual de trabajo.
Conjunto direccionable.
Ninguna de las anteriores es cierta.
Tanto 1 como 2 son ciertas.
Es un fenómeno que ocurre entre el nivel de memoria caché y memoria principal cuando ocurre un fallo en la primera, y que provoca que el bloque no encontrado se traiga desde memoria principal. 
Es el proceso de reubicación de bloques de memoria que realiza el sistema operativo al asignar un espacio de direcciones al proceso. 
Es la traducción de direcciones virtuales que produce la CPU, por una combinación de hardware y software a direcciones físicas, que pueden ser utilizadas para acceder a memoria principal.
Tiene como consecuencia que el sistema funcione con diversos fragmentos de muchos programas en memoria a la vez. Mientras un programa está esperando por una E/S y no puede ejecutarse se le asigna la CPU a otro proceso.
Son los bloques en los que se divide cada página virtual.
Es el tamaño de la memoria principal.
Son los bloques en los que se divide la memoria principal y que son del mismo tamaño que las páginas.
Ninguna de las anteriores es cierta.
Ninguna de las anteriores es cierta.
Puede considerarse como un par ordenado (p,d); en el que p es el número de la página y d es la dirección física asignada.
Puede considerarse como un par ordenado (p,d); en el que p es el número de la página y d es el desplazamiento u offset.
Puede considerarse como un par ordenado (p,d), en el que p es la posición dentro de la memoria física y d es el desplazamiento u offset.
Es una tabla en la que es posible implementar un mecanismo para conocer el estado de las páginas físicas de la memoria principal.
Es una tabla en la que es posible implementar un mecanismo de traducción de direcciones virtuales a direcciones físicas.
Es una tabla en la que es posible implementar un mecanismo que se utiliza para traducir la dirección virtual a la dirección de memoria secundaria.
Es una estructura de datos que está indexada por el número de página y que contiene la dirección física del bloque.
Ninguna de las anteriores es cierta.
Todas las respuestas son correctas excepto la 3. 
No es posible, ya que se perderían el requisito de protección que satisface esta estrategia de gestión de memoria.
Sí,  el sistema de gestión de memoria debe permitir accesos controlados a las áreas compartidas de la memoria, sin comprometer la protección básica.
No es posible, cada proceso tiene que garantizar todos los recursos necesarios para su ejecución, teniendo una copia local en su espacio de direcciones.
Sí, es muy útil a la hora de compartir recursos comunes, tales como librerías, datos, etc. que optimiza la gestión de la memoria.
Las respuestas 2 y 4 son ciertas.
Particiones dinámicas.
Segmentado.
Paginado.
Segmentado/paginado.
Ninguna de las anteriores es correcta.
La paginación fija divide la memoria en un conjunto de particiones fijas durante el inicio del sistema, el proceso debe cargarse en una partición de igual o mayor tamaño.
La paginación dinámica divide la memoria en un conjunto de particiones fijas al arrancar el sistema, estas particiones cambian de tamaño para adecuarse al tamaño de los procesos a cargar en memoria.
Segmentación simple.
Segmentación.
La paginación simple divide la memoria principal en un conjunto de marcos de igual tamaño. Cada proceso se divide en una serie de páginas de del mismo tamaño que los marcos. Un proceso se carga situando todas sus páginas en marcos libres, pero no necesariamente contiguos.
La segmentación simple divide a cada proceso en una serie de segmentos. Un proceso se carga situando todos sus segmentos en particiones dinámicas que no tienen porque ser contiguas.
La memoria virtual paginada divide la memoria principal en marcos de igual tamaño y los procesos en páginas de igual tamaño que los marcos, un proceso se carga situando todas sus páginas en marcos libres pero no necesariamente contiguos.
La memoria virtual segmentada divide cada proceso en una serie de segmentos. Para que un proceso se cargue se colocan los segmentos necesarios para la ejecución en las particiones dinámicas, que no tienen porque ser contiguas.
Las respuestas 1, 3, 4 y 6 son ciertas.
Todas las respuestas son ciertas.
Tamaño de la página de 2^6 = 64 bytes.  Página = dirección_lógica / 64. Serán necesario 4 fallos de página para llenar la memoria.
El número de páginas totales es de 2^22 y el tamaño de la página es de 10 bytes.
Tamaño de página de 2^10  = 1024 bytes. Pagina = direccion_lógica / 64.Serán necesarios 4 fallos de página para llenar la memoria.
Tamaño de la página de 2^10 = 1024 bytes. Pagina = dirección_lógica / 1024. Serán necesario 1 fallo de página para llenar la memoria. 
Tamaño de página de 2^10 = 1024 bytes. Pagina = dirección_lógica / 1024. Serán necesarios 4 fallos de página para llenar la memoria.
Todas las respuestas son falsas.
Si utiliza una tabla de páginas de un único nivel en la que cada entrada ocupa 4bytes, y el espacio de direcciones del proceso es de 8MB, la tabla de páginas ocupará 32 KB.
Sabiendo que el código del proceso ocupa 2MB, y que se utiliza una tabla de páginas de 2 niveles y cada entrada de sus tablas ocupan 2 bytes, para acceder a todo el código será necesario usar la tabla de páginas de primer nivel y 4 tablas de páginas de segundo nivel.
En el esquema con tablas de páginas de dos niveles y cada entrada de sus tablas ocupan 2 bytes, cada tabla de páginas de segundo nivel permite encontrar como máximo 1024 páginas del proceso.
Las dos primeras son ciertas.
Ninguna de las anteriores es cierta.
En un esquema de memoria virtual, el número de procesos cargados en memoria no depende del tamaño de los mismos.
En un esquema de memoria virtual con paginación se puede desperdiciar memoria en la última página de cada proceso.
La traducción de direcciones en un esquema de paginación la hace el hardware, que traduce el número de página virtual a número de marco de la memoria principal.
Todas las respuestas son correctas.
En los sistemas operativos actuales, la traducción de direcciones de la memoria es una tarea del módulo de gestión de memoria del sistema operativo.
El mecanismo de protección de memoria consiste en evitar que un proceso pueda acceder al espacio de direcciones de otro proceso.
Para proteger el acceso entre procesos, el mecanismo de control de acceso comprueba en el momento de la ejecución que las direcciones lógicas a las que hacen referencia estén cargadas en memoria.
Todas las repuestas son ciertas.
La tabla de páginas sirve para calcular el número del marco donde está cargada una página de un proceso. Esto supone la traducción de una dirección lógica a una dirección física.
El uso de la TLB en el proceso de traducción evita el acceso a la tabla de páginas en memoria en una buena parte de las referencias a direcciones.
La TLB forma parte de la unidad de gestión de memoria.
Todas las respuestas son ciertas.
Ninguna de las respuestas es correcta.
Las respuestas 2, 3 y 4 son ciertas.
Se necesitan para el segmento de código 32768/4096 = 8 páginas, se podrá cargar el programa en memoria porque solo se necesita el código para su ejecución.
Se necesitan para el segmento de código 32768 / 4096 = 8 páginas, segmento de datos 16386/4096 = 5 páginas, segmento de pila 15870/4096 = 4 páginas. Total = 17 páginas necesarias con 65536/4096 = 16 marcos de página disponibles. Por tanto no es posible cargar el programa en memoria.
Se necesitan para el segmento de código 32768 / 16384 =  2 páginas, segmento de datos 16386/ 16384 =  1 páginas, segmento de pila 15870/ 16384 =  1 páginas. Total = 4 páginas necesarias con 65536/4096 = 16 marcos de página disponibles. Por tanto es posible cargar el programa en memoria.
Todas las respuestas son falsas.
El número de páginas totales es de 2^22 y el tamaño de la página de 2^10.
El número de páginas totales es de 2^32 y el tamaño de la página depende del marco de página.
El número de paginas totales es de 2^22 pero el tamaño de la página depende del marco de página.
Todas las respuestas anteriores son falsas.
El espacio de direcciones es el espacio que es capaz de direccionar la máquina con el número de bits de la palabra de memoria. Las posiciones de memoria son el conjunto de posiciones de memoria real de la que dispone la máquina.
El espacio de direcciones y las posiciones de memoria hacen referencia al mismo concepto accediéndose a estos a través del proceso de correspondencia.
El espacio de direcciones es el espacio físico que puede direccionar una máquina y está compuesto por posiciones de memoria.
Todas las respuestas anteriores son falsas.
Partición dinámica.
Paginación.
Partición fija
Paginación simple.
Las estrategias 2, 3 y 4 son correctas.
Todas las opciones son correctas.
La compartición eficiente de la memoria entre múltiples programas, la protección básica de su espacio de direcciones y una asignación transparente de la carga del programa completo en memoria.
Compartición eficiente de la memoria entre mútiples procesos garantizando protección y eliminación de inconvenientes de la limitación del tamaño de los programas.
Aumentar el rendimiento de la máquina cargando varios procesos simultáneamente en memoria.
Reduce notablemente los esfuerzos del programador, puesto que no tiene que preocuparse del diseño de overlays(porciones de código)  ya que el propio sistema automatiza este proceso.
Todas las respuestas son falsas.
Las respuestas 2 y 4 son ciertas.
Es el proceso llevado a cabo cuando hay un fallo de página y es necesario cargar el programa desde la memoria secundaria.
Proceso por el cual la traducción de direcciones permite cargar el programa en cualquier parte de la memoria principal.
Es el proceso que se lleva a cabo cuando se elimina un programa de la memoria principal porque lleva mucho tiempo sin ejecutarse.
Todas las respuestas son falsas.
Son los bloques de memoria en los que se divide la memoria principal, corresponden con el tamaño de las páginas en las que se divide cada programa a cargar en memoria.
Denotado por W(t, A), es el conjunto de páginas a las que el proceso ha hecho referencia en las últimas A unidades de tiempo virtual.
Son los bloques en el que el sistema de gestión de memoria divide el programa y que tienen el mismo tamaño que las páginas de memoria.
También llamado conjunto residente,  es el conjunto de páginas asignadas a cada proceso activo.
Todas las respuestas son falsas.
Se produce una fragmentación externa de 50Kb, que será eliminada posteriormente durante el proceso de compactación.
Se modifica el tamaño de la partición para que el tamaño de la partición sea de 450Kb.
Se crea una nueva partición libre de 50Kb con la memoria disponible sobrante.
Se crea una nueva partición de 450Kb y otra de 550Kb al unirse los 50Kb que restan de la primera asignación.
Se produce una fragmentación interna de 50Kb.
Todas las respuestas son falsas.
Las respuestas 2 y 3 son ciertas.
3 + 326.
1x1024 + 326
3x1024 + 326
No tenemos información suficiente para calcular la dirección física.
Todas las respuestas anteriores son falsas.
8*5 bits.
32*5 bits.
8*3 bits.
32*3 bits.
Consiste en que al disminuir el número de marcos de página para asignación, el número de fallos de página aumenta.
Consiste en que al disminuir el tamaño de las páginas, el número de fallos de página aumenta.
Consiste en que al aumentar el número de marcos de página para asignación, el número de fallos de página aumenta.
Consiste en que al aumentar el grado de multiprogramación se alcanza un punto en el que el conjunto residente en promedio no es adecuado. En este punto, el número de fallos de página se eleva drásticamente y la utilización del procesador se desploma.  
Ninguna de las respuestas anteriores es correcta.
Por la anomalía de Belady, se produce una disminución del número de fallos de página.
Por la anomalía de Belady, se produce un aumento del número de fallos de página.
La anomalía de Belady se da con los algoritmos de reemplazo LRU, por lo que el número de fallos de página se mantiene.
No afecta al número de fallos de página.
Todas las anteriores son falsas.
El proceso que está en estado de "preparado".
El proceso que está bloqueado, esperando una operación de E/S.
El proceso que está en "ejecución" (activo).
La puede producir cualquiera de los anteriores.
No la puede producir ninguno.
4 bits para direccionar las páginas + 10 bits para el desplazamiento en bytes.
5 bits para direccionar las páginas + 11 bits para el desplazamiento en bytes.
5 bits para direccionar las páginas + 10 / 11 bits de desplazamiento en palabras/bytes respectivamente.
4 bits para direccionar las páginas + 11 bits de desplazamiento en bytes.
Todas las respuestas anteriores son falsas.
4 bits para direccionar los marcos de página + 10 bits para el desplazamiento en bytes.
5 bits para direccionar los marcos de página + 11 bits para el desplazamiento en bytes.
5 bits para direccionar los marcos de página + 10 bits de desplazamiento en palabras.
4 bits para direccionar los marcos de página + 11 bits de desplazamiento en bytes.
Todas las respuestas anteriores son falsas.
Las respuestas 2 y 3 son correctas.
1. La escritura con invalidación es más rápida, ya que en la escritura por actualización se tiene que  por cada escritura se debe capturar el bus y entonces proceder a la actualización. Por otro lado, tenemos que la escritura con invalidación produce un mayor número de contenciones en el bus.
En cada palabra de la memoria principal tendremos el dato a leer, y un bit de modificación(si la página ha sido modificada) y algunas más para niveles de protección. En la tabla tendremos un bit adicional que indica si la página está o no cargada en memoria. Esto disminuye el tamaño de la tabla de páginas ya que se carga en memoria principal.
En la tabla de páginas, a parte de la entrada del marco de página, tendremos un bit adicional de presencia (si está o no la página en memoria), otro de modificación (si la página ha sido modificada) y algunas más para niveles de protección.
Se pueden utilizar los métodos anteriormente descritos a elección del diseñador de la máquina y del sistema operativo.
Todas las respuestas anteriores son falsas.
La traducción de direcciones se realiza por hardware, y la gestión de la memoria la realiza el módulo de gestión de memoria del sistema operativo.
Cuando la CPU produce una dirección virtual, el sistema operativo realiza la traducción de la dirección, y comprueba el valor del bit de presencia de la tabla de páginas,si está a 0 es el mismo sistema operativo se encarga de resolver el fallo de página.
Para acceder a la memoria principal es necesario realizar la traducción de la dirección virtual proporcionada, dependiendo de la implementación de la máquina este proceso lo realiza indistintamente el hardware o el software.
Todas las respuestas anteriores son falsas.
1. Si entradas > salidas -> Conmutadores de concentración
2. Si entradas < salidas -> Conmutadores de distribución
3. Si entradas < salidas -> Conmutadores de concentración
4. Si entradas > salidas -> Conmutadores de distribución
5. Todas son falsas
6. Todas son ciertas
2. Las tramas Autonet son muy parecidas a las tramas Ethernet.
8. La 3 y la 4 son ciertas
7. La 1 y la 2 son ciertas
En las redes reconfigurables, cualquier fuente se puede conectar a un destino libre determinado, aunque haya peticiones en curso, pero para mantener estas peticiones en curso se pueden necesitar cambiar el camino utilizado por algunas de ellas (reconfigurar la red). 
En las redes reconfigurables, cualquier fuente se podría conectar a cualquier destino libre siempre y cuándo no hayan peticiones en curso (reconfigurar la red). 
En las redes reconfigurables, cualquier fuente se podría conectar a cualquier destino libre aunque haya peticiones en curso, pero, para mantener estas peticiones en curso se pueden necesitar cambiar el camino utilizado por algunas de ellas (reconfigurar la red). 
Sí.
Sí. Al subgrupo de unidireccionales.
No. Pertenecen a Medio Compartido
No. Pertenecen a Barras cruzadas.
Sí. Ambas pertenecen a redes MIN No Bloqueantes
Sí. Ambas pertenecen a redes MIN Bloqueantes Unidireccionales
No.
2. Sólo aumentó la frecuencia de reloj. 
1. Mejora el encaminamiento  y múltiples unidades segmentadas de carga/almacenamiento.
3. Ninguna de las anteriores es correcta.
4. La respuesta 1 y la 2 son ciertas.
La velocidad de reloj aumentó 5MHz.
El rendimiento aumentó el doble.
El rendimiento aumentó un 80%.
Se introdujo la red Omega.
Mayor tamaño de la caché de datos, mayor ancho de banda, aumento de la velocidad de reloj y mejora del conjunto de instrucciones.
Aumento de la velocidad de reloj y mejora del conjunto de instrucciones.
Mayor tamaño de la caché de datos, aumento de la velocidad de reloj y mejora del conjunto de instrucciones.
El software de gestión y paralelo y el ancho de banda interno.
Escalabilidad, software de gestión y paralelo, ancho de banda interno y aumento de la caché.
Escalabilidad, software de gestión, software paralelo y ancho de banda interno.
En la IBM SP1 no se encontraron fallos.
3. Las redes Autonet son las sucesoras de las redes autoconfigurables.
1. Autonet usa encaminamiento arriba/abajo.
4. La 1 y la 2 son ciertas.
Vemos que la información se escribe en la caché y en la memoria principal.
Este problema se puede resolver haciendo uso de hardware especifico. Igualmente, existen otras alternativas para componentes que no tengan un hardware especifico, tales posibilidades son tratar a esos componentes como no cacheables, o hacer uso de instrucciones que desalojen líneas de la cache y las trasladen a memoria si ésta se encuentra actualizada. 
Este problema se puede solventar haciendo uso de los protocolos de coherencia de cache, que hacen que cada escritura sea visible por todos los procesadores del sistema.
1. Una se produce cuando un procesador cada vez que se modifique una dirección en la copia que posee en su cache, se procede a la modificación de dicha dirección en todas las copias del bloque que se encuentre en las caches del resto de procesadores. 
2. Una se produce cuando un procesador emite una señal que hace que se modifique el dato que éste ha cambiado en todas las caches que se encuentren dentro del bloque de forma automática, de forma que todas las copias permanecen actualizadas en todo momento. 
3. Una se produce cuando un procesador emite una señal detectada por el resto de procesadores, esta señal hace que las copias del resto de procesadores sean invalidadas de forma que la única copia que sobrevive es la del que solicito la escritura.
4. La 1 y 2 son ciertas.
5. La 2 es cierta.
6. Todas son ciertas.
2. El mecanismo de escritura se basa en asegurar que un procesador tiene acceso exclusivo a un dato antes de que acceda a él. 
3. El acceso exclusivo, de la invalidación de escritura, asegura que no hay otras copias que se puedan leer o escribir del dato cuando se realiza la escritura, ya que todas las copias del dato se invalidan en el momento de la escritura. 
4. Los sistemas de espionaje no son escalables debido a los retardos que supone tener que esperar a que todas las caches vean las peticiones en el bus, y a que debe atender peticiones de todos los componentes del sistema. 
5. Cuando la red de interconexión no es el bus, no se puede usar el protocolo de espionaje (snoopy), pues no existe un bus que monitorizar, entonces lo que se usa el protocolo por directorio.
7. Todas son falsas.
8. La 2, 4 y 5 son falsas.
9. La 2 y 4 son falsas.
En el MSI si un procesador escribe en un bloque en su cache, éste debe realizar una transferencia que invalide las copias en el resto de caches, aunque no existan tales copias, esto hace que el protocolo de invalidación de tres estados no sea lo eficiente que se desee. Esta situación es solventada por el protocolo MESI, añadiendo para ello un nuevo estado, el estado exclusivo. En este estado, si se escribe en el bloque no es necesario generar paquetes para solicitar el uso de la línea, ni tampoco invalidar posibles copias, pues se conoce que ninguna otra cache tiene copia del bloque.
En el MSI si un procesador escribe en un bloque en su cache, éste debe realizar una transferencia que invalide las copias en el resto de caches, aunque no existan tales copias, esto hace que el protocolo de invalidación de tres estados no sea lo eficiente que se desee. Esta situación es solventada por el protocolo MESI, añadiendo para ello un nuevo estado, el estado inválido. En este estado, si se escribe en el bloque no es necesario generar paquetes para solicitar el uso de la línea, ni tampoco invalidar posibles copias, pues se conoce que ninguna otra cache tiene copia del bloque.
Para el ejemplo Ejem1.cfg, el número total de ciclos que transcurren al realizar tal operación es la suma de los tiempos necesarios para:  (1) consultar la tabla de páginas,  (2) leer desde memoria secundaria y (3) escribir en memoria principal.
Para el ejemplo Ejem2.cfg, el número total de ciclos que transcurren al realizar tal operación es la suma de los tiempos necesarios para:  (1) consultar el TLB, (2) consultar la tabla de páginas,  (3) leer desde memoria secundaria y (4) escribir en memoria principal.
Para ambos ejemplos, el tiempo necesario para escribir la página en memoria principal corresponde con el tiempo de acceso (10.000 ciclos).
En ambos ejemplos, es necesario computar el tiempo de acceso a la tabla de páginas para actualizar todos los campos, una vez que se ha escrito la página en memoria principal.
Para el ejemplo Ejem1.cfg transcurren 13.500 ciclos al realizar la operación mientras que para el ejemplo Ejem2.cfg transcurren 13.600 ciclos.
Todas las anteriores son ciertas.
Paginación por demanda: las 4 primeras líneas provocan fallo de página y se carga una página por línea.
Paginación anticipada: las 4 primeras líneas provocan fallo de página y se carga una página por línea.
En el caso de que sea necesario traer una página desde memoria secundaria hasta memoria principal se consumirán 13.600 ciclos en completar la operación.
Paginación anticipada: las 4 primeras líneas provocan fallo de página y se carga 2 páginas contiguas por línea.
La configuración con paginación anticipada provoca más trasiego en la memoria.
En el caso de que la página buscada se encuentre en el TLB, se consumirán 100 ciclos correspondientes a la consulta del TLB.
En el caso de que la página buscada no se encuentre en el TLB, se acudirá directamente a memoria secundaria para transferir la página a memoria principal.
El TLB de instrucciones no se utiliza durante la ejecución del programa a pesar de que este TLB podría almacenar páginas correspondientes a cualquier tipo de instrucción. 
Todas las anteriores son ciertas.
Las respuestas 1,2,3,y 4 son ciertas
Sólo las dos primeras son ciertas.
La respuesta 2 es falsa.
Vemos que la información se escribe solamente en la caché.
Vemos que la información se escribe solamente en la memoria principal y en memoria secundaria.
La información se escribe en caché, en memoria principal y en memoria secundaria.
Todas son falsas.
Registros (nivel 0), memoria caché (nivel 1), memoria principal (nivel 2), disco duro (nivel 3)
Memoria caché (nivel 0), registros (nivel 1), memoria principal (nivel 2), disco duro (nivel 3)
Ninguna de las respuestas anteriores es correcta
Registros (nivel 0), memoria principal (nivel 1), memoria caché (nivel 2), disco duro (nivel 3)
Ninguna de las anteriores
Memoria caché y memoria principal
Memoria caché y almacenamiento secundario (discos)
Memoria principal y almacenamiento secundario (discos)
Registros y memoria principal
Registros del procesador, memoria caché, memoria RAM, copias de seguridad y discos duros
Memoria caché, registros del procesador, memoria RAM, copias de seguridad y discos duros
Registros del procesador, memoria RAM, memoria caché, copias de seguridad, y discos duros
Registros del procesador, memoria caché, memoria RAM, discos duros y copias de seguridad
Copia de seguridad, disco duro, memoria RAM, memoria caché y registros del procesador
Copia de seguridad, disco duro, memoria caché, memoria RAM y registros del procesador
Disco duro, copia de seguridad, memoria caché, memoria RAM y registros del procesador
Disco duro, copia de seguridad, memoria RAM, memoria caché y registros del procesador
Espacial: si se referencia un elemento, los elementos cercanos a él se volverán a referenciar pronto.\nTemporal: si se referencia un elemento, probablemente no se volverá a referenciar pronto.
Espacial: si se referencia un elemento, los elementos lejanos a él se volverán a referenciar pronto.\n\nTemporal: si se referencia un elemento, probablemente no se volverá a referenciar pronto.
Espacial: si se referencia un elemento, los elementos cercanos a él se volverá a referenciar pronto.\n\nTemporal: si se referencia un elemento, probablemente  se volverá a referenciar pronto.
Espacial: si se referencia un elemento, los elementos lejanos a él se volverán a referenciar pronto.\n\nTemporal: si se referencia un elemento, probablemente se volverá a referenciar pronto.  
Para aprovechar la localidad temporal
Para aprovechar la localidad espacial
Para aprovechar la localidad espacial y temporal
Ninguna de las respuestas anteriores es correcta
Contenido completo de una dirección
Unidad mínima de memoria con la que trabaja es Sistema de Jerarquía de Memoria
Unidad máxima de memoria con la que trabaja es Sistema de Jerarquía de Memoria
Contenido completo de una dirección
Ninguna de las respuestas anteriores es correcta
Registro de direcciones, registro de datos y un codificador
Registro de memoria, registro de datos y un decodificador
Bus de direcciones, registro de memoria y un decodificador
Registro de datos, registro de direcciones y un decodificador
Coste, velocidad y rendimiento
Velociddad, rendimiento y capacidad
Rendimiento, capacidad y coste
Capacidad, velocidad y coste
Unidad máxima de memoria con la que trabaja el Sistema de Jerarquía de Memoria
2 platos y 4 cabezas
4 cabezas y 4 platos
8 platos y 4 cabezas
4 platos y 8 cabezas
Ninguna es correcta
Cada una de las divisiones de un plato o disco
Cada una de las divisiones de un cabeza
Cada una de las divisiones de un pista
Ninguna es correcta
Aumentar el número de sectores en las pistas exteriores, y usar más eficientemente el disco duro
Aumentar el número de sectores en los discos o platos, y usar más eficientemente el disco duro
Aumentar el número de sectores en las cabezas, y usar más efientemente el disco duro
Ninguna de las anteriores
ZBR (direccinamiento lógico de bloques)
CHS (cilindro-cabeza-sector)
LBA (direccinamiento lógico de bloques)
Tiempo medio de acceso, tiempo medio de búsqueda, tiempo lectura/escritura, latencia media, velocidad de rotación y velocidad de trasnferencia.
Tiempo medio de acceso, tiempo medio de búsqueda, tiempo lectura/escritura, latencia media, velocidad de rotación, velocidad de trasnferencia y cantidad de información.
Tiempo medio de acceso, tiempo medio de búsqueda, tiempo lectura/escritura, cantidad de información, velocidad de rotación y velocidad de trasnferencia.
Tiempo medio de acceso, tiempo medio de búsqueda, tiempo medio lectura/escritura, velocidad de rotación y velocidad de trasnferencia.
Tiempo medio que tarda la aguja en situarse en la pista deseada.
Tiempo medio que tarda la aguja en situarse en la pista y el sector deseado.
Tiempo medio que tarda la aguja en situarse en el sector deseado.
Tiempo que tarda un disco en leer o escribir la nueva información. 
Tiempo medio que tarda la aguja en situarse en el sector deseado.
Tiempo que tarda un disco en leer o escribir la nueva información. 
Tiempo medio que tarda la aguja en situarse en la pista y el sector deseado.
Tiempo medio que tarda la aguja en situarse en la pista deseada.
Mediante un proceso electromecánico.
Mediante un proceso electromecánico y eletromagnético. 
Mediante un proceso electromagnético.
Ninguna de las ateriores.
4096 Mb
8192 Mb
16384 Mb
9216 Mb
256 bytes
128 bytes
1024 bytes
512 bytes
Una memora ROM dentro del disco duro.
Una memoria Caché dentro del disco duro.
Una memoria RAM dentro del disco duro.
Un registro dentro del disco duro.
Tiempo de acceso, tasa de trasnferencia y velocidad de rotación.  
Tiempo de escritura/lectura, tiempo de acceso y velocidad de rotación. 
Tiempo de acceso, velocidad de rotación y tiempo de búsqueda. 
Tiempo de acceso, tiempo de búsqueda y velocidad de rotación.
Tiempo que toma la controladora en buscar los datos para ser leidos.
Tiempo que toma la controladora en procesar un requerimiento de datos.
Tiempo que toma la controladora para mover el rotor a la pista correcta. 
Tiempo que toma la controladora para leer el dato y redireccionarlo al computador. 
Ordena al controlador del HD que mueva las cabezas de lectura y escritura a la tabla de asignación de archivos de la unidad, y el SO lee la tabla para determinar en que cluster del disco en el que comienza un achivo o que zonas del disco están disponibles para albergar    un nuevo archivo. 
Ordena al controlador del HD que mueva las cabezas de lectura y escritura a la tabla de asignación de archivos de la unidad, y el motor del HD lee la tabla para determinar en que cluster del disco en el que comienza un achivo o que zonas del disco están disponibles para albergar\nun nuevo archivo. 
Ninguna es correcta.\n
FM
MFM
RLL
RLLM
FM
MFM
RLL
RLLM
Del tamaño del HD, del número de pistas y de la velocidad y engranajes del cabezal.
Del tamaño del HD, del número de sectores y de la velocidad del cabezal. 
Del tamaño del HD, del número de pistas y de la velocidad del cabezal. 
Del tamaño del HD, del número de sectores y de la velocidad y engranajes del cabezal.
El tiempo que tarda en describir un giro completo. 
La mitad del tiempo que tarda el disco en describir un giro completo.
El tiempo que tarda en describir medio giro.
La mitad del tiempo que tarda el disco en describir un medio giro.
La transferencia de datos se desarrolla a través de los diferentes puerto I/O de la controladora que también sirven para la transmisión de comandos (IN / OUT).
Es una memoria incluida en la electrónica de las unidades de disco, que almacena el contenido de una pista completa.
Es la transferencia de datos desde el disco a la memoria evitando pasar por la CPU.
La CPU puede recoger los datos de la controladora de forma más rápida, si los deja en una zona de memoria fija, ya que entonces se puede realizar la transferencia de los datos a una zona de memoria del programa correspondiente con la introducción MOV.
Es la transferencia de datos desde el disco a la memoria evitando pasar por la CPU.
En esta técnica la controladora del disco duro desconecta la controladora del bus y transfiere los datos con la ayuda de un cotrolador con control propio.
Ninguna de las ateriores. 
Pueden estar dentro del propio disco duro, la gestión de esta memoria es completamente invisible y consiste en almacenar en ella los datos más pedidos por la CPU y retirar de ella aquellos no solicitados en un determinado tiempo.
Es una memoria incluida en la electrónica de las unidades de disco, que almacena el contenido de una pista completa.\n
Tabla de localización de los ficheros, Copias de la tabla, etiqueta del volumen y zona de datos para archivos. 
Sector de arranque, tabla de localización de los ficheros, copias de la tabla, directorio raíz y zona de datos para archivos. 
Sector de arranque, tabla de localización de los ficheros, directorio raíz y zona de datos para archivos.
Ninguna de las anteriores.
Es un interface a nivel de dispositivo diseñado como un sucesor del ST506 pero con un valor más alto de transferencia de datos.
Es un interface a nivel de sistema que cumple la norma ANSI de acoplamiento a los AT y que usa una variación sobre el bus de expansión del AT (por eso también llamados discos tipo AT) para conectar una unidad de disco a la CPU, con un valor máximo de transferencia de 4 Mbytes por segundo.  
Interface que usa un controlador externo para conectar discos al PC.
El primer interface utilizado en los PC’s. Proporciona un valor máximo de transferencia de datos de menos de 1 Mbyte por segundo.
Es una conexión entre el disco duro y su sistema principal que pone funciones de control y separación de datos sobre el propio disco (y no en el controlador externo)
Interface que usa un controlador externo para conectar discos al PC.
Estriba en traducir la información CHS en una dirección de 28 bits manejables por el sistema operativo, para el controlador de dispositivo y para la interfaz de la unidad.
Es un interface a nivel de sistema que cumple la norma ANSI de acoplamiento a los AT.
Raid 0, Raid 2 y Raid 4.
Raid 0, Raid 1, Raid 2, Raid 3, Raid 4, Raid 5.
Raid 0, Raid 2, Raid 4 y Raid 5.
Raid 0, Raid 1 y Raid 4. 
Raid 1, Raid 3 y Raid 5. 
Ninguna de las anteriores. 
Un sistema que es capaz de combinar espacio físico en disco, para mejorar la fiabilidad, capacidad o rendimiento del sistema.
Un sistema que es capaz de combinar espacio físico en disco, para mejorar la fiabilidad, capacidad o integridad del sistema.
Un sistema que es capaz de combinar espacio físico en disco, para mejorar la integridad, mejorar la fiabilidad.
Ninguna es correcta. 
Un sistema que combina varios unidades de disco en una unidad lógica. 
Un sistema que combina varias unidades de disco. 
La respuesta a y b son correctas. 
Ninguna de las anteriores es correcta. 
Usa división a nivel de bytes con un disco de paridad dedicado.
Combinar múltiples discos duros físicos en un solo disco virtual.
Crea una copia exacta de un conjunto de datos en dos o más discos.
Usa división de datos a nivel de bloques distribuyendo la información de paridad entre todos los discos miembros del conjunto.
Usa división a nivel de bloques con un disco de paridad dedicado.
Distribuye los datos equitativamente entre dos o más discos sin información de paridad que proporcione redundancia.
Usa división de datos a nivel de bloques distribuyendo la información de paridad entre todos los discos miembros del conjunto.
Divide los datos a nivel de bits en lugar de a nivel de bloques y usa un código de Hamming para la corrección de errores.
El uptime.
El rendimiento de las aplicaciones.
Protección de los datos.
Facilitar el traslado a un sistema nuevo.
El rendimiento de ciertas aplicaciones.
Protección de datos.
El uptime. 
Simplificar la recuperación de un desastre. 
Es volátil, electrónica y de capacidad media.
No es volátil, electrónica y capacidad baja.
Es volátil, magnética y de capacidad baja.
Es volátil, magnética y capacidad media.\n
No es volátil, es de solo escritura y es lenta.
No es volátil, es de solo lectura y es lenta. 
Es volátil, es de solo lectura y es rápida.
Es volátil, es de solo lectura y es lenta.
Memorias internas, poca capacidad y almacenan archivos. 
Memorias externas, poca capacidad y almancenan instrucciones.
Memorias internas, capacidad media y almacenan archivos.  
Memorias internas, poca capacidad y almancena instrucciones.
Memorias externas, con poca capacidad y rápidas.
Mucha capacidad, rápidas y memorias internas. 
Memorias externas, con mucha capacidad y lentas.
Memorias internas, con mucha capacidad y lentas.
Acceder a valores muy usados generalmente en operaciones matemáticas.
Recibir las instrucciones y guardar los resultados.
Almacenar datos de forma permanente.
Son caballos de batallas del sistema y pueden ser direccionados como una palabra o parte de un byte. 
Estan compuestos por 16 bits y contiene el desplazamiento de la siguiente instrucción que se va a ejecutar.
Estan compuestos por 16 bits y contiene el desplazamiento de la siguiente instrucción que se va a ejecutar.
Indican el estado actual de la máquina y el resultado del procesamiento.
Registros que tiene 16 bits de longitud y facilita un área de memoria para el direccionamiento.
Permiten al sistema acceder datos al segmento de la pila.
Sirven para el direccionamiento de indexado y para las operaciones de sumas y restas.
Estan compuestos por 16 bits y contiene el desplazamiento de la siguiente instrucción que se va a ejecutar.
Sirven para indicar el estado actual de la maquina y el resultado del procesamiento.
Registro de Segmento.
Registro de próposito general.
Registro apuntador de instrucciones.
Registro bandera. 
Registro bandera.
Registro apuntador de instrucciones. 
Registro de segmento. 
Registro índice. 
Registro bandera. 
Registro apuntador. 
Registro índice. 
Registro apuntador de instrucciones. 
Bus de datos, bus de direcciones y banco de memoria.
Bus de direccioens, señales misceláneas y banco de memoria. 
Bus de datos, señales misceláneas y banco de memoria.  
Ninguna de las anteriores. 
Baja
Muy baja
Alta
Muy alta
Pardiad cruzada y códigos de redundancia cíclica. 
Paridad cruzada y ECC.
Bit de paridad y códigos de redundancia cíclica. 
Bit de paridad y ECC.
Es un tipo de chip de memoria ROM no volátil que se programa mediante un aparato electrónico.
Es un tipo de memoria ROM que puede ser programado, borrado y reprogramado eléctricamente.
Es una memoria ROM donde el valor de cada bit depende del estado de un fusible. 
Ninguna de las anteriores.
Memoria síncrona , con tiempos de acceso de entre 25 y 10 ns.
Memoria síncrona, envía los datos dos veces por cada ciclo de reloj.
Memoria que permiten que los búferes de entrada/salida trabajen al doble de la frecuencia del núcleo.
Memoria que promete proporcionar significantes mejoras en el rendimiento en niveles de bajo voltaje.\n
Memoria ferromagnética. 
Memoria de vídeo. 
Memoria estática de acceso aleatorio. 
Memoria electrónica de acceso aleatorio. 
Tiempo que transcurre desde que se pide una lectura hasta que llega la palabra deseada. Tiempo mínimo entre peticiones consecutivas a memoria.
Tiempo que transcurre desde que se pide una lectura hasta que llega la palabra deseada. Tiempo máximo entre peticiones consecutivas a memoria.
Tiempo que transcurre desde que se pide la escritura de una palabra deseada. Tiempo máximo entre peticiones consecutivas a memoria.
Tiempo que transcurre desde que se pide la escritura de una palabra deseada. Tiempo mínimo entre peticiones consecutivas a memoria.
Tecnología barata, menor capacidad, menor velocidad y usadas en la memoria virtual.
Tecnología cara, menor capacidad, mayor velocidad y usada en la memoria virtual. 
Tecnología cara, menor capacidad, mayor velocidad y usada en la memoria caché.
Tecnología barata, menor capacidad, mayor velocidad y usada en la memoira virtual. 
Más lentas, direccionamiento doble y mayor capacidad. 
Más rápidas, direccionamiento simple y menor capacidad. 
Más lentas, direccionamiento simple y menor capacidad. 
Más rápidas, direccionamiento doble y mayor capacidad.
Permite acceder más rápidamente a posiciones de memoria contenidas en distintas filas.
Permite acceder más lentamente a posiciones de memoria contenidas en la misma columna.
Permite acceder más lentamente a posiciones de memoria contenidas en distintas columnas.
Permite acceder más rápidamente a posiciones de memoria contenidas en la misma fila.\n
Que la instruccion de la cual se obtienen los operandos está en la posición cero del ROB
No se podría realizar register renaming, y por tanto, una instrucción al entrar en una estación de reserva no podría saber qué estación de reserva es la que va a producir alguno de sus operandos a menos que recorra el reorder buffer
Sí, si la máquina implementa anticipación y permite insertar burbujas. No se pierde ningún ciclo.
Un algoritmo de planificación dinámica que permite la ejecución fuera de orden sin importar las posibles dependencias de datos.
Se retrasa la ejecución de ADDF hasta que LF termine de escribir el valor (R2) en F6.
Porque hay excepciones que se pueden producir en la etapa de escritura.
por la cual se solapa la ejecución de múltiples threads o hilos.
El hardware en las máquinas superescalares es bastante más complejo que en las VLIW \r\n
Afina casi tanto como la política "Óptima".
Unidad mínima de memoria con la que trabaja el Sistema de Jerarquía de Memoria
Pregunta incorrecta, pues las máquinas vectoriales con registros vectoriales tienen un rendimiento inferior a las de memoria a memoria.\r\n
Las máquinas vectoriales memoria-memoria ofrecen un rendimiento inferior.
Una unidad escalar segmentada y varias unidades vectoriales
Decodificación de la instrucción y búsqueda de registros
Búsqueda de la instrucción
Se reduciría en un ciclo el tiempo necesario para calcular la dirección de salto, por lo que sólo haría falta una instrucción nop (con 5 etapas harían falta dos).
L2 donde se guardan sólo datos. 
Capacidad instruction-level parallelism (ILP).
L2 donde se guardan sólo instrucciones 
Capacidad thread-level parallelism (TLP).
Fue diseñado por Sun Microsystems.
Cuenta con un único procesador donde se integran 8 núcleos, es decir, una procesador multinúcleo.\r\n
Una capa de abstracción del hardware, que engaña a las aplicaciones que se van a ejecutar creyendo que están corriendo sobre una máquina con 32 procesadores.  
Una técnica para mejorar ILP.
Una tecnología que depende de la velocidad del procesador para aumentar TLP.
Proceso ligero que representa un hilo de ejecución dentro de un proceso.
Cuando la unidad funcional que se necesita está ocupada por otra instrucción anterior
Cuando se tiene que tomar una decisión en un branch basada en los resultados de  otras instrucciones que todavía se están ejecutando. 
Para conservar  los cálculos realizados en una instrucción individual durante todas sus etapas.
Permanece constante hasta que se sobrepasa el tamaño máximo de vector, en cuyo caso, hay que seccionar el vector, introduciendo un retardo en el coste de arranque
NRU al no borrar los bits de escritura, se sabe qué bloques han de ser copiados a memoria principal
Los predictores de saltos correlacionados tienen en cuenta el comportamiento de otros saltos a la hora de realizar las predicciones para un salto concreto
En que no haya otros stores  en la cola pendientes de ejecutarse que accedan a la misma dirección
Si se acertó en la predicción nada; si no se acertó, se vacía el reorder buffer
El tiempo de ejecución siempre es el mismo
Es un desenrollado de bucles que escoge en el desenrrollado instrucciones específicas en cada iteracción
Tienen instrucciones típicas de cualquier procesador pero se ubican éstas en una palabra larga de instrucción\r\n
No, en el desenrrollado de bucles no se especifican las instrucciones que pueden paralelizarse en diferentes iteracciones, mientras que en la técnica de software pipeling si se especifican.\r\n
Como alternativa a la ejecución especulativa se hace uso de la predicación\r\n
ganar tiempo en la ejecución de los saltos condicionales
Dos instrucciones.
